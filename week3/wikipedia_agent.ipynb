{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wikipedia Agent\n",
        "\n",
        "This notebook creates a Wikipedia agent that can:\n",
        "1. Search Wikipedia for relevant pages using the Wikipedia API\n",
        "2. Retrieve and analyze page content\n",
        "3. Answer questions by searching first, then reading relevant pages\n",
        "\n",
        "Based on week 2 Pydantic AI examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from pydantic_ai import Agent\n",
        "from typing import List, Dict, Any\n",
        "import urllib.parse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_wikipedia(query: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Search Wikipedia for pages matching the query.\n",
        "    \n",
        "    Args:\n",
        "        query: Search term (spaces will be converted to \"+\" for URL)\n",
        "        \n",
        "    Returns:\n",
        "        List of search results, each containing:\n",
        "        - title: Page title\n",
        "        - snippet: Text snippet from the page\n",
        "        - pageid: Wikipedia page ID\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert spaces to \"+\" for URL encoding\n",
        "        search_term = query.replace(\" \", \"+\")\n",
        "        \n",
        "        # Wikipedia API search endpoint\n",
        "        url = f\"https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch={search_term}\"\n",
        "        \n",
        "        # Add User-Agent header (best practice for Wikipedia API)\n",
        "        headers = {\n",
        "            'User-Agent': 'WikipediaAgent/1.0 (Educational Agent)'\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        data = response.json()\n",
        "        search_results = data.get('query', {}).get('search', [])\n",
        "        \n",
        "        # Format results\n",
        "        results = []\n",
        "        for item in search_results:\n",
        "            results.append({\n",
        "                \"title\": item.get('title', ''),\n",
        "                \"snippet\": item.get('snippet', ''),\n",
        "                \"pageid\": item.get('pageid', ''),\n",
        "                \"size\": item.get('size', 0),\n",
        "                \"wordcount\": item.get('wordcount', 0)\n",
        "            })\n",
        "        \n",
        "        return results\n",
        "    except Exception as e:\n",
        "        return [{\"error\": f\"Error searching Wikipedia: {str(e)}\"}]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_wikipedia_page(title: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the raw content of a Wikipedia page.\n",
        "    \n",
        "    Args:\n",
        "        title: Wikipedia page title (exact match - spaces are converted to underscores for URL)\n",
        "        \n",
        "    Returns:\n",
        "        Raw page content in wikitext format, or error message if page not found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Wikipedia expects underscores for spaces in URLs\n",
        "        # Convert spaces to underscores (standard Wikipedia URL format)\n",
        "        encoded_title = title.replace(\" \", \"_\")\n",
        "        \n",
        "        # Wikipedia raw content endpoint\n",
        "        # Format: https://en.wikipedia.org/w/index.php?title=PAGE_TITLE&action=raw\n",
        "        url = f\"https://en.wikipedia.org/w/index.php?title={encoded_title}&action=raw\"\n",
        "        \n",
        "        # Add User-Agent header (best practice for Wikipedia API)\n",
        "        headers = {\n",
        "            'User-Agent': 'WikipediaAgent/1.0 (Educational Agent)'\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        return response.text\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        if e.response.status_code == 404:\n",
        "            return f\"Error: Wikipedia page '{title}' not found. Please verify the page title from search results.\"\n",
        "        return f\"Error fetching page '{title}': HTTP {e.response.status_code}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching page '{title}': {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Tools Individually\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search Results:\n",
            "============================================================\n",
            "\n",
            "1. Capybara\n",
            "   Snippet: The <span class=\"searchmatch\">capybara</span> or greater <span class=\"searchmatch\">capybara</span> (...\n",
            "   Page ID: 6776\n",
            "\n",
            "2. Capybara (disambiguation)\n",
            "   Snippet: Look up <span class=\"searchmatch\">capybara</span> or <span class=\"searchmatch\">Capybara</span> in Wi...\n",
            "   Page ID: 69085306\n",
            "\n",
            "3. Lesser capybara\n",
            "   Snippet: The lesser <span class=\"searchmatch\">capybara</span> (Hydrochoerus isthmius) is a large semi-aquatic...\n",
            "   Page ID: 23188846\n"
          ]
        }
      ],
      "source": [
        "# Test the search tool\n",
        "test_search = search_wikipedia(\"capybara\")\n",
        "print(\"Search Results:\")\n",
        "print(\"=\" * 60)\n",
        "for i, result in enumerate(test_search[:3], 1):\n",
        "    print(f\"\\n{i}. {result.get('title', 'N/A')}\")\n",
        "    print(f\"   Snippet: {result.get('snippet', 'N/A')[:100]}...\")\n",
        "    print(f\"   Page ID: {result.get('pageid', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching page: Capybara\n",
            "============================================================\n",
            "Content length: 36016 characters\n",
            "\n",
            "First 500 characters:\n",
            "------------------------------------------------------------\n",
            "{{Short description|Largest species of rodents}}\n",
            "{{Other uses}}\n",
            "{{Good article}}\n",
            "{{pp|small=yes}}\n",
            "{{Use dmy dates|date=July 2022}}\n",
            "{{Speciesbox\n",
            "| status            = LC\n",
            "| status_system     = IUCN3.1\n",
            "| status_ref        = <ref name=\"iucn status 19 November 2021\">{{cite iucn |author=Reid, F. |date=2016 |title=''Hydrochoerus hydrochaeris'' |volume=2016 |article-number=e.T10300A22190005 |doi=10.2305/IUCN.UK.2016-2.RLTS.T10300A22190005.en |access-date=19 November 2021}}</ref>\n",
            "| image             = Hy\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# Test the get_page tool with a known page\n",
        "if test_search:\n",
        "    test_title = test_search[0]['title']\n",
        "    print(f\"\\nFetching page: {test_title}\")\n",
        "    print(\"=\" * 60)\n",
        "    page_content = get_wikipedia_page(test_title)\n",
        "    print(f\"Content length: {len(page_content)} characters\")\n",
        "    print(f\"\\nFirst 500 characters:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(page_content[:500])\n",
        "    print(\"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikipedia_agent_instructions = \"\"\"\n",
        "You are a helpful Wikipedia research assistant.\n",
        "\n",
        "Your workflow when answering questions:\n",
        "1. FIRST, use search_wikipedia(query) to find relevant Wikipedia pages\n",
        "2. Review the search results to identify the most relevant pages\n",
        "3. Use get_wikipedia_page(title) to retrieve the content of the most relevant pages\n",
        "4. Analyze the page content to answer the user's question\n",
        "5. Cite the Wikipedia pages you used in your answer\n",
        "\n",
        "IMPORTANT GUIDELINES:\n",
        "- Always start with a search - never try to answer without searching first\n",
        "- Use specific search terms that match what the user is asking about\n",
        "- Read multiple relevant pages if needed to provide a comprehensive answer\n",
        "- When you get page content, look for specific information that answers the question\n",
        "- Cite your sources by mentioning the Wikipedia page titles\n",
        "- If search returns no results, try alternative search terms\n",
        "- If a page is not found, try variations of the title or search again\n",
        "\"\"\"\n",
        "\n",
        "wikipedia_agent = Agent(\n",
        "    name='wikipedia_agent',\n",
        "    instructions=wikipedia_agent_instructions,\n",
        "    tools=[search_wikipedia, get_wikipedia_page],\n",
        "    model='openai:gpt-4o-mini'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is a capybara and where do they live?\n",
            "\n",
            "================================================================================\n",
            "AGENT PROCESSING...\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "AGENT RESPONSE:\n",
            "================================================================================\n",
            "The **capybara** (*Hydrochoerus hydrochaeris*) is the largest living rodent in the world, native to South America. It is part of the genus *Hydrochoerus* and is closely related to guinea pigs and rock cavies. The name \"capybara\" comes from the Tupi language, meaning \"one who eats slender leaves.\"\n",
            "\n",
            "Capybaras primarily inhabit **savannas** and **dense forests**, often located near bodies of water such as lakes, rivers, swamps, and marshes. They thrive in environments that provide ample aquatic vegetation for their diet, which consists mainly of grasses and aquatic plants. These animals are social creatures and typically form groups of 10 to 20, although larger groups of up to 100 can occur, especially around reliable water sources during dry seasons. In terms of geographic distribution, capybaras can be found throughout almost all of South America, with the notable exception of Chile. They are also seen in varying habitats, from tropical rainforests to flooded savannahs, and can adapt to urban areas as well.\n",
            "\n",
            "Capybaras are considered semiaquatic mammals due to their affinity for water; they are excellent swimmers and can hold their breath underwater for several minutes, which helps them evade predators such as jaguars and caimans. The maximum lifespan of a capybara is 8 to 10 years, though they often live shorter lives in the wild due to predation.\n",
            "\n",
            "For more detailed information, you can refer to the Wikipedia page on [Capybaras](https://en.wikipedia.org/wiki/Capybara).\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test the agent with a question\n",
        "question = \"What is a capybara and where do they live?\"\n",
        "\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(\"AGENT PROCESSING...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "result = await wikipedia_agent.run(user_prompt=question)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"AGENT RESPONSE:\")\n",
        "print(\"=\" * 80)\n",
        "print(result.output)\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try Your Own Questions\n",
        "\n",
        "Test the agent with different questions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the history of artificial intelligence?\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "AGENT RESPONSE:\n",
            "================================================================================\n",
            "The history of artificial intelligence (AI) is a rich and complex narrative that spans several centuries, beginning with early myths about intelligent automata and progressing through major technological and conceptual breakthroughs.\n",
            "\n",
            "### Antiquity to the Early 20th Century\n",
            "The concept of artificial beings with intelligence can be traced back to ancient myths, such as the Greek automaton Talos. Interest in logic and reasoning by figures like Aristotle laid the groundwork for formal reasoning. The invention of the programmable digital computer in the 1940s marked a significant technological turning point, enabling discussions about building machines capable of thought.\n",
            "\n",
            "### Birth of AI (1950s)\n",
            "The field of AI research was officially established at the Dartmouth workshop in 1956, where pioneers like John McCarthy and Marvin Minsky aimed to explore whether machines could exhibit intelligent behavior. The optimism was evident; many researchers believed that human-level AI could be achieved within a generation. Early successes included programs that could solve algebra problems and prove mathematical theorems.\n",
            "\n",
            "### The First AI Boom and Challenges (1956–1974)\n",
            "The late 1950s to early 1970s saw significant government funding, notably from DARPA, fueling a rapid expansion in AI research. However, challenges soon became apparent as researchers underestimated the complexity of replicating human intelligence. This led to critical reports, such as the Lighthill report in 1973, which criticized the excessive expectations of AI. Consequently, funding was drastically reduced, leading to what is known as the \"first AI winter.\"\n",
            "\n",
            "### Revival through Expert Systems (1980s)\n",
            "Despite the setbacks, interest in AI was revived in the 1980s through the development of expert systems, which utilized rule-based systems to solve specific problems in fields such as medicine and engineering. The government, particularly Japan, invested heavily in this area, recognizing the potential for AI applications in various industries.\n",
            "\n",
            "### The Second AI Winter (1990s)\n",
            "The late 1980s and early 1990s saw another downturn as the limitations of expert systems became evident, leading to decreased funding and public interest. Researchers began to turn away from the AI label, often rebranding their work under different terms such as \"machine learning.\"\n",
            "\n",
            "### Revival in the 21st Century\n",
            "The early 2000s marked a resurgence of AI, spurred by advances in machine learning, particularly deep learning techniques and the availability of large datasets. The development of neural networks now reshaped the field, with AI applications emerging in diverse sectors, including healthcare, finance, and autonomous vehicles.\n",
            "\n",
            "### The AI Boom (2010s to Present)\n",
            "The introduction of transformer models in 2017 revolutionized natural language processing, leading to the emergence of sophisticated models like GPT-3 and ChatGPT. The AI boom of the 2020s saw unprecedented investment in AI technologies, along with debates about the ethical implications of such advancements, including concerns over bias, privacy, and the potential for job displacement.\n",
            "\n",
            "### Conclusion\n",
            "AI's journey from speculative fiction to practical implementation reflects a dynamic interplay between technological advances, regulatory challenges, and shifting public perception. The field continues to evolve rapidly, raising questions about the future role of AI in society and the ethical considerations that come with it.\n",
            "\n",
            "For more detailed information, you can check the [History of artificial intelligence](https://en.wikipedia.org/wiki/History_of_artificial_intelligence) page on Wikipedia.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "p# Try your own question here\n",
        "your_question = \"What is the history of artificial intelligence?\"\n",
        "\n",
        "print(f\"Question: {your_question}\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "result = await wikipedia_agent.run(user_prompt=your_question)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"AGENT RESPONSE:\")\n",
        "print(\"=\" * 80)\n",
        "print(result.output)\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
