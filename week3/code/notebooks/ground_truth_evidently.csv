question,summary_answer,difficulty,intent,filename
how to create a Dataset object in Evidently,"To create a `Dataset` object, use `Dataset.from_pandas()` and pass your data along with a `DataDefinition` object to specify column types and roles.",beginner,code,docs/library/data_definition.mdx
Evidently DataDefinition examples,"The `DataDefinition` allows users to specify mappings for column types like numerical, categorical, and text to ensure proper data evaluations in Evidently.",beginner,code,docs/library/data_definition.mdx
difference between numerical and categorical columns in Evidently,"Numerical columns contain numeric values used for calculations, while categorical columns represent discrete categories or groups, each serving different roles in data evaluations.",intermediate,text,docs/library/data_definition.mdx
how to manually map columns in DataDefinition,"You can manually map columns in `DataDefinition` by specifying each column in its respective category, such as `numerical_columns` or `text_columns`, instead of relying on automatic mapping.",intermediate,code,docs/library/data_definition.mdx
example of mapping regression columns in Evidently,"To map regression columns, define the target and prediction columns in `DataDefinition` using `Regression(target='y_true', prediction='y_pred')` for example.",intermediate,code,docs/library/data_definition.mdx
automatically mapping columns in Evidently,"You can map columns automatically by passing an empty `DataDefinition()` to `Dataset.from_pandas()`, allowing Evidently to infer column types and roles based on their names and data types.",beginner,code,docs/library/data_definition.mdx
role of ID and timestamp columns in data definition,"ID columns serve as unique identifiers ignored in calculations, while timestamp columns track when data entries are recorded, also ignored in some evaluations like drift detection.",intermediate,text,docs/library/data_definition.mdx
importance of text column mapping in Evidently,"Mapping text columns in `DataDefinition` is crucial for analyses like text data drift detection. Without it, the system may not accurately process those columns during evaluations.",intermediate,text,docs/library/data_definition.mdx
Evidently DataDefinition for multiclass classification,"For multiclass classification in `DataDefinition`, use `MulticlassClassification` to specify target, prediction labels, and optionally, prediction probabilities and display labels.",advanced,code,docs/library/data_definition.mdx
using pandas.DataFrame directly in Evidently,"You can pass a `pandas.DataFrame` directly into `report.run()` for quick checks, but creating a `Dataset` object is recommended for more controlled evaluations.",intermediate,code,docs/library/data_definition.mdx
data requirements for different evaluations in Evidently,Each evaluation type in Evidently has specific column type and mapping requirements; consult the reference table for these data needs based on the evaluation you intend to run.,beginner,text,docs/library/data_definition.mdx
text data evaluation methods,"The article explains how to evaluate text data using Descriptors, which provide scores or labels for each row in the dataset, allowing for various evaluation methods based on both built-in and custom descriptors.",beginner,text,docs/library/descriptors.mdx
how to create custom descriptors,"You can create custom descriptors using LLM prompts or Python code, as described in the article, which guides on adding these to your datasets.",beginner,code,docs/library/descriptors.mdx
examples of built-in descriptors,"The article outlines several built-in descriptors, such as Sentiment and TextLength, which can be used directly for evaluating text data.",intermediate,text,docs/library/descriptors.mdx
Python code to evaluate text,"The article provides Python code snippets to create datasets, add descriptors, and evaluate text data using the Evidently library.",intermediate,code,docs/library/descriptors.mdx
how to generate toy data for testing,"A code snippet is provided for generating toy data to test the evaluation methods, demonstrating how to format data into a pandas DataFrame.",beginner,code,docs/library/descriptors.mdx
Testing conditions on descriptors,"The article describes how to apply testing conditions for descriptors, allowing you to set pass/fail criteria for evaluation results directly within the descriptor settings.",intermediate,code,docs/library/descriptors.mdx
LLM judging in descriptor evaluation,"It explains how to use built-in LLM judges as descriptors, which require API keys for evaluation, adding a layer of functionality for text evaluation.",advanced,code,docs/library/descriptors.mdx
How to export results,"You can export results and visualize them by converting the evaluation dataset into a DataFrame or other output formats like HTML or JSON, as detailed in the article.",intermediate,code,docs/library/descriptors.mdx
integrating multiple descriptors,The article covers how to combine multiple descriptors in your dataset evaluation to enrich the evaluation metrics and insights obtained from the data.,intermediate,text,docs/library/descriptors.mdx
descriptor tests summary,"It explains how to use TestSummary to combine results from multiple tests into a single summary for each row, providing more digestible results.",advanced,code,docs/library/descriptors.mdx
pass/fail checks for text data,"The article discusses how to implement pass/fail checks in your evaluations, allowing for more stringent analysis of text quality based on defined criteria.",beginner,code,docs/library/descriptors.mdx
comparison between datasets for drift detection,"It outlines how to run comparisons between datasets, which is crucial for detecting drift in text evaluations, using the Report functionality within the Evidently library.",advanced,code,docs/library/descriptors.mdx
Evidently evaluation workflow steps,"The article outlines the main steps in the Evidently evaluation workflow, including data preparation, dataset creation, and report configuration.",beginner,text,docs/library/evaluations_overview.mdx
How to create a Dataset object in Evidently,"To create a Dataset object in Evidently, you can use `DataDefinition()` with the `Dataset.from_pandas()` method to specify column roles and types.",intermediate,code,docs/library/evaluations_overview.mdx
Configuring Reports in Evidently,Configuring Reports in Evidently involves creating a Report with specified metrics and presets to evaluate datasets and summarize results.,advanced,code,docs/library/evaluations_overview.mdx
generate multiple metrics with Evidently,"The article details using metric generator helper functions from the Evidently library to generate multiple metrics at once, simplifying the testing process across dataset columns.",beginner,code,docs/library/metric_generator.mdx
using ColumnMetricGenerator for metrics,"It describes how to utilize the ColumnMetricGenerator class to apply specific metrics, like ValueDrift or UniqueValueCount, to designated columns in a dataset, including code snippets for practical implementation.",intermediate,code,docs/library/metric_generator.mdx
export evaluation results Python,"The article details how to export evaluation results in different formats including HTML and JSON, with code examples provided for saving reports.",beginner,code,docs/library/output_formats.mdx
evidently library overview,"The article explains the core concepts and components of the Evidently Python library, emphasizing its role in evaluating and monitoring AI systems.",beginner,text,docs/library/overview.mdx
how to evaluate AI models using evidently,Evidently allows you to evaluate AI models with over 100 built-in metrics and customizable templates for tailored assessments.,beginner,code,docs/library/overview.mdx
data quality checks with evidently,"The library provides various built-in tests for data quality, including checks for missing values, duplicates, and statistical distributions.",beginner,code,docs/library/overview.mdx
generate visual reports evidently,"You can export evaluation results as visual reports in formats like JSON, DataFrame, or HTML for better insights.",beginner,code,docs/library/overview.mdx
AI/ML evaluation metrics evidently,"Evidently offers a comprehensive set of metrics for different evaluation types, such as accuracy, precision, and regression metrics.",intermediate,text,docs/library/overview.mdx
synthetic data generation evidently,"Evidently features a configuration for generating structured synthetic data, particularly for testing AI systems.",intermediate,code,docs/library/overview.mdx
prompt optimization with evidently,"The library includes tools for optimizing prompts, enabling automated writing based on evaluation scoring and user feedback.",intermediate,code,docs/library/overview.mdx
using apparently for data drift detection,"Evidently offers metrics to detect and evaluate data drift, allowing users to compare datasets over time for distribution changes.",intermediate,code,docs/library/overview.mdx
how to structure datasets for evidently,The article outlines steps to prepare your data as a pandas DataFrame and convert it into an Evidently Dataset object for evaluations.,beginner,code,docs/library/overview.mdx
descriptor examples in evidently,"Descriptors in the Evidently library allow you to evaluate specific qualities of text data, such as length or sentiment analysis.",intermediate,code,docs/library/overview.mdx
reports in evidently library,You can generate detailed reports that analyze datasets across various metrics and include visual representations of the results.,intermediate,code,docs/library/overview.mdx
tests and test suites evidently,"The library includes features to create tests that validate results against established conditions, useful for automated checks.",intermediate,code,docs/library/overview.mdx
how to integrate evidently into workflows,"Evidently can be integrated into existing data pipelines, allowing for real-time evaluation and monitoring of AI systems.",beginner,text,docs/library/overview.mdx
visualization UI in evidently,"Evidently provides a lightweight UI to visualize and store evaluation results over time, enhancing data management.",beginner,code,docs/library/overview.mdx
using evidently for classification tasks,"The library supports classification model evaluations, providing metrics like ROC AUC and confusion matrices for analysis.",intermediate,code,docs/library/overview.mdx
importance of data preparation evidently,Preparing data accurately is crucial for running evaluations; the article highlights how to set up your data correctly for Effective assessments.,beginner,text,docs/library/overview.mdx
linking datasets in evidently,"Evidently allows for datasets to be linked in evaluations, facilitating side-by-side comparisons and drift detection.",intermediate,code,docs/library/overview.mdx
test presets in evidently,"There are pre-configured test presets available in Evidently, designed to streamline the testing process for various evaluations.",advanced,code,docs/library/overview.mdx
metrics presets in evidently,Evidently offers metric presets to help compute multiple related metrics with a single line of code for various evaluation scenarios.,advanced,code,docs/library/overview.mdx
self-hosting evidently platform,The article discusses how users can self-host the Evidently platform or use the cloud version for more advanced features.,intermediate,text,docs/library/overview.mdx
evidently library comparison with other tools,The article may cover the advantages of Evidently compared to other monitoring tools in AI evaluation processes.,advanced,text,docs/library/overview.mdx
advanced evaluation techniques evidently,"Evidently allows for advanced evaluation techniques, such as utilizing test suites for specific conditions durable across AI model updates.",advanced,text,docs/library/overview.mdx
how to create report with Evidently,The article explains how to generate a report using the Evidently library by importing necessary metrics and presets and passing them to the Report class.,beginner,code,docs/library/report.mdx
Evidently report templates,"The document discusses the use of pre-built report templates known as presets, which users can apply to generate reports easily without extensive configuration.",beginner,text,docs/library/report.mdx
comparing multiple evaluation results in Evidently,"You can compare multiple snapshots of evaluations side-by-side using the compare function, allowing for a quick analysis of the results.",intermediate,code,docs/library/report.mdx
limit columns in Evidently report,"To limit the columns on which a preset is applied in a report, you can specify the desired columns when creating the Report instance, enhancing focus on specific data.",intermediate,code,docs/library/report.mdx
adding tags to reports in Evidently,"The article details how to add tags to reports in Evidently, allowing users to mark evaluation runs by various criteria such as model version or status for better search and filtering.",beginner,code,docs/library/tags_metadata.mdx
custom metadata in Evidently,"It explains how to pass custom metadata as a Python dictionary when creating reports, enabling users to associate key:value pairs with their evaluation runs for improved context.",intermediate,code,docs/library/tags_metadata.mdx
conditional checks in data validation,"The article explains how to run conditional checks using Tests to validate specific conditions in datasets, ensuring data quality and adherence to expected metrics.",beginner,text,docs/library/tests.mdx
how to import evidently for tests,"To use Tests from the Evidently library, import the necessary modules including Report and Metrics as shown in the examples provided.",beginner,code,docs/library/tests.mdx
using test presets in evidently,You can leverage Test Presets to automatically run a set of predefined Tests on your data by enabling the `include_tests=True` option in the Report.,beginner,code,docs/library/tests.mdx
custom test conditions example,"The article illustrates how to set custom pass/fail conditions for Tests, allowing specific validation criteria for different metrics in your report.",intermediate,code,docs/library/tests.mdx
metrics reference for test conditions,"For details on available metrics and their test conditions, refer to the All Metrics reference table linked in the document.",intermediate,text,docs/library/tests.mdx
how to exclude tests in evidently,You can exclude certain Tests from being generated in a Report by setting the `tests` parameter to an empty list for specific Metrics as demonstrated in the examples.,intermediate,code,docs/library/tests.mdx
manage alert fatigue with test criticality,The article describes how to set the criticality of Tests to manage alert fatigue by allowing failed Tests to return a Warning instead of an outright Fail when desired.,advanced,code,docs/library/tests.mdx
testing against a reference dataset,"When validating data, you can set Test conditions relative to a reference dataset, allowing for dynamic comparisons using various thresholds.",advanced,code,docs/library/tests.mdx
multiple conditions in tests,"You can combine multiple conditions for a single Metric in Reports to perform thorough validations, as shown in the section on testing count vs. share.",advanced,code,docs/library/tests.mdx
set up alerts in Evidently Cloud,The article outlines the steps to enable alerts in Evidently Cloud by selecting a notification channel and defining alert conditions for failed tests or custom metrics.,beginner,code,docs/platform/alerts.mdx
add dashboard panel python example,"The article provides code examples for adding different types of dashboard panels using the Python API, including text, counters, and plots.",beginner,code,docs/platform/dashboard_add_panels.mdx
how to delete dashboard panel,Instructions for deleting specific panels and tabs from the dashboard using Python API commands are outlined in the article.,beginner,code,docs/platform/dashboard_add_panels.mdx
create new tab in dashboard python,"You can create a new tab in the dashboard using `project.dashboard.add_tab(""New Tab"")` as described in the article.",beginner,code,docs/platform/dashboard_add_panels.mdx
evidently dashboard panel types,"The article explains different types of dashboard panels such as text panels, counters, and plots, along with their usage and examples.",intermediate,text,docs/platform/dashboard_add_panels.mdx
example of counter panel in dashboard,"Code snippets are provided for adding counter panels displaying values with optional text, demonstrated using the `RowCount` metric.",intermediate,code,docs/platform/dashboard_add_panels.mdx
modify panel visualizations in dashboard,"The article describes how to adjust visualizations for panels, including selecting plot types like line, bar, and pie charts based on the data availability.",intermediate,text,docs/platform/dashboard_add_panels.mdx
dashboard.clear_dashboard() usage,The article explains how to use `project.dashboard.clear_dashboard()` to delete all tabs and panels from the dashboard without affecting the underlying data.,intermediate,code,docs/platform/dashboard_add_panels.mdx
evidently sdk dashboard metrics,"Different metric options available for dashboard panels are covered, emphasizing how to use `PanelMetric` to display values effectively.",intermediate,text,docs/platform/dashboard_add_panels.mdx
different aggregations for dashboard counters,"The article provides examples of adding count panels with different aggregation options like sum, average, and last values.",beginner,code,docs/platform/dashboard_add_panels.mdx
how to add pie chart in dashboard,Instructions for adding pie charts to the dashboard with various aggregation parameters are included in the article.,intermediate,code,docs/platform/dashboard_add_panels.mdx
dashboard panel parameters summary,"A summary table listing essential parameters for configuring dashboard panels, including `title`, `size`, and `values`, is presented in the article.",advanced,text,docs/platform/dashboard_add_panels.mdx
advanced usage of panel metrics,"The article discusses advanced configurations for `PanelMetric`, including handling multiple metrics and specific `metric_labels` for more precise data representation.",advanced,code,docs/platform/dashboard_add_panels.mdx
connect to Evidently Cloud for dashboard,"Before adding panels, users must connect to Evidently Cloud and create a project, as outlined in the preliminary tips of the article.",beginner,text,docs/platform/dashboard_add_panels.mdx
creating dashboard panels tutorial,The article provides a step-by-step guide on how to create and configure dashboard panels for visualizing evaluation results.,beginner,code,docs/platform/dashboard_add_panels_ui.mdx
how to add tabs in dashboard UI,"Instructions on adding tabs to organize panels on a dashboard are detailed, including options for custom tabs and available templates.",beginner,code,docs/platform/dashboard_add_panels_ui.mdx
edit delete dashboard panels,"The article explains how to enter edit mode to delete or edit existing dashboard panels, facilitating user customization.",intermediate,code,docs/platform/dashboard_add_panels_ui.mdx
dashboard panel configuration options,"Details on configuring different options for dashboard panels, such as filtering metrics and setting panel types, are outlined in the article.",advanced,text,docs/platform/dashboard_add_panels_ui.mdx
how to create a panel in Evidently dashboard,"The article explains that you can create a Panel in the Dashboard using either the Python API or the UI, where you define parameters like title, type, and metric values to display.",beginner,code,docs/platform/dashboard_overview.mdx
what is the purpose of a dashboard in Evidently,"A Dashboard in Evidently serves to provide insights into AI application performance by tracking evaluation results and live production quality over time, ultimately helping to visualize important metrics.",beginner,text,docs/platform/dashboard_overview.mdx
what is a dataset in evidently,"The article defines a dataset as a collection of data used for analysis and automated checks, which can be created from various sources like existing data, live data, or synthetic data.",beginner,text,docs/platform/datasets_overview.mdx
how to upload datasets in evidently,"You can upload datasets in Evidently by directly using the UI with CSV files, attaching them to reports during evaluations, generating synthetic data, or creating them from traces.",intermediate,code,docs/platform/datasets_overview.mdx
upload dataset to Evidently Python example,"The article explains how to prepare and upload a dataset to Evidently using the `add_dataset` method in Python, highlighting the need for a specified dataset name and optional description.",beginner,code,docs/platform/datasets_workflow.mdx
data definition role in dataset uploads,"It details the importance of a data definition when uploading datasets to Evidently, helping the platform understand specific column roles for future evaluations.",intermediate,text,docs/platform/datasets_workflow.mdx
api evals example code,"The article provides example code for running evaluations using the Evidently library, detailing how to set up the report and log results to the platform.",beginner,code,docs/platform/evals_api.mdx
workflow for running evals Evidently,"It outlines the complete workflow for running evaluations, including configuring reports, uploading results, and exploring the outcomes within the Evidently platform.",intermediate,text,docs/platform/evals_api.mdx
browse evaluation results project reports,"The article details how to navigate to the Reports section of your project to view and manage evaluation results, including options for sorting and downloading data.",beginner,code,docs/platform/evals_explore.mdx
no-code data evaluation steps,"The article outlines the steps for evaluating data in a no-code interface, starting from preparing the dataset to running the evaluation and interpreting the results.",beginner,text,docs/platform/evals_no_code.mdx
how to add descriptors in no-code evals,"It explains how to add descriptors for data evaluation through a simple interface, allowing users to select methods like sentiment analysis or regular expressions.",beginner,code,docs/platform/evals_no_code.mdx
configuring custom LLM evaluator,"The article provides a detailed description on configuring a custom LLM evaluator, explaining the parameters needed for effective text evaluations using language models.",intermediate,code,docs/platform/evals_no_code.mdx
differences between evaluation methods in no-code,"It describes various evaluation methods available in the no-code interface, such as model-based, regex, and LLM-based evaluations, along with their specific configurations.",intermediate,text,docs/platform/evals_no_code.mdx
running evaluations in Evidently platform,"The article describes how to run evaluations using the Evidently platform, covering both code-based and no-code methods for analyzing AI outputs and data.",beginner,text,docs/platform/evals_overview.mdx
code for Python evaluations Evidently,The article outlines how to perform Python-based evaluations on AI outputs through the Evidently platform by generating reports and uploading results.,intermediate,code,docs/platform/evals_overview.mdx
batch monitoring example Python,"The article provides a simple code example for batch monitoring using the Evidently library, demonstrating how to get dataset stats and upload to the workspace.",beginner,code,docs/platform/monitoring_local_batch.mdx
Evidently batch workflow steps,"It outlines the complete workflow for batch monitoring, including configuring metrics, running evaluations, uploading results, setting up dashboards, and configuring alerts.",intermediate,text,docs/platform/monitoring_local_batch.mdx
AI observability definition,"The article explains AI observability as the ability to evaluate the quality of inputs and outputs of AI applications in production, allowing for real-time monitoring and issue detection.",beginner,text,docs/platform/monitoring_overview.mdx
Evidently batch monitoring setup,"It describes how to set up batch monitoring jobs with Evidently, including creating an evaluation pipeline, running metric calculations, and visualizing the results on a dashboard.",intermediate,code,docs/platform/monitoring_overview.mdx
Benefits of scheduled evaluations in AI,"The article lists the benefits of using scheduled evaluations in AI, such as improved data capture and the ability to easily re-run evaluations, making it ideal for LLM-powered applications.",advanced,text,docs/platform/monitoring_overview.mdx
Evidently Platform features overview,"The article details key features of the Evidently Platform, including evaluation tracking, dataset management, and synthetic data generation, aimed at enhancing AI quality across the lifecycle.",beginner,text,docs/platform/overview.mdx
Evidently Python library examples,"The article mentions that the Evidently Python library can be used for running local evaluations and uploading datasets, providing a foundation for programmatic access to the platform.",intermediate,code,docs/platform/overview.mdx
create project in Evidently example,"The article provides code snippets for creating a project in both a Python script and through the UI, detailing the required parameters and methods.",beginner,code,docs/platform/projects_manage.mdx
how to connect to an existing project Python,"To connect to an existing project in Evidently using Python, you can use the `get_project` method with the project's ID, as shown in the article.",intermediate,code,docs/platform/projects_manage.mdx
edit project description UI,"You can edit a project's description in the UI by hovering over the project and clicking on the 'edit' option, as explained in the article.",beginner,text,docs/platform/projects_manage.mdx
monitoring project parameters details,"The article details the parameters of a project including name, ID, description, and monitoring dashboard, providing examples and use cases for each.",advanced,text,docs/platform/projects_manage.mdx
how to organize projects in Evidently,"The article explains how to structure Projects in Evidently by use case, application, test scenario, and phase, providing best practices for managing data and evaluations effectively.",beginner,text,docs/platform/projects_overview.mdx
LLM tracing definition,"The article defines LLM tracing as a feature that enables you to instrument AI applications to collect data for evaluation, detailing inputs, outputs, and intermediate events like function calls.",beginner,text,docs/platform/tracing_overview.mdx
install tracely library,This article explains how to install the `tracely` package from PyPi using a simple pip command.,beginner,code,docs/platform/tracing_setup.mdx
init_tracing function arguments,"The article outlines the parameters required for the `init_tracing` function, including address, api_key, and project_id.",intermediate,text,docs/platform/tracing_setup.mdx
trace_event decorator usage,"You can use the `@trace_event` decorator to collect traces for specific functions, and the article provides examples of different ways to log function arguments.",beginner,code,docs/platform/tracing_setup.mdx
set attributes in trace events,The article discusses how to set custom attributes for events using the `set_attribute` method in the current span context.,intermediate,code,docs/platform/tracing_setup.mdx
create_trace_event context manager,"You'll learn how to use the `create_trace_event` context manager for tracing specific code sections without decorators, along with its parameters.",intermediate,code,docs/platform/tracing_setup.mdx
use environment variables with tracely,The article explains how to set parameters for tracing using environment variables instead of directly in the code.,beginner,code,docs/platform/tracing_setup.mdx
nested spans for multi-step workflows,"You can nest `@trace_event` calls to trace multi-step workflows, as illustrated in the examples within the article.",intermediate,code,docs/platform/tracing_setup.mdx
pass session_id for trace events,This article mentions how to use a shared `session_id` to link trace events from different functions or threads.,advanced,text,docs/platform/tracing_setup.mdx
trace structure in LLM workflows,"The article shows how different parts of a tracing workflow can be represented hierarchically, allowing for clearer trace structures.",advanced,text,docs/platform/tracing_setup.mdx
handle multiple datasets in tracely,"You can adjust the `as_global` parameter in the `init_tracing` function to manage multiple datasets in your tracing context, detailed in the documentation.",intermediate,text,docs/platform/tracing_setup.mdx
Evidently Cloud account setup guide,"This article explains the process of creating a free Evidently Cloud account, setting up an organization, and connecting to it using the Evidently Python library.",beginner,text,docs/setup/cloud.mdx
self-host Evidently UI setup,"The article provides step-by-step instructions on setting up the Evidently UI service for self-hosting, including creating a workspace and launching the UI.",beginner,code,docs/setup/self-hosting.mdx
create workspace for Evidently UI,"Instructions for creating a workspace in Evidently are included, specifying how to designate a local or remote directory for storing evaluation results.",beginner,code,docs/setup/self-hosting.mdx
Evidently UI local vs remote workspace,"The article explains the differences between local and remote workspaces, detailing how to run the UI service and where to store data in each scenario.",intermediate,text,docs/setup/self-hosting.mdx
launch Evidently UI service example,It offers code examples on how to launch the Evidently UI service using different options based on your workspace configuration.,beginner,code,docs/setup/self-hosting.mdx
delete workspace command Evidently,"The article provides a command to delete a workspace but warns that it will erase all associated data, emphasizing the importance of backing up reports before deletion.",intermediate,code,docs/setup/self-hosting.mdx
CI/CD LLM testing with Evidently and GitHub Actions,"The article explains how to use Evidently in conjunction with GitHub Actions to automate the testing of LLM outputs during code pushes or pull requests, ensuring any issues are caught early in the development process.",beginner,code,examples/GitHub_actions.mdx
LLM evaluation methods overview,"The article provides a comprehensive overview of different methods for evaluating LLMs, detailing both reference-based and reference-free approaches.",beginner,text,examples/LLM_evals.mdx
code example for LLM evaluation,"The article includes a code example that demonstrates setting up and running different LLM evaluation methods, which can be found in the provided Open Notebook link.",intermediate,code,examples/LLM_evals.mdx
how to create an LLM judge,"The article provides a step-by-step tutorial on creating an LLM judge using Python, focusing on setting up prompts for evaluating responses against reference answers and custom criteria.",beginner,code,examples/LLM_judge.mdx
evaluate responses with LLM,The article discusses methods to use an LLM for evaluating text responses either through reference comparison or custom criteria evaluation.,beginner,text,examples/LLM_judge.mdx
reference-based evaluation LLM,It explains how to compare new responses against a reference for regression testing using an LLM judge for better accuracy.,intermediate,text,examples/LLM_judge.mdx
how to set up LLM evaluator,"The process of defining the prompt and criteria for the LLM evaluator is detailed, including coding examples to implement it effectively in Python.",intermediate,code,examples/LLM_judge.mdx
using LLM with Python,The tutorial highlights the installation of the Evidently library and demonstrates how to interact with LLM via code examples.,beginner,code,examples/LLM_judge.mdx
conditional formatting LLM responses,"The article covers how to implement conditionals in an LLM judge to differentiate between correct and incorrect evaluations, enhancing accuracy.",intermediate,code,examples/LLM_judge.mdx
Evidently Platform integration,It describes how to upload the results to Evidently Cloud for better management and visualization of evaluation reports.,beginner,code,examples/LLM_judge.mdx
create evaluation dataset LLM,"The tutorial shows how to create a toy Q&A dataset to use for evaluating responses with the LLM judge, including coding examples.",beginner,code,examples/LLM_judge.mdx
next steps after LLM evaluation,The article suggests further integrating the LLM judge into broader evaluation workflows and tracking results using Evidently Cloud.,intermediate,text,examples/LLM_judge.mdx
correctness evaluator prompt,"Instructions on crafting the binary classification prompt for the correctness evaluator are provided, along with essential coding examples.",intermediate,code,examples/LLM_judge.mdx
LLM judge capabilities,The article reviews how an LLM judge can be customized to assess not only correctness but also response verbosity and other aspects of text evaluation.,intermediate,text,examples/LLM_judge.mdx
create and run an LLM judge,"It includes comprehensive steps on how to set up and execute an LLM judging process, aimed primarily at beginners eager to implement AI evaluation.",beginner,code,examples/LLM_judge.mdx
evaluate text accuracy with LLM,"The tutorial explains using LLM to score text accuracy against predefined criteria, focusing on the importance of manual labels.",intermediate,text,examples/LLM_judge.mdx
verbosity evaluator template,Details about constructing a prompt for evaluating text conciseness and its implementation in code are provided in the article.,intermediate,code,examples/LLM_judge.mdx
report generation with LLM,"Steps to generate a report summarizing evaluation results from the LLM judgments are included, allowing users to visualize performance metrics.",intermediate,code,examples/LLM_judge.mdx
manual labels in LLM evaluation,The article emphasizes the importance of using manual labels for reference to ensure effective evaluation by the LLM judge.,beginner,text,examples/LLM_judge.mdx
running LLM evaluations in Jupyter Notebook,It suggests using Jupyter Notebook or Google Colab for running the LLM evaluations to utilize rich visual outputs from the analysis.,beginner,code,examples/LLM_judge.mdx
best practices for LLM prompt crafting,"The article offers guidance on how to effectively craft prompts that maximize the evaluative ability of the LLM, targeting both correctness and quality.",advanced,text,examples/LLM_judge.mdx
LLM evaluation metrics insights,"It discusses various evaluation metrics such as precision and recall used to assess the performance of the LLM judge, aiding in fine-tuning.",advanced,text,examples/LLM_judge.mdx
examples of incorrect LLM responses,The article provides examples of how the LLM judge identifies incorrect responses versus correct ones based on the evaluation criteria defined.,intermediate,code,examples/LLM_judge.mdx
evaluate LLM outputs with multiple judges,"The article discusses how to use multiple LLMs to evaluate outputs, emphasizing the importance of consensus in determining if an output is a 'pass'.",beginner,text,examples/LLM_jury.mdx
setup multiple LLM judges for evaluation,"Instructions on setting up multiple LLMs as judges are provided, including API key configuration and evaluator setup.",beginner,code,examples/LLM_jury.mdx
Evidently installation for LLM evaluation,"To evaluate LLM outputs, the article outlines the installation process for the Evidently library using pip.",beginner,code,examples/LLM_jury.mdx
define evaluation prompt for LLMs,It describes how to use a BinaryClassificationPromptTemplate to create specific prompts for judging email appropriateness in evaluations.,intermediate,code,examples/LLM_jury.mdx
interpret LLM evaluation results,"The article explains how to interpret the results of evaluations, including methods to view and analyze the summary report from LLM judges after processing outputs.",intermediate,code,examples/LLM_jury.mdx
aggregate evaluation results from LLMs,"The approach encourages aggregating evaluation results for outputs, considering consensus among multiple LLMs for robust decisions.",beginner,text,examples/LLM_jury.mdx
custom descriptor for LLM disagreement detection,A method for creating a custom descriptor to flag and understand disagreements among LLMs in their evaluations is outlined for improved analysis.,advanced,code,examples/LLM_jury.mdx
run report on LLM evaluation metrics,"The article shows how to generate and run a report that summarizes overall evaluation metrics from multiple LLM judges, providing insights into outputs.",intermediate,code,examples/LLM_jury.mdx
examples of generated emails for evaluation,The article provides toy examples of user inputs and generated email outputs to illustrate the evaluation process by LLM judges.,beginner,text,examples/LLM_jury.mdx
RAG system evaluation metrics,"The article discusses key metrics to evaluate retrieval and generation quality in RAG systems, using the Evidently library for assessment and reporting.",beginner,text,examples/LLM_rag_evals.mdx
using Evidently for RAG evaluation,Learn how to utilize the Evidently library to assess the performance of RAG systems through metrics for retrieval and generation quality.,beginner,code,examples/LLM_rag_evals.mdx
retrieval quality assessment in RAG,"The article provides methods to evaluate the quality of retrieved contexts, including relevance assessments for both single and multiple contexts in RAG systems.",intermediate,text,examples/LLM_rag_evals.mdx
installing Evidently library,Instructions on installing the Evidently library via pip are provided to facilitate the setup for RAG evaluation.,beginner,code,examples/LLM_rag_evals.mdx
evaluating generation quality with ground truth,The article explains how to evaluate generated responses against a ground truth dataset using LLM-based metrics and comparisons in RAG.,intermediate,code,examples/LLM_rag_evals.mdx
dataframe visualization for evaluation,It explains how to visualize evaluation results as a pandas dataframe for better understanding of retrieval and generation scores.,beginner,code,examples/LLM_rag_evals.mdx
ContextQualityLLMEval implementation,Instructions to implement the ContextQualityLLMEval descriptor in Python code to assess context quality during evaluations are covered.,intermediate,code,examples/LLM_rag_evals.mdx
creating synthetic dataset for evaluation,"A step-by-step guide on generating a synthetic dataset for testing retrieval and generation in RAG systems is included, using simple example data.",beginner,code,examples/LLM_rag_evals.mdx
using FaithfulnessLLMEval,The article describes how to use FaithfulnessLLMEval to assess the faithfulness of generated responses in a RAG system without ground truth.,intermediate,code,examples/LLM_rag_evals.mdx
upload evaluation results to Evidently Cloud,Instructions on how to upload your evaluation results to the Evidently Cloud for tracking and interaction are provided.,intermediate,code,examples/LLM_rag_evals.mdx
Mean Relevance score calculation,It includes methods for calculating average relevance scores for multiple retrieved contexts when evaluating RAG systems.,advanced,code,examples/LLM_rag_evals.mdx
LLM judge metrics customization,Guidance on customizing LLM judge metrics to tailor evaluation definitions according to specific needs in RAG performance assessments is discussed.,advanced,text,examples/LLM_rag_evals.mdx
generating evaluation reports,The article illustrates how to generate comprehensive evaluation reports to summarize RAG assessment results effectively.,beginner,code,examples/LLM_rag_evals.mdx
regression testing for LLM outputs,"The article outlines methods for conducting regression testing specifically for LLM outputs, allowing for comparison between old and new responses.",beginner,text,examples/LLM_regression_testing.mdx
create toy dataset for LLM testing,It includes steps to create a small Q&A dataset that serves as a reference for testing LLM outputs.,beginner,code,examples/LLM_regression_testing.mdx
compare new and old LLM responses,The tutorial explains how to compare old and new responses by re-running the same inputs with different parameters.,beginner,code,examples/LLM_regression_testing.mdx
Python code for LLM regression testing,"The article provides specific Python code examples for implementing regression testing, including dataset creation and evaluation metrics.",intermediate,code,examples/LLM_regression_testing.mdx
how to run tests on LLM outputs,It includes instructions for creating and running tests to evaluate the correctness and style of new LLM outputs.,intermediate,code,examples/LLM_regression_testing.mdx
Evidently Cloud account requirements,A section of the article specifies that an Evidently Cloud account is necessary to track and upload test results.,beginner,text,examples/LLM_regression_testing.mdx
build dashboard for LLM regression results,The article explains how to create a monitoring dashboard to visualize the results of regression tests over time.,intermediate,code,examples/LLM_regression_testing.mdx
correctness judge for LLM outputs,It describes how to implement a correctness evaluator using LLM as a judge to validate the accuracy of new outputs against reference answers.,intermediate,code,examples/LLM_regression_testing.mdx
LLM regression testing metrics,"The metrics used for LLM regression testing include length, correctness, and style consistency, which are detailed in the tutorial.",beginner,text,examples/LLM_regression_testing.mdx
install Evidently for LLM,Instructions for installing Evidently via pip for use in LLM regression testing are provided in the article.,beginner,code,examples/LLM_regression_testing.mdx
new responses generation in LLM,The tutorial shows how to generate new responses from the LLM after initial prompt changes.,intermediate,code,examples/LLM_regression_testing.mdx
monitoring test results over time,It outlines how to track changes and improvements in regression test results over time using a dashboard.,intermediate,text,examples/LLM_regression_testing.mdx
Implementing LLM judge for style,"It covers how to create a custom judge for style checking in LLM responses, highlighting subjective aspects of evaluation.",advanced,code,examples/LLM_regression_testing.mdx
get reports from LLM testing,"Instructions for generating reports from testing results are provided, which can include both raw data and summary metrics.",intermediate,code,examples/LLM_regression_testing.mdx
alerts for LLM regression failures,"The article suggests integrating alerts to notify users when regression tests fail, enabling quick responses to issues.",advanced,code,examples/LLM_regression_testing.mdx
set up Python environment for LLM testing,"Basic setup instructions for a Python environment, including necessary imports and package installations, are provided.",beginner,code,examples/LLM_regression_testing.mdx
using LLM evaluation descriptors,It discusses the importance of using descriptors in regression testing to assess various aspects of the responses.,intermediate,text,examples/LLM_regression_testing.mdx
differences between response styles in LLM,"The article examines how to determine style consistency between the reference and new LLM responses, a key aspect of regression testing.",advanced,text,examples/LLM_regression_testing.mdx
performance metrics for testing LLM,A rundown of the performance metrics useful for evaluating LLM outputs during regression testing is presented.,intermediate,text,examples/LLM_regression_testing.mdx
connect Evidently Cloud with LLM,It provides code snippets on how to connect to Evidently Cloud to store and visualize test results from LLM outputs.,intermediate,code,examples/LLM_regression_testing.mdx
LLM evaluation quickstart guide,"The article features a quickstart guide for evaluating the quality of LLM outputs, aimed at newcomers to LLM evaluations.",beginner,code,examples/introduction.mdx
best practices for LLM evaluations,"The article outlines best practices for evaluating LLMs, including setup procedures and different evaluation techniques used across various examples.",intermediate,text,examples/introduction.mdx
integrating Evidently with GitHub,"It provides a walkthrough on integrating Evidently evaluations within a CI/CD workflow using GitHub Actions, showcasing native integration for regression testing.",beginner,code,examples/introduction.mdx
how to optimize LLM judge prompts,The article offers two tutorials on optimizing multi-class and binary classifiers using target labels and free-form feedback for LLM judge prompts.,intermediate,code,examples/introduction.mdx
tracking data drift in AI models,"Users can learn how to visualize Evidently data drift evaluations on a Grafana dashboard, providing insight into model performance over time.",beginner,code,examples/introduction.mdx
what is RAG evaluation,"The article introduces RAG evaluation and explains various metrics used for assessing retrieval-augmented generation systems, with practical examples.",intermediate,text,examples/introduction.mdx
ML metrics cookbook examples,"Includes a resource on various data and ML metrics such as regression, classification, and data quality, providing code examples for implementation.",beginner,code,examples/introduction.mdx
tutorial on classification evaluation methods,"The classification evaluation tutorial focuses on testing LLMs against a predictive ML baseline in multi-class tasks, with step-by-step guidance.",intermediate,code,examples/introduction.mdx
Evidently deployment steps,"The article features a tutorial on creating a workspace and project within Evidently, detailing steps for successful deployment.",beginner,code,examples/introduction.mdx
overview of LLM evaluation methods,"The article covers the anatomy of LLM evaluations, including reference-based and reference-free methods for comprehensive understanding.",intermediate,text,examples/introduction.mdx
visualizing LLM evaluations with Grafana,"It shows how to visualize LLM evaluation metrics with Grafana, leveraging Postgres as a database to enhance data interpretation.",advanced,code,examples/introduction.mdx
adversarial testing for LLMs,"Tutorials on running scenario-based risk testing on forbidden topics and brand risks for LLMs are provided, highlighting necessary precautions.",advanced,code,examples/introduction.mdx
Evidently Cloud v2 features,"The article outlines the major updates in Evidently Cloud v2, including a redesigned dashboard, improved performance, and better support for LLM evaluations.",beginner,text,faq/cloud_v2.mdx
Evidently 0.6 new API features,"The article highlights new API features in Evidently 0.6, including the introduction of the Report object, which can be imported from `evidently.future`.",beginner,text,faq/migration.mdx
How to migrate from old Evidently to 0.6,"It explains the necessary migration steps for users transitioning from previous versions to Evidently 0.6, including using legacy APIs during the transition period.",beginner,code,faq/migration.mdx
Evidently Dataset and DataDefinition usage,"The guide clarifies the usage of Dataset and DataDefinition in Evidently 0.6, emphasizing the need to explicitly create Dataset objects for reports.",intermediate,code,faq/migration.mdx
Example for running a Report in Evidently 0.6,"It provides example code for running a Report in Evidently 0.6, showing how to configure and execute it with a Dataset.",intermediate,code,faq/migration.mdx
Breaking changes in Evidently 0.7,The article points out that Evidently 0.7 will make the new API the default and explains how to import it for compatibility.,intermediate,text,faq/migration.mdx
Merge Reports and Tests in Evidently,"It discusses the merging of Reports and Tests in the new version, allowing for unified results in the same report output.",advanced,text,faq/migration.mdx
Simplified Metric object in Evidently,"The article explains the redesign of the Metric object in Evidently, highlighting how it improves JSON result parsing and UI integration.",advanced,text,faq/migration.mdx
Evidently Python library features,"The Evidently Python library includes data evaluation tools and generates reports, suited for individual data scientists and ML engineers.",beginner,text,faq/oss_vs_cloud.mdx
Tracely library real-time data,"Tracely allows users to capture near real-time data from AI applications and is built on OpenTelemetry, making it efficient for monitoring AI systems.",beginner,code,faq/oss_vs_cloud.mdx
Evidently Cloud vs Enterprise support,"Evidently Cloud offers managed services and support, while Enterprise requires internal management but provides extensive implementation assistance.",intermediate,text,faq/oss_vs_cloud.mdx
Open-source vs Commercial features comparison,"The article compares core features available in both open-source and commercial versions, highlighting premium features exclusive to the commercial editions.",intermediate,text,faq/oss_vs_cloud.mdx
Setting up Evidently OSS,"Setting up Evidently OSS requires managing the deployment and maintenance yourself, including backups and updates, whereas cloud options handle these aspects automatically.",intermediate,code,faq/oss_vs_cloud.mdx
Benefits of Evidently Cloud hosting,"Evidently Cloud simplifies deployment by managing infrastructure and offering automatic updates and security, making it ideal for teams focused on product development.",beginner,text,faq/oss_vs_cloud.mdx
Evidently telemetry data types,"The article outlines the various types of anonymous telemetry data that Evidently collects, including environment and service usage data.",beginner,text,faq/telemetry.mdx
How to disable telemetry in Evidently,Users can disable telemetry in Evidently by setting the environment variable DO_NOT_TRACK to any value after starting the service.,beginner,code,faq/telemetry.mdx
What is collected in Evidently telemetry,"Evidently collects basic environment data and anonymous service usage actions, helping developers enhance the tool based on user interaction.",intermediate,text,faq/telemetry.mdx
Is personal data collected by Evidently,"The telemetry system in Evidently does not collect any personal data, focusing only on anonymous usage data and system environments.",beginner,text,faq/telemetry.mdx
Evidently telemetry version 0.4.0,"The telemetry data collection in Evidently began with version 0.4.0, specifically when using the Monitoring UI.",beginner,text,faq/telemetry.mdx
Examples of telemetry event logs,"The article provides specific examples of telemetry event logs for actions like startup, indexing, and project management within Evidently.",intermediate,code,faq/telemetry.mdx
How to view telemetry collected by Evidently,The article does not specify viewing telemetry directly; it focuses on what is collected while using the tool and how it helps improve features.,advanced,text,faq/telemetry.mdx
Environment data collected by Evidently,"Evidently collects various environment data, including OS name, version, and Python version to analyze tool performance across different setups.",intermediate,text,faq/telemetry.mdx
Opting out of telemetry in Evidently,"Users can easily opt out of telemetry by modifying environment variables, ensuring they maintain control over their data sharing practices.",beginner,code,faq/telemetry.mdx
Impact of telemetry on Evidently development,Telemetry helps Evidently's developers understand user patterns and prioritize feature enhancements based on actual usage data.,intermediate,text,faq/telemetry.mdx
Evidently features overview,"The article outlines the comprehensive feature set of Evidently, including its modular approach, built-in evaluations, and collaboration tools designed for AI quality workflows.",beginner,text,faq/why_evidently.mdx
Open-source Evidently library benefits,"Evidently is an open-source library that offers transparency and an intuitive API, enabling users to trust the implementation of metrics and evaluations while benefiting from a thriving community.",beginner,text,faq/why_evidently.mdx
Built-in evaluations in Evidently,"Evidently provides over 100 built-in evaluations that cover various ML and LLM use cases, allowing users to quickly assess model performance without extensive preprocessing.",intermediate,code,faq/why_evidently.mdx
Integrating Evidently with existing tools,"The article explains how Evidently integrates seamlessly with existing tools, enabling users to export metrics and reports while adapting to their specific needs in AI development.",advanced,code,faq/why_evidently.mdx
evidently library tutorial,"The article introduces the Evidently library, offering a quick start guide and links to tutorials and examples for evaluation of AI systems.",beginner,code,introduction.mdx
LLM evaluations overview,"This article provides an extensive overview of evaluation methods for Large Language Models (LLMs), detailing several descriptors and their use cases.",beginner,text,metrics/all_descriptors.mdx
code examples for row-level evaluations,The article includes a link to a cookbook that serves as a reference for implementing row-level evaluations in code.,beginner,code,metrics/all_descriptors.mdx
ExactMatch code example,"The ExactMatch descriptor checks for exact matches between two columns and returns a boolean for each row, as shown in the detailed examples.",intermediate,code,metrics/all_descriptors.mdx
how to implement RegExp descriptor,"The RegExp descriptor allows for text matching against regular expressions, returning True/False based on defined patterns.",intermediate,code,metrics/all_descriptors.mdx
using BeginsWith in evaluations,"BeginsWith checks if text starts with a given prefix and returns True/False for each input, useful in specific content filtering scenarios.",intermediate,code,metrics/all_descriptors.mdx
EndsWith descriptor usage,"The EndsWith descriptor is used to verify if text concludes with a particular suffix, with clear usage examples provided in the article.",intermediate,code,metrics/all_descriptors.mdx
Contains descriptor details,"The Contains descriptor verifies if specific items exist within the text, returning True/False for presence checks, with examples outlined in the content.",intermediate,code,metrics/all_descriptors.mdx
DoesNotContain examples,"DoesNotContain evaluates the absence of specified items in text, providing True/False results, and is explained with practical examples.",intermediate,code,metrics/all_descriptors.mdx
how to create a custom descriptor,"The article explains the process of implementing a custom descriptor using Python, accommodating unique checks based on user-defined logic.",advanced,code,metrics/all_descriptors.mdx
IsValidJSON function description,"IsValidJSON checks whether a column contains valid JSON syntax, crucial for data validation in structured formats.",intermediate,code,metrics/all_descriptors.mdx
examples of JSONSchemaMatch,"The JSONSchemaMatch function validates JSON formatting against a specified schema, ensuring key/value compliance and structure integrity.",advanced,code,metrics/all_descriptors.mdx
TextLength descriptor usage,"The TextLength descriptor measures the character count in a text column, returning an absolute number for statistical evaluations.",beginner,code,metrics/all_descriptors.mdx
how to calculate sentiment using NLTK,"The sentiment descriptor analyzes text sentiment, scoring from -1 (negative) to 1 (positive), as detailed here.",intermediate,code,metrics/all_descriptors.mdx
ContextQualityLLMEval application,"ContextQualityLLMEval evaluates whether provided context is sufficient to answer a question, returning either a label or a score.",advanced,code,metrics/all_descriptors.mdx
NegativityLLMEval specifics,"NegativityLLMEval identifies negative texts, assisting in content moderation and analysis using a straightforward scoring system.",beginner,code,metrics/all_descriptors.mdx
SemanticSimilarity function details,"The SemanticSimilarity descriptor calculates cosine similarity between two text columns, useful for evaluating near-duplicate content.",intermediate,code,metrics/all_descriptors.mdx
BERTScore implementation,"The BERTScore method measures text similarity based on token embeddings, enhancing content comparison methods.",advanced,code,metrics/all_descriptors.mdx
using HuggingFace models in evaluations,"The article guides on scoring texts using user-selected models from HuggingFace, broadening evaluation capabilities.",intermediate,code,metrics/all_descriptors.mdx
how to detect PII in text,"The PIILLMEval descriptor identifies text containing personally identifiable information, pivotal for privacy checks.",intermediate,code,metrics/all_descriptors.mdx
what is the ContextRelevance descriptor,"ContextRelevance assesses the relevance of multiple context chunks to a given question, crucial for generating accurate responses.",advanced,text,metrics/all_descriptors.mdx
integrating custom columns in evaluations,"CustomColumnsDescriptor allows the application of checks on specific columns, enhancing flexibility in evaluations.",intermediate,code,metrics/all_descriptors.mdx
how to use IncludesWords in row evaluations,"IncludesWords verifies the presence of vocabulary words in text, returning results based on specified criteria, explained with examples.",intermediate,code,metrics/all_descriptors.mdx
word and character count descriptors,"The article outlines descriptors like WordCount and CharacterCount for gathering text statistics, essential for understanding data characteristics.",beginner,code,metrics/all_descriptors.mdx
DeclineLLMEval explanation,"DeclineLLMEval detects text instances where refusal or rejection occurs, important for analyzing user interactions with LLMs.",intermediate,text,metrics/all_descriptors.mdx
advanced example using CompletenessLLMEval,"CompletenessLLMEval checks if an LLM response fully utilizes the information provided in context, crucial for thorough evaluations.",advanced,code,metrics/all_descriptors.mdx
all metrics for dataset evaluation,"The article lists all metrics used for dataset evaluation, detailing their usage and parameters.",beginner,text,metrics/all_metrics.mdx
how to use TextEvals() in Evidently,It provides guidelines on using the TextEvals() metric which summarizes results from text evaluations and necessitates specifying descriptors.,beginner,code,metrics/all_metrics.mdx
examples of ValueStats() metric usage,"The article gives examples of how to implement the ValueStats() metric, detailing parameters required and default test conditions.",beginner,code,metrics/all_metrics.mdx
what are data quality metrics,The article defines various data quality metrics available for assessing column-level data integrity and quality within datasets.,beginner,text,metrics/all_metrics.mdx
parameters for MissingValueCount(),It outlines the parameters necessary for executing the MissingValueCount() metric and how it evaluates missing values in columns.,intermediate,code,metrics/all_metrics.mdx
how to analyze data drift,The article discusses metrics to analyze data drift and how these methods can be employed for text and tabular data.,beginner,text,metrics/all_metrics.mdx
using PrecisionTopK() for ranking evaluations,It explains how to implement the PrecisionTopK() metric to evaluate the precision at the top K retrieved items in recommendations.,intermediate,code,metrics/all_metrics.mdx
difference between RMSE and MAE,"The article distinguishes between RMSE and MAE as metrics, explaining how they assess prediction errors differently.",intermediate,text,metrics/all_metrics.mdx
conditional tests in metrics,It discusses the ability to add conditional tests to metrics with standard operators like 'eq' or 'gt' to enhance evaluations.,intermediate,code,metrics/all_metrics.mdx
data definition for metrics mapping,The importance of mapping column types as per the data definition is emphasized for accurate metric results.,beginner,text,metrics/all_metrics.mdx
examples of regression metrics implementation,The article provides examples for implementing various regression metrics like R2Score and MeanError.,beginner,code,metrics/all_metrics.mdx
key metrics for classification tasks,"It offers a review of crucial metrics suitable for evaluating classification task performance, including accuracy and precision.",beginner,text,metrics/all_metrics.mdx
how to interpret dataset stats,"The article explains how to interpret various dataset stats including row and column counts, and their significance in evaluations.",intermediate,text,metrics/all_metrics.mdx
examples for Dummy model metrics,"It gives examples of metrics used to evaluate the performance of dummy models, which serve as benchmarks for comparison.",intermediate,code,metrics/all_metrics.mdx
what are almost constant columns metrics,The article outlines metrics that identify almost constant columns in the dataset for better data quality checks.,advanced,text,metrics/all_metrics.mdx
guidelines for implementing MeanError(),It describes the steps and parameters needed for implementing the MeanError() metric in regression analysis.,intermediate,code,metrics/all_metrics.mdx
ColumnCount() metric usage,The article details how to use the ColumnCount() metric to determine the number of columns in a dataset.,beginner,code,metrics/all_metrics.mdx
parameters to specify for CategoryCount(),It outlines the required and optional parameters to use with the CategoryCount() metric for counting categorical occurrences.,intermediate,code,metrics/all_metrics.mdx
how to perform exploratory data analysis,The article introduces metrics applicable for exploratory data analysis and the respective commands to execute them.,beginner,text,metrics/all_metrics.mdx
how to check for empty columns,Instructions on how to utilize metrics to count and assess empty columns within a dataset.,intermediate,code,metrics/all_metrics.mdx
how to use the ClassificationPreset(),It explains how to implement the ClassificationPreset() for evaluating various classification metrics collectively.,intermediate,code,metrics/all_metrics.mdx
visualizations available for Precision(),The article lists available visualizations when utilizing the Precision() metric for classification accuracy evaluation.,advanced,text,metrics/all_metrics.mdx
parameters for DriftedColumnsCount(),It specifies parameters required for the DriftedColumnsCount() metric that calculates drift in specific dataset columns.,intermediate,code,metrics/all_metrics.mdx
outlier detection in regression metrics,The article addresses how to detect outliers and their impact on regression metrics like RMSE and MAE.,advanced,text,metrics/all_metrics.mdx
configuring test defaults for metrics,Explains how to configure test defaults when applying different metrics to appropriately assess data attributes.,intermediate,code,metrics/all_metrics.mdx
relationship between precision and recall in metrics,The article elaborates on the relationship between precision and recall metrics in the context of classification performance.,intermediate,text,metrics/all_metrics.mdx
guide to using F1Score() in classification,It presents guidelines on how to use F1Score() in evaluating the performance of classification models.,intermediate,code,metrics/all_metrics.mdx
benefits of using RecallTopK(),Describes the advantages of using RecallTopK() for assessing the effectiveness of recommendations in top K contexts.,beginner,text,metrics/all_metrics.mdx
parameters needed for NDCG(),The article specifies the parameters needed for the NDCG() metric focused on evaluating ranking effectiveness.,intermediate,code,metrics/all_metrics.mdx
criteria for evaluating almost constant columns,It outlines the criteria for evaluating almost constant columns through dedicated metrics in the dataset.,advanced,text,metrics/all_metrics.mdx
testing for missing values with MissingValueCount(),Instructions on how to test and evaluate missing values in datasets using the MissingValueCount() metric.,beginner,code,metrics/all_metrics.mdx
how to check for data drift in datasets,It provides a discussion on metrics for checking data drift within datasets and the methodology to employ them.,beginner,text,metrics/all_metrics.mdx
best practices for implementing regression metrics,The article advises on the best practices when implementing multiple regression metrics to analyze model performance.,intermediate,text,metrics/all_metrics.mdx
conditions for evaluating UniqueValueCount(),Specific conditions and parameters necessary for employing the UniqueValueCount() metric in column evaluations are discussed.,intermediate,code,metrics/all_metrics.mdx
how to visualize regression evaluation results,It discusses various visualizations available for regression evaluations and their implications on metrics performance analysis.,advanced,text,metrics/all_metrics.mdx
using MAPE() for model accuracy,The article elaborates on how to use MAPE() for assessing the accuracy of regression models.,intermediate,code,metrics/all_metrics.mdx
how to benchmark model quality with dummy metrics,It explains how to benchmark model quality against dummy metrics for validating model improvements and performance ratios.,intermediate,text,metrics/all_metrics.mdx
how to implement QuantileValue(),Instruction on how to use the QuantileValue() metric to compute quantile values for specified numerical columns.,intermediate,code,metrics/all_metrics.mdx
importance of mapping data types,The article emphasizes the significance of mapping data types accurately for obtaining correct evaluation results.,beginner,text,metrics/all_metrics.mdx
understanding regression performance with R2Score(),It discusses how to analyze regression performance in models using R2Score() as a determining factor.,beginner,text,metrics/all_metrics.mdx
conditions for evaluating logarithmic loss with LogLoss(),It outlines conditions that need to be fulfilled when utilizing the LogLoss() metric in classification evaluations.,intermediate,code,metrics/all_metrics.mdx
performing quality checks on datasets,The article summarizes various metrics that can be employed for performing quality checks on datasets.,beginner,text,metrics/all_metrics.mdx
available evaluation templates for datasets,"The article lists various pre-built evaluation templates aimed at simplifying the process of dataset assessment, including categories for text, data drift, and classification evaluations.",beginner,text,metrics/all_presets.mdx
data drift detection methods,"The article outlines various methods for detecting data drift, specifying how to select and customize them based on column types and thresholds.",beginner,text,metrics/customize_data_drift.mdx
custom parameters for data drift,You can override default drift detection methods and thresholds by passing custom parameters to metrics or presets according to your needs.,beginner,code,metrics/customize_data_drift.mdx
setting drift thresholds example,"The article provides examples for setting drift thresholds for different methods, including the PSI method for categorical columns.",intermediate,code,metrics/customize_data_drift.mdx
how to implement custom drift method,It details how to create a custom drift detection method by subclassing the StatTest class and registering it for use.,intermediate,code,metrics/customize_data_drift.mdx
data drift preset customization,You can create a DataDriftPreset with specific drift share and methods to customize data drift detection in your reports.,beginner,code,metrics/customize_data_drift.mdx
default data drift algorithm,"The default data drift algorithm automatically selects methods based on column types and volumes, as explained in the article.",beginner,text,metrics/customize_data_drift.mdx
value drift detection example,Example code snippets are provided for setting up value drift detection using specific methods for individual columns.,intermediate,code,metrics/customize_data_drift.mdx
dataset drift share setting,"You can define the share of columns that must be drifting to be flagged as dataset drift, with default settings adjustable in the metrics.",beginner,code,metrics/customize_data_drift.mdx
text data drift detection,"The article describes how to apply different methods for detecting drift in raw text data, along with specific threshold settings.",intermediate,text,metrics/customize_data_drift.mdx
pass custom drift methods,Instructions are given on how to pass custom drift detection methods and their thresholds using parameters in your reports.,intermediate,code,metrics/customize_data_drift.mdx
understanding PSI method for drift,It clarifies how the Population Stability Index (PSI) is used to determine drift in numerical and categorical datasets.,intermediate,text,metrics/customize_data_drift.mdx
column-level drift settings,You can specify drift detection methods and thresholds at the column level to observe specific behavior for each variable in your dataset.,advanced,code,metrics/customize_data_drift.mdx
drift share default value,"The default drift share value is set to 0.5, which can be changed to adjust sensitivity in detecting dataset drift.",beginner,text,metrics/customize_data_drift.mdx
StatTest class in data drift,The article explains the StatTest class for implementing custom drift detection methods effectively in data drift analysis.,advanced,code,metrics/customize_data_drift.mdx
threshold meaning in drift detection,"Thresholds in drift detection can signify different meanings depending on the method used, which is explained in detail.",intermediate,text,metrics/customize_data_drift.mdx
available drift detection methods list,A comprehensive list of available drift detection methods and their applicability is provided for user reference.,beginner,text,metrics/customize_data_drift.mdx
registering a custom StatTest,The article includes guidance on how to register a custom StatTest with required parameters and implementation details.,advanced,code,metrics/customize_data_drift.mdx
create custom text evaluator Evidently,"The article provides a step-by-step guide on implementing custom row-level text evaluators in Evidently, including necessary imports and example functions.",beginner,code,metrics/customize_descriptor.mdx
how to define CustomColumnDescriptor,It explains how to create a `CustomColumnDescriptor` that checks values in a dataset column and returns transformed scores or labels.,intermediate,code,metrics/customize_descriptor.mdx
examples of CustomDescriptor in Evidently,"The article includes examples of using `CustomDescriptor` to evaluate and transform multiple dataset columns, returning multiple scores.",advanced,code,metrics/customize_descriptor.mdx
HuggingFace text evaluation tutorials,"The article provides detailed guidance on using HuggingFace models as evaluators for text data, including step-by-step implementation examples.",beginner,code,metrics/customize_hf_descriptor.mdx
setup HuggingFace models in Python,"To set up HuggingFace models, you'll need to import specific descriptors and create a Dataset object as shown in the article.",beginner,code,metrics/customize_hf_descriptor.mdx
HuggingFace emotion classification,The article describes how to classify texts by emotion using HuggingFace models and includes example code for implementation.,beginner,code,metrics/customize_hf_descriptor.mdx
evaluate text using HuggingFace descriptors,"You can evaluate text using HuggingFace descriptors by adding them to your Dataset, as explained in the article.",beginner,code,metrics/customize_hf_descriptor.mdx
custom descriptors in HuggingFace,The article allows you to create custom checks as Python functions and integrate them using the general HuggingFace descriptor.,intermediate,code,metrics/customize_hf_descriptor.mdx
HuggingFaceToxicity evaluation example,"An example of using HuggingFaceToxicity to evaluate text for toxicity levels is provided in the article, including sample code.",intermediate,code,metrics/customize_hf_descriptor.mdx
parameters for HuggingFace models,The article outlines the required and optional parameters needed to use various HuggingFace models in your implementations.,intermediate,text,metrics/customize_hf_descriptor.mdx
using HuggingFace for zero-shot classification,Instructions for performing zero-shot classification using HuggingFace models are detailed in the article along with code snippets.,intermediate,code,metrics/customize_hf_descriptor.mdx
understanding HuggingFace score outputs,"The article explains how HuggingFace models output scores or labels, which is essential for interpreting the results of your evaluations.",advanced,text,metrics/customize_hf_descriptor.mdx
integrating different models with HuggingFace descriptor,"You can integrate various models with the HuggingFace descriptor, as the article describes, though you'll need to test for compatibility.",advanced,text,metrics/customize_hf_descriptor.mdx
using built-in LLM evaluators,The article details how to use built-in LLM evaluators in Evidently for tasks like toxicity detection and refusal evaluation.,beginner,code,metrics/customize_llm_judge.mdx
custom LLM evaluator setup,Learn how to create a custom LLM evaluator using provided templates to classify text based on specific criteria.,beginner,code,metrics/customize_llm_judge.mdx
evaluate text data with Evidently,Here's how you can evaluate text data using Evidently's LLM-based descriptors and built-in evaluators.,beginner,text,metrics/customize_llm_judge.mdx
examples of built-in evaluators,The article lists examples of built-in evaluators like ToxicityLLMEval and ContextQualityLLMEval available in Evidently.,beginner,text,metrics/customize_llm_judge.mdx
add descriptors to evaluation,You can add descriptors such as ToxicityLLMEval to your evaluation dataset by modifying the eval_df object.,beginner,code,metrics/customize_llm_judge.mdx
parameterize LLM evaluations,Learn how to parameterize evaluations by switching output formats and including or excluding reasoning in results.,intermediate,code,metrics/customize_llm_judge.mdx
switching to different LLM providers,The article explains how to change LLM providers and models for your evaluations beyond OpenAI.,intermediate,code,metrics/customize_llm_judge.mdx
LLM evaluation criteria templates,You can define custom evaluation criteria templates for classifiers within the Evidently library using prompt templates.,intermediate,text,metrics/customize_llm_judge.mdx
generate toy data for evaluation,Instructions are provided on creating toy data to form the basis for LLM evaluations in Evidently.,beginner,code,metrics/customize_llm_judge.mdx
OpenAI API key setup,The article provides guidance on how to set the OpenAI API key as an environment variable for LLM evaluations.,beginner,code,metrics/customize_llm_judge.mdx
use of BinaryClassificationPromptTemplate,Details on how to utilize BinaryClassificationPromptTemplate to classify text into two categories based on defined criteria.,intermediate,code,metrics/customize_llm_judge.mdx
evaluate context quality using LLMs,Here's how you can evaluate the context quality of responses against given questions using LLMs in Evidently.,intermediate,code,metrics/customize_llm_judge.mdx
insert context in evaluation criteria,The article shows how to include context in your evaluation criteria when creating custom LLM descriptors.,intermediate,code,metrics/customize_llm_judge.mdx
define multi-class classification,You can define multi-class classification using the MulticlassClassificationPromptTemplate to assess responses in various categories.,intermediate,code,metrics/customize_llm_judge.mdx
importance of concise responses,The article emphasizes the importance of concise vs. verbose responses in text evaluations using LLMs.,beginner,text,metrics/customize_llm_judge.mdx
LLM model options in evaluations,Information on how to specify different models and providers for LLM evaluations is discussed in the article.,advanced,code,metrics/customize_llm_judge.mdx
implementing additional columns in evaluation,You can use additional columns in your evaluations to provide more context for the texts being assessed.,intermediate,code,metrics/customize_llm_judge.mdx
Cross-provider tutorial for LLMs,The article references a cross-provider tutorial for using different external evaluator LLMs to expand evaluation capabilities.,intermediate,text,metrics/customize_llm_judge.mdx
importance of using aliases in evaluations,Aliases help define clearer names for evaluation columns in the output results of your LLM evaluations.,beginner,text,metrics/customize_llm_judge.mdx
API limits for LLM evaluations,Details regarding rate limits and usage constraints are provided when evaluating texts using LLMs in Evidently.,advanced,text,metrics/customize_llm_judge.mdx
Understanding output formats of evaluations,The article clarifies how to switch output formats in LLM evaluations to suit different result needs.,advanced,text,metrics/customize_llm_judge.mdx
Hiding reasoning in evaluations,You can configure your LLM evaluations to exclude reasoning in the results if only the classification label is desired.,advanced,code,metrics/customize_llm_judge.mdx
Common LLM evaluation errors,The article discusses some common pitfalls and errors that occur during LLM evaluations and how to avoid them.,advanced,text,metrics/customize_llm_judge.mdx
evaluating answers to questions,It covers how to evaluate answers based on their relevance to the questions they respond to using LLMs.,intermediate,code,metrics/customize_llm_judge.mdx
create custom metric in evidently,"The article outlines the steps needed to create a custom metric in Evidently, highlighting the required calculation method and optional test conditions.",beginner,code,metrics/customize_metric.mdx
using plotly for metric visualization,"You can define a custom visualization for your metric using Plotly, as explained in the article, and it provides an example of how to integrate this into your implementation.",intermediate,code,metrics/customize_metric.mdx
default tests for custom metrics,"The article provides details on setting up default tests for custom metrics, which can help ensure your metrics have the correct baseline evaluations.",intermediate,text,metrics/customize_metric.mdx
max value metric example,"An example implementation of a 'MyMaxMetric' is provided in the article, demonstrating how to calculate the maximum value from a column and incorporate it into a report.",advanced,code,metrics/customize_metric.mdx
classification accuracy explanation,The article explains classification accuracy as a standard metric that measures the proportion of correct predictions made by a model out of all predictions.,beginner,text,metrics/explainer_classification.mdx
how to interpret confusion matrix,"It describes the confusion matrix, which visualizes classification errors, allowing users to interpret the types of mistakes made by the model.",beginner,text,metrics/explainer_classification.mdx
code example for ROC AUC calculation,"The article provides guidance on calculating the ROC AUC metric, essential for evaluating the performance of classification models at various thresholds.",intermediate,code,metrics/explainer_classification.mdx
precision-recall curve implementation,It includes explanations of how to implement a precision-recall curve to analyze the trade-off between precision and recall for different classification thresholds.,intermediate,code,metrics/explainer_classification.mdx
F1-score definition,"The F1-score is defined as the harmonic mean of precision and recall, balancing the trade-off between the two metrics in classification tasks.",beginner,text,metrics/explainer_classification.mdx
visualizing class separation quality,"The article details how to visualize class separation quality through scatter plots, aiding in understanding model calibration and prediction accuracy.",intermediate,code,metrics/explainer_classification.mdx
using probability distribution for model evaluation,It explains the concept of probability distribution in classification to represent how predicted probabilities are distributed across different classes.,beginner,text,metrics/explainer_classification.mdx
classification quality by feature analysis,"The article describes how to analyze classification quality by feature, showing how model performance varies with different feature values.",advanced,code,metrics/explainer_classification.mdx
data quality overview widget,"The article describes a summary widget that provides an overview of the dataset, highlighting missing features and general statistics.",beginner,text,metrics/explainer_data_stats.mdx
feature widget visualizations,"It discusses the features widget, which generates visualizations for different feature types, including categorical, numerical, datetime, and text features.",beginner,code,metrics/explainer_data_stats.mdx
how to analyze missing data in features,"The article explains methods to analyze missing and empty features, detailing how to understand their impact on the dataset.",intermediate,text,metrics/explainer_data_stats.mdx
statistical summaries for categorical features,The article includes how to interpret statistical summaries specifically tailored for categorical features within the features widget.,beginner,code,metrics/explainer_data_stats.mdx
correlation between features in datasets,"It covers how to view and interpret correlations between different features in a dataset, using Cramer's v and Spearman's correlation methods.",intermediate,text,metrics/explainer_data_stats.mdx
visualizing feature interaction with target,"The article provides examples of visualizations that show how features interact with a target variable, enhancing understanding of relationships in data.",intermediate,code,metrics/explainer_data_stats.mdx
trends in data features over time,"It discusses how to visualize feature behavior over time, allowing users to spot trends and anomalies in their dataset.",advanced,code,metrics/explainer_data_stats.mdx
data drift detection examples,The article provides an overview of how to implement data drift detection using default methods and examples of metrics like `DataDriftPreset` and `ValueDrift`.,beginner,code,metrics/explainer_drift.mdx
how to check for data drift,It details the steps for comparing two datasets to check for data drift based on distribution changes in individual columns and explains statistical tests used for detection.,beginner,text,metrics/explainer_drift.mdx
understanding data drift metrics,"The article explains the key metrics used for detecting data drift, such as `DriftedColumnsCount`, and how they reflect changes in data distributions.",intermediate,text,metrics/explainer_drift.mdx
statistical tests for data drift,"It discusses various statistical tests for detecting data drift depending on column types and dataset sizes, such as the Kolmogorov-Smirnov and chi-squared tests.",intermediate,text,metrics/explainer_drift.mdx
how to handle empty columns in data drift,"The article advises checking for non-empty columns before computing drift, as empty columns will yield errors and can skew results.",beginner,code,metrics/explainer_drift.mdx
preferred algorithms for dataset drift,"It covers which algorithms are suitable for dataset drift detection based on data type and size, such as Wasserstein distance for larger datasets.",intermediate,text,metrics/explainer_drift.mdx
data drift detection for text data,The article describes how to detect drift in text data using a domain classifier and the ROC AUC score to assess differences in distribution.,advanced,text,metrics/explainer_drift.mdx
customizing drift detection parameters,It highlights how to modify drift detection methods and thresholds based on user-defined parameters and preferences for specific scenarios.,advanced,code,metrics/explainer_drift.mdx
understanding RecallTopK in recommendations,"The article explains RecallTopK as a measure of how well a recommender system retrieves relevant items within the top K results, detailing how to compute it by user and overall.",beginner,text,metrics/explainer_recsys.mdx
PrecisionTopK formula example,"PrecisionTopK indicates the proportion of relevant items among the top K results, with the article providing methods to compute this metric both at user level and overall.",beginner,code,metrics/explainer_recsys.mdx
how to calculate F Beta score in RecSys,"The article describes the F Beta score at K as a combined metric of precision and recall, highlighting its computation and interpretation based on the value of Beta.",intermediate,code,metrics/explainer_recsys.mdx
Mean Average Precision vs Precision,"MAP at K evaluates the ranking quality of relevant items more stringently than simple precision, with the article explaining how to calculate it using Average Precision.",intermediate,text,metrics/explainer_recsys.mdx
normalized discounted cumulative gain explained,"NDCG compares the quality of rankings to an ideal scenario, and the article provides a detailed method for calculating DCG and normalizing it to derive NDCG.",intermediate,text,metrics/explainer_recsys.mdx
compute Hit Rate for users,The article outlines how to determine the Hit Rate at K by assessing if each user has at least one relevant item in their top K recommendations.,beginner,code,metrics/explainer_recsys.mdx
MRR metric importance,Mean Reciprocal Rank (MRR) is crucial for measuring how well items rank for each user based on the position of the first relevant item; the article explains its calculation and significance.,intermediate,text,metrics/explainer_recsys.mdx
using entropy in score distribution,"The article describes how to compute score distribution entropy using predicted score transformations and KL divergence, emphasizing its role in understanding score distributions in recommendations.",advanced,code,metrics/explainer_recsys.mdx
metrics for evaluating recommender systems,"The article covers multiple metrics such as Recall, Precision, NDCG, and others, that serve to evaluate the effectiveness of recommender systems in retrieving relevant items.",beginner,text,metrics/explainer_recsys.mdx
implementing Recall and Precision in code,"The article provides implementation details for calculating Recall and Precision metrics, including code snippets to help practitioners apply these metrics to their systems.",intermediate,code,metrics/explainer_recsys.mdx
mean error definition in regression,"The article defines Mean Error (ME) as a standard metric for evaluating model quality in regression analysis, providing an overview of its calculation and significance.",beginner,text,metrics/explainer_regression.mdx
how to calculate absolute percentage error,It describes how to calculate the Mean Absolute Percentage Error (MAPE) as a key regression quality metric to quantify prediction accuracy.,beginner,code,metrics/explainer_regression.mdx
scatter plot for predicted vs actual values,"The article discusses the use of scatter plots to visually compare predicted values against actual values, highlighting their importance in regression analysis.",beginner,code,metrics/explainer_regression.mdx
error distribution analysis,"It details methods for analyzing the distribution of model error values, which helps identify patterns or biases in prediction errors.",intermediate,text,metrics/explainer_regression.mdx
error normality quantile-quantile plot,"The article explains how to use quantile-quantile plots to assess the normality of model errors, helping to understand error distribution characteristics better.",intermediate,text,metrics/explainer_regression.mdx
visualizing mean error per group,"It presents techniques for visualizing and summarizing mean error metrics for different segments of data, such as overestimation and underestimation groups.",intermediate,code,metrics/explainer_regression.mdx
how to interpret error bias by feature,"The article clarifies the concept of error bias per feature, illustrating how to analyze histograms to ascertain relationships between feature values and error rates.",advanced,text,metrics/explainer_regression.mdx
predicted vs actual per feature scatter plot,"It shows how to create scatter plots for each feature to visualize the predictions' performance, aiding in identifying segments that underperform.",advanced,code,metrics/explainer_regression.mdx
mean absolute error visualization,"The article guides on visualizing Mean Absolute Error (MAE) through interactive graphics, enhancing model performance assessment.",beginner,code,metrics/explainer_regression.mdx
classification preset examples,"The article provides code examples for using the ClassificationPreset to evaluate classification tasks on datasets, including how to run tests for quality assessment.",beginner,code,metrics/preset_classification.mdx
classification model comparison techniques,"It describes various use cases for the ClassificationPreset, particularly in comparing system performance across datasets during A/B testing or production monitoring.",intermediate,text,metrics/preset_classification.mdx
customizing classification report,The article outlines how to customize reports by modifying test conditions and adding extra metrics to evaluate model performance more granularly.,advanced,code,metrics/preset_classification.mdx
data drift detection methods,"The article explains various drift detection methods like PSI, K-L divergence, and others, which can be customized based on user needs.",beginner,text,metrics/preset_data_drift.mdx
how to run DataDriftPreset in Python,"To run the DataDriftPreset, you can use the provided code snippet to create a report and evaluate your current and reference datasets.",beginner,code,metrics/preset_data_drift.mdx
customize Data Drift Report,"The article outlines how to customize the Data Drift Report by changing drift detection parameters, selecting specific columns, and adding metrics for a comprehensive evaluation.",intermediate,code,metrics/preset_data_drift.mdx
importance of monitoring data drift,"Monitoring data drift is crucial for maintaining machine learning model performance, especially in environments without ground truth, as it indicates when retraining may be necessary.",intermediate,text,metrics/preset_data_drift.mdx
edge cases in data drift,"For advanced users, the article hints at possible edge cases in data drift evaluation, especially when custom drift detection methods are implemented.",advanced,text,metrics/preset_data_drift.mdx
data summary preset overview,"The article explains the Data Summary Preset, detailing its features for visualizing descriptive statistics of datasets and columns, enabling comparison between datasets.",beginner,text,metrics/preset_data_summary.mdx
data summary preset example python,"Code examples for using the Data Summary Preset in Python are provided, demonstrating how to run reports and add data quality tests with single or reference datasets.",intermediate,code,metrics/preset_data_summary.mdx
customizing data summary report,"The article discusses various customization options for the Data Summary Report, including selecting specific columns and modifying test conditions for a tailored analysis.",advanced,code,metrics/preset_data_summary.mdx
recommender systems getting started guide,"The article serves as a starting guide for using the Recommender Systems Preset in the Evidently API, outlining essential commands and structure for running evaluations.",beginner,text,metrics/preset_recsys.mdx
how to run RecSysPreset in Evidently,"The article includes Python code snippets for running the RecSysPreset, showing how to evaluate top-k recommendations on a dataset.",beginner,code,metrics/preset_recsys.mdx
customizing report for recommendation metrics,"Users can customize reports by changing test conditions and adding additional metrics to evaluate various aspects of recommendation systems, as detailed in the article.",intermediate,code,metrics/preset_recsys.mdx
regression quality preset overview,"The article outlines the Regression Quality Preset, explaining its functionality in evaluating regression tasks and visualizing results using various metrics.",beginner,text,metrics/preset_regression.mdx
how to run regression preset report,"It explains how to run a regression analysis report using the RegressionPreset class, demonstrating it with Python code examples.",beginner,code,metrics/preset_regression.mdx
customize regression report metrics,The article describes how to customize the regression report by changing test conditions and adding additional metrics for deeper analysis.,intermediate,code,metrics/preset_regression.mdx
text evals python example,"The article provides a code example for running the `TextEvals` Report in Python, showing how to compute evaluation metrics on a dataset.",beginner,code,metrics/preset_text_evals.mdx
text evals report customization options,"It outlines several customization options available for the `TextEvals` Report, including selecting specific descriptors and modifying test conditions.",intermediate,text,metrics/preset_text_evals.mdx
data requirements for text evals,The article specifies that the input dataset must include computed descriptors and can either be a single dataset or two for comparisons.,beginner,text,metrics/preset_text_evals.mdx
easily evaluate LLM outputs,"The article explains how to use Evidently to automatically evaluate LLM outputs with minimal setup, enabling faster iterations and confident decisions.",beginner,text,quickstart_llm.mdx
install Evidently library,"Instructions for installing the Evidently library using pip are provided, allowing users to set up their environment quickly.",beginner,code,quickstart_llm.mdx
run LLM evaluations in Python,"The document walks through running evaluations on LLM answers using Python, including setting up a dataset and applying various evaluation descriptors.",intermediate,code,quickstart_llm.mdx
create LLM evaluation report,It covers how to create and visualize reports summarizing LLM evaluation results within the Python environment using the Evidently library.,intermediate,code,quickstart_llm.mdx
examples of LLM evaluation descriptors,"The article lists various descriptors available for LLM evaluation, including sentiment analysis and text length checks, detailing how to apply them.",intermediate,text,quickstart_llm.mdx
setup environment for LLM evaluations,"For users wanting to run evaluations locally, the article describes the necessary environment setup, including cloud signup and project creation steps.",beginner,code,quickstart_llm.mdx
track LLM evaluation trends,"Instructions on creating dashboards to monitor LLM evaluation results over time are provided, helping users to visualize trends and set alerts.",advanced,code,quickstart_llm.mdx
customize LLM evaluators,Guidance on implementing custom LLM judges using predefined templates and criteria for specialized evaluations is covered in the article.,advanced,text,quickstart_llm.mdx
prepare dataset for LLM evaluations,"The article explains how to prepare a dataset for LLM evaluations, including an example of a toy chatbot dataset.",beginner,code,quickstart_llm.mdx
data drift detection in machine learning,"The article explains how to evaluate data drift using Evidently, highlighting its importance in maintaining model quality despite the absence of ground truth labels.",beginner,text,quickstart_ml.mdx
how to install Evidently library,"You can install the Evidently Python library using pip with the command `!pip install evidently`, as mentioned in the setup section of the article.",beginner,code,quickstart_ml.mdx
generate data drift report Python,"The article provides a step-by-step guide to generate a data drift report in Python using the Evidently library, which includes running a `Report` with the specified preset.",intermediate,code,quickstart_ml.mdx
evaluate prediction quality ML,"It covers various methods to evaluate prediction quality, including classification or regression accuracy checks, which are essential for monitoring ML system performance.",beginner,text,quickstart_ml.mdx
setting up Evidently Cloud environment,"The article details how to set up the Evidently Cloud environment, including necessary installations and creating projects for ML evaluations.",intermediate,code,quickstart_ml.mdx
advanced data drift customization options,"The article briefly touches on how to customize drift parameters in Evidently, allowing users to choose different methods and thresholds for their evaluations.",advanced,text,quickstart_ml.mdx
set up LLM tracing,The article outlines how to set up tracing for an LLM application using the Tracely library and Evidently Cloud to capture inputs and outputs.,beginner,code,quickstart_tracing.mdx
install Evidently and Tracely,You can easily install the Evidently and Tracely libraries using 'pip install evidently' and 'pip install tracely' as detailed in the article.,beginner,code,quickstart_tracing.mdx
view traces in Evidently Cloud,The article explains how to navigate to the Traces menu in Evidently Cloud to view and manage your captured tracing data.,intermediate,text,quickstart_tracing.mdx
using sessions with Tracely,"It details how to create distinct sessions for each input when tracing LLM interactions, storing unique IDs for better tracking.",intermediate,code,quickstart_tracing.mdx
advanced tracing configurations,"For advanced users, the article describes how to specify tracing dataset attributes and run detailed evaluations on the captured data.",advanced,text,quickstart_tracing.mdx
adversarial testing examples for AI models,"The article provides examples of adversarial tests designed to challenge AI models, including scenarios that generate harmful content or exploit forbidden topics.",beginner,code,synthetic-data/adversarial_data.mdx
how to create an adversarial test dataset,"The article outlines the steps to create an adversarial test dataset in Evidently, including selecting scenarios, configuring the dataset, and exporting the results.",intermediate,code,synthetic-data/adversarial_data.mdx
generate synthetic test inputs for AI,"The article provides a step-by-step guide on generating synthetic test inputs using the Evidently platform, allowing for tailored inputs that enhance test coverage for AI systems.",beginner,code,synthetic-data/input_data.mdx
generate synthetic test data examples,"The article provides examples of how to generate synthetic test inputs and outputs using Evidently Cloud, which can be used for various testing purposes in AI systems.",beginner,code,synthetic-data/introduction.mdx
creating RAG evaluation dataset guide,"The article outlines the steps to create a RAG evaluation dataset, emphasizing how to generate test cases from a knowledge base and refine them for accurate evaluation.",beginner,code,synthetic-data/rag_data.mdx
synthetic data for AI testing,"The article explains how synthetic data is crucial for testing AI systems by allowing for the rapid generation of diverse test cases, especially when real data is not accessible.",beginner,code,synthetic-data/why_synthetic.mdx
when to use synthetic data in LLM,"It details scenarios where synthetic data is beneficial, such as starting from scratch or needing to test complex or adversarial inputs during evaluations of LLMs.",intermediate,text,synthetic-data/why_synthetic.mdx
