{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d472b176-4eab-4199-bc95-02afba0bb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74c553df-2ddf-49b8-8cd5-67171c445503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_data(repo_owner, repo_name):\n",
    "    allowed_extensions = {\n",
    "        \"md\", \"py\", \"ipynb\", \"yaml\", \"yml\", \"Makefile\", \"Dockerfile\", \"toml\",# \"json\"\n",
    "    }\n",
    "\n",
    "    reader = docs.GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions,\n",
    "    )\n",
    "    \n",
    "    return reader.read()\n",
    "\n",
    "repo_owner = 'alexeygrigorev'\n",
    "repo_name = 'data-engineering-rag'\n",
    "\n",
    "files = read_github_data(repo_owner, repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a1507f6-77bf-4717-a4a5-e0a3f8159b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {}\n",
    "\n",
    "for f in files:\n",
    "    file_index[f.filename] = f.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f456184b-62d7-4e07-8fe2-33a6a8be9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert import MarkdownExporter, PythonExporter\n",
    "from nbconvert.preprocessors import ClearOutputPreprocessor\n",
    "\n",
    "class NotebookMarkdownFormatter:\n",
    "    \"\"\"Converts Jupyter notebook content to markdown format.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.exporter = PythonExporter()\n",
    "        self.exporter.register_preprocessor(ClearOutputPreprocessor(), enabled=True)\n",
    "\n",
    "    def format(self, raw_notebook: str) -> str:\n",
    "        nb_parsed = nbformat.reads(\n",
    "            raw_notebook,\n",
    "            as_version=nbformat.NO_CONVERT,\n",
    "        )\n",
    "        md_body, _ = self.exporter.from_notebook_node(nb_parsed)\n",
    "        return md_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bb6986b-1f60-456d-a4a3-8195368e6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_processor = NotebookMarkdownFormatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88c1cf20-0445-4dcb-88a1-743a013a06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {}\n",
    "\n",
    "for f in files:\n",
    "    content = f.content\n",
    "    if f.filename.endswith('.ipynb'):\n",
    "        content = notebook_processor.format(content)\n",
    "    file_index[f.filename] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "493c0fe7-4acd-4fff-9cab-69cfbc26a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class ProjectEvaluationTools:\n",
    "    def __init__(self, file_index: dict[str, str]):\n",
    "        self.file_index = file_index\n",
    "\n",
    "    def read(self, filename):\n",
    "        \"\"\"Return the contents of a file.\"\"\"\n",
    "        return self.file_index.get(filename)\n",
    "\n",
    "    def grep(self, patterns: list[str]):\n",
    "        \"\"\"\n",
    "        Search for all patterns in all files.\n",
    "        Returns: { filename: [matching lines] }\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        for filename, content in self.file_index.items():\n",
    "            lines = content.splitlines()\n",
    "            matches = []\n",
    "            for line in lines:\n",
    "                if any(p in line for p in patterns):\n",
    "                    matches.append(line)\n",
    "            if matches:\n",
    "                results[filename] = matches\n",
    "\n",
    "        return results\n",
    "\n",
    "    def tree(self, wd: str = '.'):\n",
    "        \"\"\"\n",
    "        Return a sorted list of files under the given directory.\n",
    "        \"\"\"\n",
    "        normalized = wd.rstrip('/') + '/'\n",
    "        if wd in ('', '.', './'):\n",
    "            normalized = ''  # root\n",
    "    \n",
    "        return sorted(\n",
    "            f for f in self.file_index.keys()\n",
    "            if f.startswith(normalized)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bba1033-cd9c-4298-8e9d-e361aac83a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tools = ProjectEvaluationTools(file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "873c6cdb-4709-45fe-a653-e096cbc0a7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md',\n",
       " 'agents.ipynb',\n",
       " 'data-processing.ipynb',\n",
       " 'debug.ipynb',\n",
       " 'evals/analysis.ipynb',\n",
       " 'pyproject.toml',\n",
       " 'zc_agent/__init__.py',\n",
       " 'zc_agent/eval/async_paralell.py',\n",
       " 'zc_agent/eval/calculate_metrics.py',\n",
       " 'zc_agent/eval/generate_questions.py',\n",
       " 'zc_agent/eval/run_agent.py',\n",
       " 'zc_agent/llm.py',\n",
       " 'zc_agent/load_data.py',\n",
       " 'zc_agent/logs.py',\n",
       " 'zc_agent/main.py',\n",
       " 'zc_agent/prepare_data.py',\n",
       " 'zc_agent/prompts/__init__.py',\n",
       " 'zc_agent/prompts/code_doc.md',\n",
       " 'zc_agent/prompts/eval_checklist.md',\n",
       " 'zc_agent/prompts/eval_question_generator.md',\n",
       " 'zc_agent/prompts/eval_user.md',\n",
       " 'zc_agent/prompts/notebook_edit.md',\n",
       " 'zc_agent/prompts/search_agent.md',\n",
       " 'zc_agent/search_agent.py',\n",
       " 'zc_agent/search_tools.py']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_tools.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1b4e0e5-5883-4197-bb4f-68354d520151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal, Optional\n",
    "\n",
    "class EvaluationItem(BaseModel):\n",
    "    points: int\n",
    "    description: str\n",
    "\n",
    "class EvaluationCriterion(BaseModel):\n",
    "    name: str\n",
    "    kind: Literal[\"single\", \"checklist\"]\n",
    "    items: list[EvaluationItem]\n",
    "    comment: Optional[str] = None\n",
    "\n",
    "class EvaluationCriteria(BaseModel):\n",
    "    criteria: list[EvaluationCriterion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c33f9289-b906-4370-8512-cad1d1089b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a85c85b4-5997-4936-a69f-bde08dc47373",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('criteria.yaml', 'r') as f_in:\n",
    "    criteria_raw = yaml.load(f_in, yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f503bfa-df1c-4fd1-9663-d09d4c350c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = EvaluationCriteria.model_validate(criteria_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f364387-efe1-49af-b25c-56e8856272e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationCriteria(criteria=[EvaluationCriterion(name='Dataset', kind='single', items=[EvaluationItem(points=0, description='The project uses the FAQ dataset from https://github.com/DataTalksClub/faq'), EvaluationItem(points=2, description='The project uses an original dataset')], comment=None), EvaluationCriterion(name='Data pipeline', kind='single', items=[EvaluationItem(points=0, description='No data processing pipeline'), EvaluationItem(points=1, description='Basic data loading with minimal processing'), EvaluationItem(points=2, description='Well-structured data pipeline with ingestion, processing, and indexing steps')], comment=None), EvaluationCriterion(name='Agent implementation', kind='single', items=[EvaluationItem(points=0, description='No agent code or agent code is embedded in notebooks'), EvaluationItem(points=1, description='Agent code exists but not well-organized or partially in notebooks'), EvaluationItem(points=2, description='Agent code is modular, reusable, and stored in separate Python scripts/modules')], comment=None), EvaluationCriterion(name='Agent evaluation', kind='single', items=[EvaluationItem(points=0, description='No evaluation of the agent'), EvaluationItem(points=1, description='Basic evaluation with simple metrics or manual testing'), EvaluationItem(points=2, description='Comprehensive evaluation with metrics, test cases, and quality assessment')], comment=None), EvaluationCriterion(name='User interface', kind='single', items=[EvaluationItem(points=0, description='No user interface'), EvaluationItem(points=1, description='Basic UI (simple script)'), EvaluationItem(points=2, description='Interactive UI (Streamlit, Gradio, or web application)')], comment=None), EvaluationCriterion(name='Code organization', kind='single', items=[EvaluationItem(points=0, description='Everything in one Jupyter notebook or unorganized scripts'), EvaluationItem(points=1, description='Code is split into multiple files but organization could be improved'), EvaluationItem(points=2, description='Well-organized codebase with clear separation of concerns (modules, functions, classes)')], comment=None), EvaluationCriterion(name='Reproducibility', kind='single', items=[EvaluationItem(points=0, description='No documentation on how to run the project'), EvaluationItem(points=1, description='Basic instructions provided but incomplete or unclear'), EvaluationItem(points=2, description='Clear setup instructions, dependency management, and easy to reproduce')], comment=None), EvaluationCriterion(name='Documentation quality', kind='checklist', items=[EvaluationItem(points=1, description='README has clear project goal and purpose'), EvaluationItem(points=1, description='README includes setup/installation instructions'), EvaluationItem(points=1, description='README provides usage examples or quickstart guide'), EvaluationItem(points=1, description='Project includes visuals (demo video, screenshots, or GIFs)')], comment=None)])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a730caf-70af-4f5c-8ee7-447314fa10d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
