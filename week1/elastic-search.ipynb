{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daed2ec8-8f7f-4b30-bc00-2774ba0ed117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20ae64c0-df56-444f-950e-7915fa4f2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the code we wrote in docs.ipynb is in the github.py file\n",
    "\n",
    "import github\n",
    "\n",
    "github_data = github.read_github_data()\n",
    "parsed_data = github.parse_data(github_data)\n",
    "chunks = github.chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caaf7488-973d-492c-a2bc-874430c32acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 2000,\n",
       " 'content': '\\n\\n- **Export scores** as JSON or Python dictionary.\\n- **As a DataFrame**, either as a raw metrics table or by attaching scores to existing data rows.\\n- **Generate visual reports** in Jupyter, Colab, or export as HTML\\n- **Upload to Evidently Platform** to track evaluations over time\\n\\nThis exportability makes it easy to integrate Evidently into your existing workflows and pipelines â€“\\xa0even if you are not using the Evidently Platform.\\n\\nHere is an example visual report showing various data quality metrics and test results. Other evaluations can be presented in the same way, or exported as raw scores:\\n\\n![](/images/concepts/report_test_preview.gif)\\n\\n**ðŸ“Œ Links:**\\n\\n- Quickstart for [LLM evaluation](/quickstart_llm) \\n- Quickstart for [ML evaluation](/quickstart_ml)\\n\\nOr read on through this page for conceptual introduction.\\n\\n**2. Synthetic data generation [NEW]**\\n\\n<Check>\\n  **TL;DR**: We have a nice config for structured synthetic data generation using LLMs.\\n</Check>\\n\\nPrimarily designed for LLM use cases, Evidently also helps you generate synthetic test datasets - such as RAG-style question-answer pairs from a knowledge base or synthetic inputs to cold-start your AI app testing.\\n\\n**ðŸ“Œ Links:** \\n- [Synthetic data](docs/library/synthetic_data_api) \\n\\n**3. Prompt optimization [NEW]**\\n\\n<Check>\\n  **TL;DR**: We help write prompts using labeled or annotated data as a target.\\n</Check>\\n\\nEvidently also includes tools for automated prompt writing. This features uses built-in evaluation capabilities to score prompt variations, optimizing them based on a target dataset and/or free-form user feedback.\\n\\nThis feature also help automatically generate LLM judge prompts to streamline the creation of custom evaluations.\\n\\n**ðŸ“Œ Links:** \\n- [Prompt optimization](docs/library/prompt_optimization)\\n\\n4. **Tracking and Visualization UI**\\n\\n<Check>\\n  **TL;DR**: There is also a minimal UI to store and track evaluation results.\\n</Check>\\n\\nThe Evidently library also includes a lightweight self-hostable UI for sto',\n",
       " 'title': 'Introduction',\n",
       " 'description': 'Core concepts and components of the Evidently Python library.',\n",
       " 'filename': 'docs/library/overview.mdx'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a9e6c7d-76d0-4976-a31c-ffc35db2f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c184017-cd81-4aec-a584-c895dd7942c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x296b9bbccd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0220bb3-da2c-429c-adaa-b40312f8b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search('how do I use llm-as-a-judge for evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50f430f7-9e03-481b-99e0-bd691f90a1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, weâ€™ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ',\n",
       " 'title': 'LLM as a judge',\n",
       " 'description': 'How to create and evaluate an LLM judge.',\n",
       " 'filename': 'examples/LLM_judge.mdx'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199017bd-3248-4565-9c81-0647f8f47f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
