0:00 hi everyone Welcome to our event this
0:02 event is brought to you by data talks
0:04 club which is a community of people who
0:05 love data we have weekly events this is
0:08 one of such events if you want to find
0:09 out more about the events we have there
0:12 is a link in the description so go there
0:14 in the description click on this link
0:15 and you'll see all the events we have in
0:17 the
0:18 pipeline uh we don't have yet a podcast
0:21 event in interview for the next Friday
0:23 but we will add this soon so keep an eye
0:25 and the best way for keeping an eye on
0:28 the content we have on this channel is
0:30 of course subscribing it to to this
0:32 channel so if you haven't subscribed to
0:34 the channel yet now it's the best time
0:36 to do this and we have an amazing slack
0:38 community so if you haven't joined it
0:40 yet join it to talk to other dat
0:43 enthusiasts during today's interview you
0:45 can ask any question you want so there
0:47 is a link in the in the live chat so
0:51 it's pinned click on this link ask your
0:53 question and I will be covering these
0:55 questions during the
0:57 interview and with that I I think
1:01 the intro is
1:04 over and I just want to make sure that I
1:06 pronounce your name correctly before we
1:08 start is it's
1:12 suprit suit okay it's uh always
1:16 interesting when uh with International
1:18 environments like how to pronounce the
1:21 name
1:22 correctly just one second okay so I
1:26 think I'm
1:27 ready um are you ready
1:32 yeah okay so this week we'll talk about
1:36 responsible and explainable Ai and we
1:39 have a special guest today suprit suprit
1:42 is um an assistant VP in data strategy
1:45 at Morgan Stanley Morgan Stanley Stanley
1:49 yeah and
1:51 um yeah so she's U also the founder of
1:54 databas so you'll probably tell us a bit
1:57 about that what it is uh she's a writer
2:00 she's a speaker and she loves
2:03 communicating um what she knows on
2:05 different platforms so it's a my
2:08 pleasure to have you today hi hello
2:11 thank you for having me on the
2:13 show so before we go into our main topic
2:17 of explainable and responsible AI let's
2:20 start with your background can you tell
2:22 us about your career Journey so
2:24 far sure so in 2017 I came to State reue
2:29 my m
2:30 I'm personally from India the capital
2:32 city I started my MERS of business and
2:35 science It's a combination of MBA and Ms
2:39 in data science a very unique degree um
2:42 and then I started my career in JN 2019
2:45 as a data science consultant uh I was a
2:48 data science and technology consultant
2:50 for three years before I decided to
2:51 switch gears a little bit and uh be more
2:54 involved in the data strategy um as I
2:57 realized the importance of that in my um
3:00 early career
3:02 years um after these few years of
3:05 chatting with so many ambitious people
3:08 who want to Pivot in this technology and
3:10 data science um I realized that there
3:13 was something that I could do out there
3:15 help everyone and hence I started the
3:17 community data busz to Mentor people who
3:20 want to Pivot in this field have had a
3:22 very unique career journey I would say
3:24 from a non-traditional non-engineering
3:27 background privating in this field um
3:29 and that's what's goal of a
3:33 databas well that's interesting we will
3:35 probably spend a bit more time about
3:38 that uh I'm quite curious to know more
3:40 about this I'm also interested in your
3:43 title so your title is um assistant VP
3:47 in data strategy so I'm really curious
3:50 what does it mean what do you do in your
3:52 day-to-day job yeah of course um so
3:55 Morgan Stanley is definitely a big firm
3:59 we have different divisions we have
4:00 Asset Management investing banking
4:02 wealth management I'm in the analytics
4:05 and data team for wealth management and
4:07 data strategy and products uh Team
4:10 basically owns different AI driven
4:13 products we are the product owners um
4:15 ensuring that the data strategy
4:18 streamlined um introducing new data
4:21 ideas into the streams um and obviously
4:25 creating different AI products helping
4:26 launch different AI products um in
4:29 Morgan Sly so yeah we work very closely
4:31 with data science make everything happen
4:34 so you more on analytical side of the
4:38 data product and
4:42 analytic and uh so yeah the topic today
4:45 is responsible AI so what is what is
4:48 responsible AI so how can AI be
4:51 responsible yeah um definitely you know
4:54 um it's it's a very interesting field
4:57 and it's a very butting field I would
4:59 say so so I've been in this regulated
5:01 environment I was in and I'm in finance
5:04 for um three years three plus years now
5:08 so I have definitely appreciated uh
5:11 responsible Ai and trustworthy Ai and as
5:13 the name suggests it's all about
5:15 developing algorithms and process so
5:18 that you can Empower your employee and
5:21 end users or customers right so there
5:24 have been numerous I would say studies
5:27 out there which say that your consumers
5:30 and customers if they trust you with the
5:33 outcome they they'll invest in you it's
5:35 as simple as that so it's basically a
5:37 collaborative way of working with other
5:40 stakeholders with keeping your end users
5:42 informed and be able to integrate
5:45 everyone's feedback ultimately making
5:47 this a very collaborative
5:51 process what does it actually mean so
5:53 does it mean that if they trust us um
5:57 then we are kind of responsible because
6:00 we don't want to violate this thrust or
6:02 what does it what does it mean to be
6:04 responsible here like we're responsible
6:06 for data for the decision that our
6:08 machine learning and the I systems make
6:10 right so and we don't want to lose this
6:11 trust that people put in us right
6:14 exactly exactly and you basically have
6:17 the right Tools in place right so that
6:20 all your stakeholders and users are
6:22 feeling confident about the decision so
6:24 if anyone asks you like how did you
6:26 arrive with this outcome you have some
6:28 tool you have something to show them
6:30 okay this is the step this is what my
6:33 algorithm followed and we were able to
6:35 derve at this decision so I would say
6:38 it's a set of tools and Frameworks to
6:39 empower
6:40 you so I'm just trying to think of an
6:43 example I don't know maybe can you use
6:45 some example from your uh uh from your
6:48 work or maybe something that you can
6:50 talk about um that where this is
6:53 important so you said that we want to
6:56 explain uh right so we want to feel
6:58 confident in this predictions so we want
7:00 to explain how they happen so we have
7:02 this trust so can you give an example of
7:05 course of course um so a few months back
7:08 was um an article that came out and that
7:12 there were there was a husband and a
7:13 wife both of them were eligible um for a
7:17 credit card with the same limit uh but
7:19 the wife ended up receiving a lower
7:21 limit uh than the husband and it was a
7:24 big lawsuit against the company uh
7:26 because she felt that somewhere AI was
7:29 biased towards her just because she was
7:31 a female and maybe the algorithm
7:33 ultimately assumed that she had a lower
7:36 income and gave her a lower limit credit
7:38 card so this is where I like if you have
7:42 the process in place you are able to
7:44 justify why wife got that limit and why
7:48 the husband got a certain limit and even
7:51 before that you are able to check did
7:54 your data actually have bias in it and
7:59 you can curb that um so that you can
8:01 produce a fair decision so there are two
8:04 two sides of the point I would say first
8:06 is your data side definitely we you have
8:08 to check uh in terms of accuracy and
8:11 fairness and then is your model
8:13 predictions because if your data is
8:14 biased no way that you're going to get
8:17 unbiased
8:18 predictions so what is the relationship
8:22 between responsible Ai and explainable
8:25 AI because from what I I hear now is if
8:28 you want to feel confident in the
8:30 predictions we need to be able to
8:32 explain them so does it mean that
8:34 responsible AI is explainable AI or
8:37 what's the connection there yeah so I
8:39 think uh definitely I will Deep dive
8:42 into explainable Ai and talk more about
8:45 that but ex responsible uh so
8:47 explainable AI is more like a postmortem
8:51 report I would say an incident has
8:53 happened and now okay what what do you
8:55 need to do okay let's build like a
8:57 simple algorithm justify the responsible
9:00 a is more like okay we have these
9:01 Frameworks in place and your starting
9:04 point or whenever you are building an
9:06 algorithm of project you have all of
9:08 these tools and Frameworks in mind right
9:11 so this is more of so that the incident
9:14 doesn't happen anymore we have these
9:17 tools and process in place so it's more
9:20 of mindset and explainable AI just gives
9:22 you um The Authority or I would say the
9:25 tools to build those Frameworks right
9:28 and I can leave dive into you know some
9:30 of the Practical techniques that you can
9:32 use to implement explainable AI in your
9:34 day-to-day
9:36 projects okay so what you're saying is
9:39 we should try to prevent these
9:40 situations like uh the example when
9:43 husband and wife got different uh credit
9:46 proposals right different loans um so
9:49 ideally we should not let this even
9:52 happen so we should catch all the bias
9:54 we have in our data and try to mitigate
9:56 it before we train our uh credit scoring
10:00 system right and then when the credit
10:02 scoring system is live then we should be
10:04 able to justify so if the vi in this
10:07 case comes to the to the financial
10:11 financial institution and asks why did
10:14 you give me lower loan like like the the
10:18 bank would just say Okay this this and
10:19 this is the reason right are the reasons
10:22 and then yeah but if we don't have that
10:25 then uh then we have a problem right
10:29 yeah
10:29 so how do we so you mentioned tools you
10:32 mentioned framework so how do we do this
10:35 yeah so uh this is where I you know I
10:38 think it's the right point where I can
10:41 explain some of the explainable AI
10:43 techniques right so it's what is
10:45 explainable AI ENT it's a framework that
10:47 can be integrated with your existing
10:49 machine learning model so that you can
10:51 understand the output of your AI or ml
10:54 algorithms right and as I said that this
10:57 is not only used to explain the results
10:59 behind a machine learning algorithm but
11:01 you can use it to receive feedback so
11:04 that you can retrain your model um you
11:06 can use it to detect bias in your data
11:10 um so it's called The Glass boox
11:13 approach U right as they say oh AI
11:16 machine learning is a black box but here
11:18 it's giving you transparency and it's
11:21 called the glass box glass box I heard
11:23 white box but then I was always thinking
11:25 wondering like is white really
11:27 transparent
11:29 is as intransparent as black right glass
11:33 box makes much more sense yeah so um I
11:38 will start with the data level right
11:40 like first when we talk about the data
11:41 level we talk about the fairness and
11:43 bias testing um there you have a few
11:46 data quality checks that I feel every
11:49 data scientist does uh they do some sort
11:51 of exploratory analysis dig into the
11:54 data see what they have um there as well
11:57 right like we can do a few checks one of
12:00 them is qess you could check how your
12:02 data looks like and it could be that you
12:05 are missing out one one population over
12:07 the other if your data is highly skewed
12:10 um right and other could be missing data
12:12 if you have too much missing data it's
12:14 important to analyze uh what are you
12:16 missing and you might be able to uh talk
12:19 to business stakeholders and get a sense
12:21 like are we missing out on an entire
12:24 population right so these are like some
12:27 simple checks that can be done on the
12:28 day side um to to ensure that your data
12:32 is not biased and you're covering a wide
12:35 array of
12:37 population so this is one of them right
12:39 and then on the model side you have
12:41 different techniques but I'm just going
12:43 to pause to see if there are any
12:44 questions or any comments on the data
12:47 side well there are there is a question
12:50 what tools do we use to check if there
12:52 is if there is any bias in data and I
12:55 think this is related to what you were
12:56 talking about so this is before we train
12:59 model right and you you told so you you
13:02 talked about Eda exploratory data
13:05 analysis you talked about SK skus
13:08 analysis missing data are there any
13:11 particular tools and techniques that we
13:13 can use for checking
13:15 this this is more like um you know this
13:18 is more on the Eda side right right here
13:20 you are not doing anything you're just
13:22 exploring the data honestly you're just
13:24 uh analyzing what's out there what does
13:27 my input look like and then when you are
13:30 when you actually dig deep into this
13:33 yeah I know it's kind of more of like a
13:35 conscious check plus um a technical
13:38 check um but yeah here this is where
13:40 you'll see right like what is happening
13:44 what what is happening exactly um and
13:46 then I would also say that there's this
13:48 bias checks right that you can do now
13:51 bias is very what do you it's a very
13:55 subjective term right bias can occur in
13:57 a lot of shapes and Farms um there is
14:01 there are there's a book called
14:02 trustworthy AI I don't know how many of
14:05 you have read where the author talks
14:07 about different types of bias and you
14:09 know she says that it can be like a
14:12 gambler's fallacy right which is the
14:14 probability of a random event which is
14:17 uping in the future is influenced by um
14:20 a past event right and that is what we
14:22 assume when we are building AI models
14:24 most of the time oh that my history is
14:26 the correct representation of my future
14:28 but that is not not always true and
14:29 that's why you need that human touch to
14:32 check for such biases in your data and
14:35 in your Mor so yeah so basically you
14:40 need to just as a human as an analyst or
14:44 as a data scientist you need to get your
14:47 data set from your database CV file
14:49 whatever and just spend enough time
14:52 trying to understand what's happening
14:54 there right and I guess uh like should
14:58 we watch out
14:59 for for anything particular like let's
15:03 say if we see columns like age and
15:05 gender should we already like think okay
15:08 like red flag should we do something
15:10 about these columns or how does it
15:12 usually look like yeah I mean I I would
15:15 actually say it depends right because
15:17 age and gender and those things are very
15:19 pii information sensitive information
15:22 most of the organizations wouldn't even
15:24 let you touch that those attributes uh
15:27 right especially if you're in a
15:29 regulated in so I'm not sure if you if
15:32 you would first of all even have access
15:34 to that kind of information to even make
15:36 this um judgment I would say but it
15:39 would be other factors you know it could
15:41 be the income of the person it could be
15:44 um the other factors about that person
15:46 of whatever Cas you are doing um that
15:49 can lead you um to make some I would say
15:53 assumptions predictions uh decisions and
15:56 which you check back with your business
15:59 stakeholders to see if they're even
16:00 valid or some subject matter expert on
16:03 the
16:04 DAT okay so you think this financial
16:07 institutions we talked about Banks they
16:09 don't have access to age and
16:11 gender um I depends right like which
16:15 organization are we talking about
16:17 everyone um you know Works in a
16:19 different way but like it's it's kind of
16:22 a recommendation right how you are
16:23 handling that pii information like in
16:25 Pharma world we never had access to all
16:28 of this uh data so yeah or even if we
16:31 have it you know we wouldn't use it like
16:33 we'll mask it out so yeah but in farm I
16:36 guess it's important right because uh
16:38 the way drugs work on people depends on
16:41 their age and depends on gender right so
16:44 there it is a justifi Justified use case
16:48 right while in risking like building a
16:52 credit risk profile maybe less so right
16:54 and then exactly ex and I don't think
16:57 even it's like a healthcare or Finance
16:59 institution question it's like a General
17:01 Industry thing if in your use case this
17:04 type of data is not required you
17:05 wouldn't use sensitive information again
17:07 this is also responsib right like
17:09 because you're handling the data
17:10 responsibly of a customer so yeah
17:13 definitely we we do our due diligence
17:15 like in determining do we even need this
17:17 data in the first place and how do we do
17:21 this is it usually done by this uh so we
17:24 talked about data checks right yeah
17:29 like to discuss this should we go into
17:31 the model checks to understand like do
17:33 we even need this
17:34 data yeah
17:36 like exactly exactly so when you are
17:39 kind of determining whatever use case
17:40 you want to solve you will def have a
17:43 detailed discussion with your product
17:45 managers business stakeholders and that
17:47 is where you will determine what data
17:49 you need and if it's sensitive
17:51 information and if you can mask it out
17:53 that is where it'll get you know out of
17:55 your data pipeline for your AI model so
17:58 you can use other
17:59 sets data sets so yeah well would it be
18:02 wise to completely just throw away
18:03 gender
18:04 data I mean say if
18:07 it's yeah it'll again you know depend on
18:11 the that's AAL answer but I I don't
18:14 think I can answer it any better um it's
18:17 again it depend on your use case what do
18:19 yourm say what does your compliance say
18:22 what does your model Review Committee
18:24 says before you decide on any how much
18:28 of this can be
18:29 automated like at least when we talk
18:32 about detecting biases in Data before we
18:35 come to
18:36 modeling yeah I think for all the
18:39 missing data the data skewness you can
18:41 always develop DQ tools I would say data
18:44 quality tools that can do all of these
18:46 simple tests for you um and tell you um
18:49 you know they can actually give you
18:50 those alarms I would say um and then you
18:54 can go there and investigate if there is
18:56 an alarm and you feel that an investig
18:59 is needed so most of this can be
19:01 automated MH so what about this model
19:04 part yes so for model also uh I think a
19:09 few years back we didn't have as many
19:11 open- Source Technologies but now we
19:13 have a lot of those um right like Google
19:16 IBM everyone has come with such good
19:19 open source technologies that can easily
19:21 be fetched into your python um pipeline
19:24 or your pypar whichever tool you are
19:27 using there is a what of tool by Google
19:30 it's almost an interactive tool where
19:31 you can understand the data it's it's um
19:35 it gives an interface of the dashboard
19:37 so you can basically like fetch in all
19:39 your parameters see what's the working
19:41 behind your machine learning model
19:43 especially if you're using a tensor flow
19:46 which is again uh you know by Google so
19:48 what if tool is a great tool to use then
19:52 we have other tools as well like we have
19:55 skater it's an open source python
19:57 Library uh that was again designed so
20:00 that you can see some of the model
20:02 functioning now how called can you again
20:05 say how like the second Library I didn't
20:08 hear the name skater so it's SK t r yeah
20:13 skater yeah that that is another tool
20:16 that kind of helps you to see the
20:18 functioning behind your machine learning
20:20 model um now the trick is obviously how
20:23 you will integrate in your use case will
20:26 again depend on uh what you're trying to
20:28 achieve right you might not find all of
20:31 these tools um which can be integrated
20:35 with your use case so there it's more of
20:37 a exploration again and then there's
20:40 another one a 360 by IBM um and that is
20:45 another amazing Source uh open source
20:48 toolkit uh which you can use to
20:50 comprehend um your predictions and your
20:53 machine learning
20:54 model so this one is um so I'm just
20:58 trying to understand so what if to me if
21:01 I understood it correctly this is uh you
21:04 can see this as another exploratory data
21:06 analysis tool but smarter than the usual
21:10 analytics right so smarter than just
21:12 counting you know missing values and
21:15 then you can see uh like what kind of
21:18 questions you can ask us it what if I
21:20 remove this column or what if I had this
21:22 data or what kind of what if questions
21:25 can you ask it um again right it's more
21:28 like like if you want to also see like
21:30 the importance of some of the features
21:32 if you want to see okay what if I remove
21:33 this feature again like how will my
21:35 module get impacted if you just want to
21:38 see how your data looks like like as I
21:40 said like the skewness and everything um
21:43 it'll easily allow you to do a plot and
21:46 that'll be very Dynamic you can do it
21:48 over time over month over years uh
21:50 whatever use case you have in hand so
21:52 all of those things right and if you
21:55 were using tensor flow then how did you
21:57 arrive at that particular ition right
21:59 what if you remove the feature how will
22:01 impact your model how will it impact
22:03 your accuracy so all of those
22:06 things and then Scutter like does it do
22:09 something similar to that it's similar
22:12 but it's it's not just a
22:14 t more
22:16 broad uh it's more on other models if
22:20 you want to use um and it's basically
22:23 you know you can do it based on your
22:24 complete data set so it's always
22:27 explainable AI uh whenever you are doing
22:29 a you're using any framework right like
22:31 the idea of that is that if I were to
22:34 apply this on a larger data set how will
22:37 the impact look like right but whenever
22:39 you build a explainable AI technique you
22:41 basically take a small data set and that
22:44 obviously there's an assumption that
22:45 this is a true representation of my
22:47 bigger data set and then you build
22:49 around that so skater will facilitate
22:51 some of those techniques for you mhm and
22:56 then the last one you mentioned is a iix
23:00 360 from IBM right so this is my
23:04 understanding was that it's used for
23:06 interpreting the output the predictions
23:09 of the models or not only yeah and
23:11 actually all of these can be used right
23:13 for that I mean ultimately you're using
23:15 all of this it's just that um if you're
23:17 not using tensor flow if you're using
23:19 other open source packages they'll help
23:21 you facilitate that mhm I've heard about
23:24 tools like U lime and shapely value and
23:29 are these somehow related to that too so
23:32 I these are the ones that I just spoke
23:33 about are more of Open Source python
23:37 packages I would say now if you actually
23:39 want to build things right like from
23:42 scratch the way you'll actually build
23:44 any algorithm so that's where kind of
23:46 lime comes into play lime and then lime
23:49 has so lime is also I would say it's a
23:53 it's a combination of different
23:54 techniques right like you can do a
23:55 linear regression model or you can do a
23:57 decision treat model and that is
24:00 basically are building a line model um
24:03 you might be actually you might be doing
24:05 a neural network model but now you just
24:07 want to be able to interpret that so you
24:10 again take a small data set and build a
24:11 decision three on that to come close to
24:14 what the interpretations would look like
24:16 so that you're able to explain it to
24:17 your stakeholders so yes lime is very
24:20 similar to
24:21 that then I wondering so I'm quite
24:24 interested in this uh so let's say there
24:27 are some
24:29 um some features that we shouldn't use
24:31 because they are pii personally
24:33 identifiable information like gender age
24:37 but in some
24:39 cases I kind of wish we could use them
24:41 so for example in in
24:43 advertisement um maybe there is a
24:45 product that would be more appealing to
24:47 females than males and then um um like
24:52 how can I if I decide to be responsible
24:56 and uh you know trust for
25:00 then I should remove this feature right
25:03 but I know that this feature is actually
25:05 important if I want to be profitable and
25:08 then I have a dilemma right so do I want
25:10 to be Prof profitable do I want to make
25:12 money or do I want to be responsible and
25:16 you know remove bias from data like do I
25:19 want to get rich exploiting this biases
25:22 or not right so how can we still be
25:26 profitable but without you know
25:28 exploiting all these biases yeah and I
25:32 don't think um you know all these
25:34 decisions are not by one person and
25:35 definitely they are not by data science
25:37 practitioners um and that is what
25:39 responsible AI is right it it's a very
25:41 collaborative process that's why they
25:43 say it because when you kind of have 10
25:46 different people in a room discussing
25:48 such use cases and having 10 different
25:50 perspectives you will be able to come up
25:53 um I would say you'll be able to meet
25:55 halfway right and that is where you'll
25:57 be able to decide the priorities of what
26:00 you really want to do and how you can
26:03 achieve it and that is exactly what we
26:06 aim to do it's beyond these techniques
26:09 that I just mentioned yeah so yes but
26:13 then I guess for this uh for this one
26:15 let's say if there are 10 people who has
26:18 different perspective they should be
26:20 different people right so it's not like
26:23 uh all data scientists all white males
26:26 right so then they probably would think
26:28 okay like there's nothing wrong in using
26:30 gender let's just use it right no no no
26:32 definitely I I meant like different
26:34 representation from different groups
26:36 because data scientists think
26:37 differently right and project managers
26:40 will think differently now you add
26:41 compliance people they'll think
26:44 differently so that's what and that is
26:46 why like launching an AI product and
26:49 doing all of this responsibly is
26:52 definitely uh it's a
26:55 journey yeah because the reason I'm
26:57 mentioning this is uh in many courses
27:00 and also the courses we do at data talks
27:02 Club there are data sets there are some
27:04 problems where we do actually use age
27:08 and gender like for example like for I
27:11 think actually in the course we have
27:13 right now there are two projects in
27:15 which we use age and gender right and
27:17 then like you might think as a data
27:18 scientist oh there's nothing wrong right
27:20 so what to talk about here let's just
27:23 use this right yeah exactly I we would
27:26 never know like for us it's like it's
27:27 all features so this is important that's
27:30 what our algorithm is saying okay we
27:32 don't really think in that direction so
27:34 yeah but yeah it's
27:37 important so what who what kind of
27:39 people we need to have in this room to
27:43 be able to have this fruitful discussion
27:45 so you said we need perhaps data
27:47 scientists analysts and people from
27:49 compliance um who else we should your
27:53 your subject matter experts who
27:55 understand the data better um then you
27:57 can have have your um yeah and your your
28:00 senior leadership right ultimately um as
28:03 well so yeah and I I think uh obviously
28:07 it's not that it's going to be it's easy
28:10 and you you know get get an answer in
28:12 one meeting and oh yeah we we you know
28:14 decide to come at a conclusion I think
28:16 at that time you'll be thinking about
28:18 some Innovative workarounds right okay
28:20 let's use this one feature over this
28:22 feature and let's see how it performs
28:24 let's drop this feature uh but at the
28:26 end of the day it's possible that the
28:28 use case that you have in hand like
28:30 gender is one of the most important uh
28:33 feature right uh and then then again
28:35 it's a company's decision to make mhm
28:38 and then I guess uh like who is making
28:41 this decision is it uh senior leadership
28:45 or collaboratively everyone but then if
28:48 everyone is responsible then nobody
28:51 is I don't think so I think those
28:54 discussions are very productive
28:55 obviously everyone's responsible but at
28:57 the same time in a business and you want
28:59 to make money okay yeah but then at the
29:03 end what happens I guess you come up
29:05 with some sort of terms of service and
29:07 then you say okay this is the data we
29:09 collect we collect gender and if you
29:12 don't agree with this you don't use the
29:14 up or how does it usually work yeah I
29:15 mean obviously like it's also about kind
29:17 of um I think the way we do those
29:19 declarations and the way we sign all the
29:22 terms and agreements you know where
29:23 somewhere um the customer kind of might
29:26 give you the consent that okay yeah just
29:28 you know use my data and some customers
29:30 might not uh give you the consent so
29:34 that that's also very important
29:35 component mhm but I'm still thinking
29:38 about that case that you that example
29:41 where the wife and the husband perhaps
29:44 both of them gave consent to using their
29:46 gender information and then at the end
29:49 this is what happened so the company
29:50 still got suit right even though they
29:53 had probably paperwork in place yeah I
29:55 mean I I'm not sure like you know
29:56 obviously I read it in a news newspaper
29:58 so I don't know you know whether the
30:00 company was able to justify that
30:02 decision or not because we obviously get
30:04 the news um you know it's from a third
30:08 party right so you don't you don't
30:09 really know the facts but that's what
30:11 kind of it made me think that uh was it
30:14 able to justify right like if if the
30:16 consumer is questioning you are you able
30:18 to justify okay what led to this
30:21 difference in decision making and can
30:23 you say that it was beyond gender can
30:25 you say it was not bias right that you
30:27 can only say if you've already done your
30:29 due diligence before see that there's no
30:32 bu in your data set MH and if you found
30:36 out that there is actually bias in your
30:37 data set and uh somebody's already suing
30:40 you that's too late right yeah
30:44 exactly see yeah yeah so what we should
30:49 have is uh per my understanding so we
30:52 should have we should analyze our data
30:55 right so we should have a human who
30:56 actually goes through the data said some
30:58 things could be automated right but then
31:01 at the end a human should actually go
31:03 through this and play um do some
31:06 analytics right then after that uh there
31:09 are techniques uh that can automate this
31:13 like all this what if uh tools and
31:16 similar like was it scatter scatter
31:19 right skater yeah and then um like all
31:23 these other tools um so that should be
31:26 used by I guess unlist and data
31:29 scientists to make sure that this
31:31 actually doesn't happen yeah and then
31:34 also at the end we want to be profitable
31:37 but at the same time we want to be
31:39 trustworthy and responsible so then we
31:42 should have this meeting with people
31:45 from compliance from data analytics from
31:49 uh subject matter experts from senior
31:51 leadership who get together and discuss
31:54 um the case right am I missing anything
31:57 no no and I think U like we also kind of
32:00 highlighted the challenges on the way
32:02 right like this is one of the challenge
32:04 as you said like profitability versus
32:06 being responsible and then um we have
32:10 accuracy versus interpretability because
32:13 for all of your complex models you still
32:15 might not be able to make an explanate
32:19 explainable AI model right so all of
32:22 those are I feel like still the
32:23 challenges and pitfalls um of
32:26 explainable AI and respons
32:28 yeah and actually we have a question
32:30 from Shivam that is exactly about that
32:33 so the question is like how to manage
32:36 the trade-off between model complexity
32:38 and explainability because the complex
32:40 models um do not necessarily have good
32:43 explainability so how do we manage that
32:45 exactly yeah and that's that's again as
32:48 I said like that's kind of one of the
32:49 challenges and in some of the use cases
32:52 and I think it's like a tradeoff right
32:54 like as data scientists we uh face other
32:57 tradeoff off like you know we have this
32:58 bias versus variance tradeoff so this is
33:00 one of those things where you need to
33:03 again analyze what you're trying to
33:06 achieve who you're trying to Target what
33:09 is your end goal what do you want to
33:11 achieve what does your business
33:12 stakeholders want to achieve and if we
33:15 were to take a step back on accuracy
33:18 will that help us in long term right
33:20 like will our consumers trust in us more
33:23 and at the end of the day you're also
33:25 trying to build a brand it might be a
33:27 bank or it might your Healthcare or
33:28 whatever right you want people to
33:30 indulge uh with you in some sort in some
33:34 form or another um so yeah it's one of
33:36 those
33:37 Crossroads I would so if you I guess if
33:42 you make a decision to prefer accuracy
33:45 versus explainability you should be
33:48 prepared that eventually um like
33:50 somebody from Senior Management like I
33:52 don't know Mark Zuckerberg will have to
33:54 talk in front of Congress and explain
33:56 things right
33:58 if you don't want this to happen then
33:59 maybe you
34:02 don't hope this doesn't happen but yes
34:05 um and and it's it's not that every time
34:07 you will be choosing like okay let me be
34:10 make the most interpretable models right
34:12 let me make the most so every time you
34:14 are encounter a use case your decision
34:16 making will be different and that's why
34:18 it's important to not make all of these
34:20 decisions at so
34:25 invols yeah because I imagine that if if
34:27 you let's say use linear or logistic
34:29 regression for everything then it will
34:31 be like I know um not so biased very
34:35 explainable but then accuracy could uh
34:37 suffer right but then if you go with
34:39 deep learning then it's the other um
34:42 like it's the opposite the other side of
34:43 the spectrum then you have little
34:46 explainability but a lot of accuracy and
34:48 then so what I understand is like you
34:51 just need to keep
34:52 the long long-term goal in mind of the
34:55 company and ask our El like are we ready
34:59 to sacrifice a bit of profit uh I'm not
35:03 like are we ready to sacrifice our image
35:05 with some extra profit right is okay if
35:08 people start talking bad about us right
35:11 or we're an evil Corporation and it's
35:14 fine to um you know store all the data
35:18 about you and then sell it to
35:22 others not pointing fingers
35:28 yeah the question there is a question
35:29 from Raquel is what does um un need a
35:32 human touch mean I think this is related
35:35 to our discussion that the first step is
35:37 always like a human analyzing the
35:41 data and then the question go on is does
35:44 this mean that responsible Ai and
35:47 checkpoints and alarms cannot be
35:49 automated so maybe we can think of what
35:53 can be automated and what cannot not be
35:55 aut right yeah as I said right like all
35:58 the all your DQ checks and you know you
36:02 can be so creative you can write pieces
36:04 of codes and you can do all the checks
36:06 on the world but the end of the day you
36:08 just cannot be like oh my DQ checks you
36:13 know they passed I don't really need to
36:15 you know look at it and see what's
36:17 happening um as your data keeps evolving
36:21 your tools and all your even all your
36:23 production models need to get evolved
36:26 right and often as data science you hear
36:28 oh we are having drift in our model and
36:32 drift can also be in you know in this
36:34 form right it can be in form of bias as
36:37 well you didn't have bias Data before
36:39 now you have the bias data so how if you
36:42 can build adoptable techniques
36:44 definitely do that but at the end of the
36:46 day there has to be someone who can go
36:48 in analyze all of those and you don't
36:51 obviously have to go row by row you know
36:54 but in a cumulative fashion see what's
36:57 happening with your data and your uh
36:59 model
37:00 performance so yeah that that's what I
37:03 mean by human touch but at the end of
37:05 day it's a human who's going to write
37:06 the code as well to make that happen Soh
37:10 so human automates it right so we're
37:12 automate but the end it's the human who
37:14 is doing the automation right and then
37:16 they need to put some thoughts in this
37:19 process not just blindly take a tool
37:22 from I don't know Google or who knows uh
37:24 what company and just put it there and
37:27 forget about this right exactly exactly
37:30 okay and to your point um like about
37:33 drift that we should monitor for bias
37:37 there is a comment that um so for
37:39 example um a comment from abishek so if
37:43 data talk Club model sees more males
37:45 attending the events females will not
37:48 will not get recommended the event and
37:50 this can introduce a feedback loop that
37:53 next time the model will recommend the
37:56 events only to males right and we will
37:58 ignore females and then this is
38:00 something like it happens gradually so
38:03 at the beginning maybe it's uh you know
38:06 equal but then there is like more and
38:08 more and more and more feedback loop and
38:10 then at the end it's only males
38:13 attending the events right yeah so are
38:16 there tools that can help detect this
38:19 kind of drift drift that uh at this kind
38:23 of bias that gets into your model into
38:26 your data because of this feedback books
38:28 yeah so I feel like if your model is in
38:30 production you are already using some
38:32 sort of tool to monitor your model's
38:35 performance uh but you might just be
38:37 measuring your performance in terms of
38:39 accuracy now you also need to build uh
38:42 some sort of monitoring in terms of the
38:44 population samples if it's a male or a
38:47 female you already if you already have a
38:49 unique identifier in your data set how
38:53 is my population drifting so basically
38:55 all the DQ checks that you did at the
38:56 first uh you know during your input
38:59 phase you need to redo them even after
39:03 every time you know you maybe launch a
39:05 version of the product or every time
39:06 you're monitoring your product it's
39:08 basically about integrating those two
39:09 pieces um right and I don't think
39:12 there's like one tool that will be able
39:13 to solve your problem I feel like it
39:16 will vary from your problem to problem
39:18 and then you'll be get creative uh
39:21 building but basic statistics I think
39:23 all of us can do right like mean median
39:25 mode like basic statistics to find out
39:27 okay what is the mean of my data like
39:30 how is my population what the sample
39:32 size looks like so all of those will
39:34 also give you um at least a perview of
39:37 what's happening and then you can do
39:39 that because I imagine if we only
39:41 monitor for accuracy then in this
39:43 example of this uh uh feedback loop the
39:46 model will maybe become 100% accurate
39:50 and again over fitting him that's yeah
39:53 yeah exactly and then uh okay from this
39:56 uh model Performance Point of View it's
39:59 kind of doing well right so like
40:01 everyone who is coming enjoy the event
40:03 but then we're like maybe it has like
40:06 high Precision but recall is bad right
40:08 so we are missing out on other people
40:10 who might enjoy it but simply because
40:12 the model overfit they don't get to see
40:14 this in the recommendation fit and then
40:17 a way
40:18 to to monitor this uh is uh checking
40:22 these basic stats like I don't know
40:24 looking at some demographic um
40:27 information information yes yes and uh
40:30 the way you'll actually do it for your
40:32 drift also there are these KS tools and
40:35 I mean there are a lot of tools in the
40:36 market even AWS if you're using you know
40:38 in your notebook instance you have these
40:41 tools where you can include all these
40:43 basic statistics so whatever is the
40:45 identifier for you and that'll again
40:47 vary U from your data for your data so
40:51 you do need to see how that is
40:53 performing over time the way you'll
40:55 monitor the accuracy you can even
40:57 monitor your
41:02 data and yeah so there is a similar I
41:05 probably we covered so there's another
41:07 question from Rael is um if we're using
41:10 data to train our models and identify
41:12 our checkpoints and alarm how is the
41:15 data protected from Human bias and I
41:17 think we answered that correct me if I'm
41:20 wrong so we answered it by saying that
41:22 we should monitor this demographic uh
41:25 information right so like we should the
41:28 make a population sample or maybe if
41:30 we're not predicting I don't know for
41:31 people maybe there's some there are some
41:33 other indicators that we can monitor so
41:35 probably various Case by case but we
41:37 should um have a metric that uh probably
41:40 indicates some sort of
41:42 diversity in the sample right and again
41:47 right like that human touch part comes
41:49 in like when someone will be analyzing
41:51 those cumulative statistics and look at
41:53 that data they will be able to see that
41:55 if there's a pattern the next next time
41:57 they're monitoring it they'll be easily
41:59 easily be able to see it and the other
42:01 thing that's also important is to check
42:03 the source of your data right like
42:05 sometimes the source that you're are
42:06 doing um I know that in some of the use
42:09 cases of some of the organizations I was
42:10 reading somewhere they collect all of
42:13 these U feedback from humans basically
42:15 they do these surveys and then they
42:17 collect that data and they do it over a
42:19 larger period of time and then use that
42:22 data to build the model now a survey
42:24 data like what did your population look
42:26 like right like from that data only you
42:28 can start detecting the bias like the
42:31 way you're even collecting the data can
42:33 be
42:34 questionable so check the source as
42:38 well I'm just curious how did you become
42:41 interested in this topic like was it
42:42 something you studied and then you
42:44 became fascinated or it's something
42:46 through practice that you realized that
42:48 this is a super important topic or how
42:50 did it happen for you yeah I mean uh it
42:53 it just happened because I was in two of
42:56 the most regulated Industries and right
42:58 everything is high alarming you know you
43:00 you you have to take every step in a
43:03 very thoughtful way because it impacts
43:05 the lives of people and it can lead to
43:09 um lawsuits easily I would say so that
43:12 is where I you know it was kind of my
43:15 observation and then my reading that how
43:18 important it is to be cautious and
43:21 responsible and how companies are
43:22 evolving and and that's why it became a
43:24 topic of my interest I would say mhm so
43:28 for areas like um Finance I guess
43:32 Insurance uh Health Care this is
43:35 especially important right exactly
43:37 exactly and actually anywhere where you
43:39 are dealing with like sensitive consumer
43:41 information which is being labeled as
43:44 pii it kind of becomes important but now
43:47 I would say that it's panning out to all
43:49 the industries because you know has
43:51 people get more conscious of what they
43:54 are getting and people are very brand
43:56 conscious for everything even for the
43:58 clothes we buy we like okay how are they
43:59 treating animals and what are they doing
44:02 and so it's just people are becoming
44:03 more brand conscious now so that's then
44:07 maybe the like speaking again of age and
44:10 gender so like if you think about this
44:13 they are not really pii in the sense
44:16 that if I tell you that uh this person
44:20 is a female who is 35 or like who is
44:25 between 30 and 40 you will not be able
44:28 to locate to find where this person
44:31 lives right so in this sense it's not
44:33 really personal identifiable
44:38 information oh yeah it's again
44:40 subjective right um you know this I
44:43 don't know if you've heard about there's
44:44 a tool by am I think it was by Amazon
44:47 they created this HR tool to like screen
44:49 the applicants now there all of this
44:51 information is easily available because
44:53 on the resume people will write about
44:56 you know their at least the date of
44:57 birth and everything and they shouldn't
45:00 probably yeah they kind of observed
45:03 right that for all the physically
45:05 demanding jobs like the algorithm was
45:08 just selecting males like it was
45:11 ignoring females again the bias right
45:14 because data was so biased the
45:15 historical data was all male so it
45:17 assumed so again right in some of in
45:20 some of such use cases you require all
45:22 of those things because age is an
45:24 appropriate Factor there gender might be
45:26 an appropriate Factor there so yeah do
45:30 you know how did Amazon um solve this
45:34 problem what did they do no I I so it
45:36 was a tool that was developed by them
45:38 but then it was being used by other CEOs
45:40 so again the book that I was Ring to
45:42 trustworthy Ai and that is where she
45:45 kind of talks about this Dilma of a CEO
45:47 who's trying to select candidates for
45:50 this role and how they detected it is
45:53 because you know she was like why are
45:55 you only giving me Mals to interview
45:57 right like where are females like from
45:59 past five months you haven't got a
46:00 female candidate so again the human
46:02 touch the conscious someone was able to
46:04 question the decisions and they were
46:05 able to see how the model is biased so
46:09 and yeah I mean it was kind of like an
46:11 ATS tracker that they that they use but
46:13 yeah so I'm not sure if it was again
46:16 don't quote me on this if it was by
46:17 Amazon or some other
46:21 company okay I've seen that tool
46:25 mentioned something like Fair learn so
46:28 you talked about this
46:30 a
46:31 360 right and then the aishik mentions
46:35 another tool Fair learn do you know what
46:38 this tool
46:39 is probably haven't used but I'll check
46:41 it out what is it fair learn I guess
46:43 it's like psyit learn but instead of
46:45 psyit it's fair oh okay nice I think I
46:51 assume it's something similar to I
46:52 personally haven't used this to um yeah
46:57 something similar to secondy at least
46:58 the name suggest that it's probably
47:00 something for fairness yeah and how like
47:04 since we talk about fairness I think we
47:05 spend quite a lot of time talking about
47:07 fairness like how is this related to you
47:11 know reliable machine learning like why
47:14 does a reliable model have always be
47:16 fair and the other way around like if a
47:19 model is fair does it all is it always
47:22 reliable yeah I mean that's kind of
47:25 again yeah that's a kind of a very part
47:27 ful question right like are all my
47:29 models need to be fair comes down to
47:33 what is the use case I'm trying to solve
47:35 if I'm trying to send someone
47:36 personalized recommendations do I want
47:39 to exclude a population do I want to um
47:43 you know
47:44 exclude excluding a population also kind
47:47 of has some effects right because you
47:50 might be excluding again some of the
47:51 potential customers that are ready to
47:54 engage with you but just because you're
47:56 model was not fair enough you just
47:58 secluded a population and you secluded
48:00 some potential customers so it's about
48:02 that the other thing is again it's also
48:05 about the happiness of your users right
48:08 like again in the same family there
48:10 might be different people indulging with
48:12 your product one is receiving something
48:14 the other is not obviously everyone's
48:16 not going to put a lawsuit against you
48:19 but you know it's it's unhappy might
48:21 some might right so you don't want to
48:24 risk that right uh yeah there is um
48:27 abishek who is writing a lot of comments
48:29 probably is also into this topic so what
48:33 he wrote is that there is a cool paper
48:35 titled what would an Aventure do do you
48:38 know this paper no I haven't read that
48:41 paper I'm really curious what it is but
48:44 maybe we can check it after the episode
48:48 and then see what is there what there
48:50 are people saying that uh this is a nice
48:53 paper the title is amazing I hope it's
48:56 not like you know all this um um never
49:00 going to give up kind of you know
49:03 [Laughter]
49:06 recrawling because like I recently
49:09 somebody sent me like a recr link and
49:11 then I
49:13 got okay um yeah we have quite a few
49:18 questions so maybe we can cover them
49:21 too so do you know if there are any
49:24 synthetic indicators for bias in data
49:26 similar to rmsse that is an indicator of
49:30 accuracy for
49:32 models like is there a number that can
49:34 just tell us hey your model seems to be
49:36 biased uh go check
49:39 it I mean I can tell you a number but I
49:42 feel like even if I was working on 10
49:44 use cases that number will be different
49:46 for all the 10 use cases it'll so depend
49:50 because you're obviously not using one
49:51 data set to build the model you're
49:53 probably using 15 especially if you're
49:55 building a production AI model model um
49:57 so I don't think that there's any number
50:00 there can be baselines that you can you
50:02 know set based on your company's data
50:04 okay if it goes beyond this Baseline
50:07 it's an alarm right based on what You
50:09 observe over time but I don't think you
50:11 can set up a starting number again you
50:13 might be getting into the same
50:15 situation so then you still need to to
50:18 have this human tou right like a person
50:20 needs to say hey why are you bringing me
50:23 all the male candidates where are femal
50:26 somebody still needs to raise this
50:28 question and say hey what's going on
50:30 right and then uh so there's no like
50:33 it's not possible to remove the remove
50:35 the human touch at all right it's like
50:38 the human has to be in this process
50:40 always I mean we don't know where
50:42 technology is heading to but but right
50:47 now there is another point that in the
50:49 age of out
50:51 ml like when you just get a data you
50:53 throw in in a black box and then it
50:56 gives you it gives back like a good
50:58 model and then you just can go and use
51:00 it like uh when it becomes so simple to
51:03 train a model even for people who are
51:05 maybe not data scientists right right so
51:08 you can just go and train a
51:11 model like uh how can we uh still be
51:15 cautious about this and not completely
51:17 you know put a lot of trust in the model
51:20 yeah but again I feel like even
51:22 interpretability and all these
51:24 explainable AI tools I mean uh that
51:26 might not be a priority of everyone
51:29 right it's a priority of some Industries
51:32 so again what's your company's goal
51:35 right you might be just fine with
51:36 developing one of the most wonderful and
51:39 accurate algorithm and just might be
51:42 happy with that you don't you might not
51:44 care about interpretability and that's
51:46 where it comes like this versus that um
51:49 so yeah it's an answer that you have to
51:51 do but I feel even in automl um yeah you
51:54 are giving your input data but you still
51:56 have some power over the data you can
51:58 still explore that and see what's
52:01 there just blindly before blindly just
52:04 inserting it the
52:07 algorithm so can you tell us about the
52:10 data bus Community what it is how did
52:13 you start yeah of course um so U data BU
52:17 as I said like I I'm from a non-computer
52:20 science background and I have had some
52:21 kind of unique challenges uh to get into
52:24 this field um you know being from a
52:27 non-coding background being able to
52:29 justify that I can be a data scientist
52:32 and exploring other Realms of data
52:34 science because for people it might be
52:36 like oh there's a data scientist no
52:38 there's a data strategist there's a AI
52:40 consultant there's an envelops engineer
52:42 and there's so many things you can do
52:44 with your unique skill set and
52:46 especially if you are pivoting from a
52:49 business and kind of like a business
52:51 plus a science field um there is
52:53 something unique that you have to offer
52:55 so datab buas basically kind of its goal
52:58 is to Mentor those people engage those
53:00 people um and also there's so much to
53:02 learn you know so I am an evid reader so
53:06 whenever I see something cool like I try
53:08 to post it on my page so that my
53:09 community can benefit or any cool
53:12 resource that'll help you upskill is
53:14 posted on my LinkedIn page so you can
53:17 follow that and I also provide
53:19 one-on-one consultations for free and I
53:21 connect you with my large network of
53:24 people that I have from the past 5 years
53:27 um you know they can help you to Pivot
53:29 where you would like to Pivot you've
53:31 been doing this for 5 years that's
53:33 that's amazing uh no so actually I
53:36 started doing I mean I was doing it
53:37 unofficially for you know a lot of years
53:39 but now I started this community in the
53:42 starting of this year because I F okay
53:43 now I can you know officially do this
53:48 so and what kind of so you said that
53:51 it's not just data scientists there are
53:53 so many data roles and uh like I also
53:57 get quite often requests like okay I'm
53:59 doing X how can I become a data
54:02 scientist and this x usually varies from
54:04 I don't know Baristas to software
54:07 Engineers yeah and then um yeah so how
54:11 first I guess the the that what you need
54:13 to explain to people is it's not just
54:15 data science right and my question is uh
54:18 like what is out there in addition to
54:20 data you mentioned a few roles but um
54:23 like what are the the typical roles that
54:25 people can look
54:27 at I think first of all the question to
54:30 ask is like what is my skill set and
54:32 what do I want to do with right like do
54:34 you actually want to be building model
54:37 some people are lot into research so
54:39 they these days they're becoming machine
54:40 learning Engineers or they're called
54:42 research scientists or applied
54:43 scientists uh in the tech world you know
54:46 who actually research on the algorithm
54:48 some people are interested in oh the
54:50 aftermath of my productionizing the
54:52 model the mlops engineer so they those
54:55 might be software developers who now
54:57 want to Pivot in the ml field or devops
54:59 Engineers who want to Pivot now the
55:01 other are who want to relate the
55:03 business to the AI these are the AI
55:05 Consultants who actually search all of
55:07 these AI use cases and then pitch uh
55:10 clients and here you typically have like
55:13 a Consulting um I would say relationship
55:16 with your and then you have these data
55:19 analysts you have you know business
55:21 analysts even those are kind of a data
55:23 scientist because now you're doing all
55:25 the project management when you're doing
55:26 the analytics and Reporting U for your
55:29 data science results and even before the
55:30 data science results and then definitely
55:32 you have data
55:33 strategist you know who are strategizing
55:36 on what data uh you need all of these so
55:40 yes for example a project manager
55:42 somebody who works in as a produ project
55:45 manager in I don't know some traditional
55:49 industry I don't know like manufacturing
55:51 or anything so how can they select uh
55:55 what work for them like it doesn't have
55:57 to be project manager but
55:59 usually like every person has unique
56:02 skills right that like how do you
56:04 understand if all you know is that the
56:06 the data scientist is like the sex job
56:09 and you might not know about like
56:12 others no Ian that that's a tough
56:14 question right like even when I was
56:15 trying to decide my good I think I would
56:17 have spoken to like 100 plus people I
56:20 did coffee chats with people from
56:22 different Industries different domains
56:24 and try to explore like what what do
56:26 they actually do you just cannot read
56:28 the job description and know okay this
56:29 is where I fit so I feel like all these
56:32 networking events confes coffee chats
56:36 and you know engaging in such
56:37 communities will probably lead you where
56:40 you want to
56:42 go and from all these roles do anyone do
56:46 everyone need to know and care about
56:50 responsible Ai and explainable AI I
56:53 don't think so I feel like all of the
56:56 please um you know bring something
56:57 unique to the table and again if your or
57:00 does care about you you do need to add
57:02 that touch um but it'll again depend
57:05 from an organization to you might BR in
57:07 a startup who might say Oh no just you
57:09 know care about The Profit just build me
57:11 a most profitable AI algorithm so it
57:15 depend then but then I as a data
57:18 scientist I probably have some moral uh
57:21 duty to tell to the C hey wait a minute
57:25 like uh this is not how it's done right
57:27 so you should think about
57:29 reputation that also depends where you
57:31 are in the stage of your career right
57:33 you are you are you an influencer are
57:35 you a decision maker in the decision
57:37 making process or are you just the
57:39 person who exec stuff you know whatever
57:42 uh is told to you if someone's actually
57:44 starting in the data science domain it
57:46 might be hard for them to convince
57:48 people what needs to be but once youve
57:50 established yourself you know then it
57:52 then it is your moral responsibility
57:54 being in the industry for three four
57:55 years
57:56 do everything
57:58 responsibly but probably at least
58:00 everyone who gets in the data World
58:02 should perhaps study ethics or something
58:04 like this I mean but yeah like this is
58:07 such a I would say this is such a buding
58:09 field that so many people areed even
58:13 though I don't build explainable AI
58:14 models but I feel like as a person again
58:18 as an ethical person I feel it's my
58:20 responsibility to understand what I'm
58:22 doing and how I'm doing
58:24 it actually at work um um so if I want
58:28 to have access to data and I as a data
58:31 scientist have to have access to data
58:32 right so I need to go through a special
58:34 training and this training tells me what
58:38 is good and what is not and uh I think
58:40 more and more companies are actually
58:42 implementing this I mean of course I can
58:45 use my common sense and then when I like
58:48 there are quizzes and when I need to
58:50 answer these quizzes most of the time I
58:52 just use my common sense and more often
58:54 than not the answer is correct
58:56 but it's still a good idea to tell
58:58 people that you know what this is how
59:00 it's possible to do wrong with
59:04 data proba this is like every all the
59:07 data people should do this sort of thing
59:09 yeah yeah definitely and I think even
59:11 when you join a firm right like you go
59:12 through some
59:14 train what do you do in case of
59:16 emergency and I feel like this is kind
59:18 of one of the things right like when you
59:19 have such data how do you deal with this
59:23 and become a responsible individual so
59:25 yeah definitely
59:27 okay I think that's um the time is up so
59:30 thanks uh for joining us maybe before we
59:34 finish before we wrap up is there
59:36 anything you want to mention that maybe
59:38 you
59:39 forgot um I just want to thank everyone
59:42 for joining and for being so um
59:45 interactive I would say and if there was
59:47 any question that was not answered if
59:49 there's something you know personally
59:50 you would like to discuss I'm very
59:52 active on LinkedIn do connect with me
59:54 there um you can even uh look into the
59:56 databas page and all the views and
59:59 everything that I spoke about today was
1:00:01 my opinion and had nothing to do with
1:00:03 Morgan St so
1:00:07 that that was an important part to
1:00:09 mention
1:00:11 right so yeah yeah yeah thanks for
1:00:13 joining us uh also thank you everyone
1:00:16 for joining us too for being active for
1:00:17 asking all these questions uh there was
1:00:19 a very Lively discussion lovely
1:00:22 discussion and Lively in live chat so
1:00:25 thanks for for doing that and I wish
1:00:28 everyone a great weekend yeah thank you
1:00:31 goodbye everyone