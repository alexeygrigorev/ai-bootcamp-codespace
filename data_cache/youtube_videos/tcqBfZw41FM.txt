0:00 hello everyone welcome to our event this
0:02 event is brought to you by data talks
0:04 club which is a community of people who
0:05 love data we have weekly events today is
0:08 one of such events if you want to find
0:10 out more about the events we have we
0:12 have a
0:13 special web page on our website the link
0:16 is in the description go there check it
0:18 out we have quite a few events lined up
0:20 for example next week we have a cool
0:22 event about
0:24 data observability so check it out it
0:27 will be a workshop
0:28 and if you haven't subscribed to our
0:30 youtube channel now it's the best time
0:32 to do this so when you subscribe you
0:34 will get notified about all our videos
0:37 and then join our amazing slack
0:39 community where you can hang out with
0:41 other data enthusiasts
0:43 during today's interview you can ask any
0:45 question you want there is a link in the
0:47 live chat click on this link
0:50 ask your question and i will be covering
0:52 these questions during the interview
0:55 so
0:56 that's it for the introduction now i
0:59 need to
1:01 open the questions i prepared
1:06 and
1:07 i'm ready are you
1:09 i am
1:11 okay so let's start
1:14 so
1:15 just one second okay i'm ready
1:18 this week we'll talk about innovation
1:20 and design for machine learning and we
1:22 have a special guest today elizabeth
1:24 elizabeth has been working on topics
1:26 related to strategy product and ai for
1:28 the last six years
1:29 first at mckinsey and then at process
1:32 and process is the parent company of
1:34 willix this is where i work so elizabeth
1:37 and i
1:38 were we were working together
1:40 and elizabeth led an innovation division
1:43 at twilight's
1:44 so now she is joining a new company
1:47 and she will work on climate resilient
1:49 targeted
1:51 agriculture it's a difficult one so
1:53 maybe you'll tell us a few words
1:55 about that but yeah welcome to our event
1:59 thank you and thanks so much for uh
2:01 inviting me very happy to be here and i
2:04 think you already gave a very nice
2:06 overview of some of the things that i've
2:07 been doing in the past year so i won't
2:09 spend too much time on it but as you
2:10 said b2c kind of you know past few years
2:13 i've been working on where strategy ai
2:17 and product meet
2:19 uh i'm an engineer by training so i have
2:21 a master's degree in applied physics
2:24 but i am also an art historian
2:27 and ever since my studies i've really
2:29 enjoyed kind of trying to meet at art
2:32 and science or at least try to find ways
2:35 in which more technical topics merge
2:37 with more softer topics like
2:39 more strategy or product or design or
2:41 innovation so the ones that we'll talk
2:43 about today
2:44 yeah and as you mentioned spent some
2:46 years working as a strategy consultant
2:48 but then
2:49 worked on all sorts of topics but i
2:51 really wanted to dive deeper into the
2:53 topic of ai machine learning that's when
2:55 i joined process and also there i've
2:57 been doing many things but i think you
2:58 can kind of summarize it as trying to
3:00 find ways um to scale the impact that ai
3:05 machine learning have across
3:06 organizations uh so that's where i've
3:08 been working on topics like
3:11 you know how does ai actually fit into
3:12 your overall business strategy of a
3:14 company
3:15 or
3:17 how can you kind of design and optimize
3:19 your products in such a way that you
3:20 have amazing feedback groups going or
3:23 how can you
3:24 upscale a whole organization
3:26 to understand for them to understand the
3:28 basics of the eye machine learning
3:30 and help enable the work of the data
3:32 scientists in an organization so a lot
3:35 of different topics that we're working
3:36 on and i'm very happy to be talking
3:38 about design and innovation today
3:41 and so you had a double degree right or
3:44 how or first you got one and then
3:46 another
3:47 yeah so i have
3:48 my uh art history degree since
3:51 undergraduate so just a bachelor degree
3:53 and i did that together with my physics
3:54 and then i did the master's in applied
3:56 physics yeah
3:59 that's that's pretty cool so
4:01 did you say it was quite useful in your
4:03 career that you have like both
4:04 perspectives sort of on things
4:08 i would say that um the humanities or
4:10 art history definitely has a completely
4:13 different way of thinking about
4:15 problems so it's much more about
4:18 you know in physics it was always the
4:20 case he had sort of a challenge and
4:22 there's only one possible answer whereas
4:25 in history or arts there's always you
4:27 know a single topic that you can look at
4:30 from different perspectives so that's
4:31 much more about trying to also
4:33 understand other people's perspective on
4:35 a certain situation and really being
4:37 able to kind of maybe convince someone
4:40 of of your perspective so i would say
4:42 it's a very nice and complementary skill
4:44 set
4:45 yeah especially with history right so
4:47 the textbook is not enough right because
4:49 you always have different opinions and
4:51 you need to learn to work with sources
4:54 to really find out what happened
4:56 because textbooks are always written
4:58 like they sometimes meet some things
5:01 yes there's always you know someone
5:03 always has their own view on a certain
5:05 situation definitely
5:07 okay so what is design
5:09 um like is it about making websites look
5:12 good or there is more to that
5:15 right so you're you know you're asking a
5:17 physicist here to to define design and
5:19 probably designer will have a different
5:21 perspective but because i'm not a
5:23 designer by training i actually very
5:26 nicely am able to see design really as a
5:28 tool
5:29 and then for me more specifically as a
5:32 tool to help
5:33 build
5:35 user-centered products
5:36 so i see it as a whole set of processes
5:40 that you can apply to make sure that
5:43 whenever you build a product a feature
5:46 or an ai application
5:48 that you really start from the problem
5:50 that you're trying to solve and really
5:52 always take into account sort of the
5:54 customer view or the
5:57 whoever is going to use this this uh
5:59 this application that you're building
6:00 that you always make sure that you take
6:01 that into account that for me is what
6:04 design is about and why is it called
6:06 design to me design is like okay let's
6:09 design a car that looks that is
6:11 beautiful let's design a dress that
6:13 looks beautiful let's design a website
6:15 like it's always to me it's about i i
6:19 don't like a aesthetical component
6:24 like i don't know but
6:25 when i started working in twilights i
6:27 learned that designers into excellent in
6:30 other companies especially ux designers
6:32 right so they're it's not only about
6:35 making buttons look good right but
6:38 how easy it is to find a button on the
6:40 website right and thanks
6:43 yeah and especially uh when you try to
6:46 combine that with ai data science
6:49 machine learning for me it becomes about
6:52 much more than just you know what the
6:53 button needs to look like what color it
6:56 should be
6:56 um and i have some examples that i can
6:58 share with you how that might work so
7:02 uh maybe the closest to ux i have an
7:04 example of what you would call
7:07 um algorithm friendly design or
7:11 it's also sometimes called i'm seeing as
7:14 an algorithm it's a very nice uh article
7:17 that i can recommend by an author named
7:19 eugene way what he talks about it's
7:22 almost as if you know when you think
7:23 about a certain product experience or
7:25 the way in which a customer interacts
7:27 with the product
7:28 why don't you also see the algorithm as
7:31 stakeholder in that because you know for
7:34 an algorithm to work very well needs to
7:36 have very clear signals so why not build
7:40 your product in such a way that the
7:42 algorithm can actually
7:43 collect all of the signals that it needs
7:47 i think you know this is kind of a new
7:48 way of thinking about
7:51 maybe the interaction between design or
7:53 product design and data science but i
7:55 think it's a very powerful one also fits
7:57 into this
7:58 broader story of maybe data centric ai
8:02 so you know back in the days three years
8:03 ago when i joined process i had to fill
8:05 in a lot of data science teams they
8:07 would build ai applications using the
8:09 data that was already there that was
8:11 usually generated
8:13 for a different purpose not necessarily
8:15 for the purpose of training their models
8:18 but imagine that you move to a different
8:20 scenario where from the beginning
8:23 of designing a product you take into
8:26 account that algorithm and what it might
8:28 need so
8:29 you know think about olex it's about
8:31 buying and selling uh and sometimes
8:33 buyers and salaries have have different
8:35 incentives so a seller just wants things
8:38 to be very easy i don't want to provide
8:39 that much information because you know
8:41 they it takes too much time
8:43 but a buyer might want to know a lot of
8:45 details so there's already a little bit
8:46 of conflict that usually when you design
8:48 a product you take into account both the
8:50 needs of the buyer and the seller but
8:52 imagine that you may be also taking into
8:54 account the needs of the algorithm there
8:56 and maybe the algorithm is it's all
8:58 about
8:59 you know understanding um
9:02 the preferences of a certain buyer so
9:04 maybe really in detail understanding you
9:07 know what style of secondhand clothing
9:09 they might like or what style of
9:10 furniture they might like and maybe in a
9:13 way designing also the product to
9:15 collect really strong signals about
9:17 exactly exactly that
9:20 so you know that's an example for me
9:21 where data science and design come
9:23 together
9:25 i think you mentioned one important word
9:27 that i um
9:29 i took a note of this word so you
9:31 mentioned it's this is about interaction
9:33 right design it's about how you interact
9:35 with something
9:36 be it a piece of furniture or a website
9:39 right
9:40 a physical or virtual product
9:43 and
9:44 so it's about
9:46 making website easy to use and then when
9:48 we talk about design and machine
9:51 learning it's also about thinking how
9:53 this
9:54 thing will interact with an algorithm
9:57 right not only with the human with the
9:58 user but also with our algorithm that we
10:01 will use to make user experience better
10:03 right
10:04 yeah and and especially you know
10:07 how can we collect the signals that the
10:09 algorithm might need in order to train
10:12 faster and this article that i just
10:14 mentioned about seeing as an algorithm
10:16 that very nicely describe the difference
10:18 between instagram and tick tock here
10:20 right so imagine scrolling an instagram
10:22 feed i think a lot of people will be
10:24 familiar with that
10:25 well you know you scroll past a lot of
10:27 different posts but if you read it from
10:29 the perspective of an algorithm it's
10:31 kind of hard to understand if someone
10:33 likes a post or not because typically
10:35 you know you might see
10:37 one bit of a post here you see a bit of
10:39 a post below it you're looking at the
10:41 comments so what is it actually that the
10:43 user is interacting with whereas if you
10:45 look at tick tock
10:46 there it's about short videos and
10:48 someone's looking at one video at the
10:50 time and maybe clicks on the creator
10:53 maybe clicks on um a heart right and
10:57 then you collect much uh more sharp
11:00 signals about that one particular video
11:04 which actually allows tick tock and its
11:05 algorithms to learn much faster what it
11:08 is that you like and that you don't like
11:09 so that's where you know in the way that
11:11 you design your interaction with the
11:13 user um you're taking into account how
11:16 you can speed up the learning of the
11:17 algorithm and actually extracting but
11:19 the interests are of a particular user
11:22 yeah i guess in case of instagram and
11:24 other social networks so there are
11:26 multiple ways you can interact with a
11:29 piece of content you can like it right
11:31 you can also comment or you can reshare
11:34 it or you can just
11:36 watch it right and then people
11:39 the the developers data scientists in
11:42 that company now need to figure out okay
11:44 now we have this
11:45 a ton of signals coming
11:48 out of this how do we actually now get
11:50 all the signals and combine them in such
11:52 a way that we can put this into our
11:54 recommender system and we get a good fit
11:57 while as you said in tiktok
11:59 so maybe in these social networks they
12:01 first designed the feed and then thought
12:03 okay now how do we
12:05 add the
12:06 the ranking here right while in tik tok
12:08 they first they
12:10 thought about this in the first place
12:12 right exactly exactly yeah yeah but
12:15 it might be nice if i mention another
12:17 example of how for me design and machine
12:19 learning come together because what
12:20 we've just been talking about is really
12:22 a bit more about design as a way that
12:25 you maybe shape the interface between
12:27 the user and the product but for me
12:30 design can also be much more let's say a
12:33 process or a way of working that you
12:34 agree on with each other and
12:36 an example of a technique that's used up
12:38 by designers is called um the double
12:41 diamonds it's kind of a way to go from
12:44 you know a rough
12:46 problem area something that you might
12:49 want to solve or something that you
12:50 think you want to build to an actual
12:52 working solution and then you know
12:54 imagine
12:56 shape of two diamonds so it's kind of
12:57 like diverging and converging and that's
13:00 how that way of working uh works so
13:03 you start with something like hey i want
13:05 to
13:06 reduce fraud in in my business or in my
13:09 products then you diverge and you start
13:11 to research okay what is fraud really
13:13 about
13:14 what ways of fraud do we see on the
13:16 platform
13:17 what do users care about when they think
13:19 about fraud and then you
13:22 converge again and you say okay you know
13:24 if i want to solve frauds there's this
13:26 particular sub problem that i actually
13:28 should be solving because it's most
13:29 important to my users
13:31 and then you start to diverge again and
13:32 you look at all of these different
13:35 potential solutions in which you can
13:37 solve that type of fraud
13:39 and you start experimenting and you
13:41 start testing them and then you convert
13:43 to a particular solution that works and
13:45 that for me is another design method
13:47 that's actually used to make sure that
13:50 um you're solving the right problem and
13:52 also you're solving the problem in the
13:54 right way and some of this might seem a
13:56 little bit obvious but
13:58 to me again it touches also to some of
14:01 the core challenges that i still see
14:03 data science teams that i've worked with
14:05 namely
14:06 um
14:07 what's really the problem that we're
14:08 trying to solve here
14:10 first of all and also second of all is
14:12 machine learning really the right way of
14:14 solving it and i have seen a lot of data
14:16 science teams that you know go to the
14:18 solution very quickly but if as a team
14:20 you agree on these type of design
14:22 practices it might help you to make sure
14:25 that you're solving the right problem
14:27 and
14:28 you're using the the right tools to do
14:30 so
14:31 so i was taking notes about this diable
14:33 diamond this is not the first time i
14:35 hear this but let me try to summarize it
14:38 to make sure i understood it so you have
14:40 like this sort of two big
14:43 steps in the process yeah the first step
14:46 is you really need to understand the
14:47 problem what is the problem you're
14:49 trying to solve right so first you do a
14:51 bit of brainstorming and you try to
14:53 understand okay fraud what does it
14:55 actually mean right and then you do
14:57 brainstorming you talk to people
14:59 and you collect a lot of data right and
15:01 then out of this data
15:03 you actually want to pick one area and
15:06 focus on that and then by doing this you
15:08 really understand the problem you want
15:10 to solve the problem that your users
15:11 have right so first you start with the
15:13 problem and then the second diamond the
15:15 second step is okay now we found out
15:18 what the problem is now let's find a
15:20 solution and then you again go into the
15:22 brainstorming mode and you say okay i
15:25 can solve it with neural network i can
15:27 solve it with i don't know gradient
15:28 boosting t i can solve it without any
15:30 machine learning i can solve it just by
15:31 sitting there and labeling data myself
15:33 or
15:34 i don't know i can hire a vendor to do
15:37 this or like you exist all these
15:39 possible things
15:40 right and then at the end okay now we
15:43 have all these possible solutions like
15:45 we can build thing in-house we can solve
15:48 it without machine learning we can you
15:50 know hire vendor
15:51 let's evaluate all these solutions and
15:53 find the right one right and then at the
15:55 end of this process
15:56 you
15:57 know what the problem is
15:59 and you know what's the best way to
16:01 solve it right exactly
16:03 and
16:04 hopefully i think that was a very nice
16:06 way of describing it and it of course it
16:09 depends on the team and your
16:10 organization how exactly they might
16:12 apply a process like this but what you
16:13 also often see is when you go to that
16:15 solution phase there's a lot of
16:16 experimentation so out of these you know
16:18 six possible ways that we've identified
16:21 maybe let's start three of them let's
16:23 see if they actually work if they work
16:25 as easily as we thought they would um so
16:28 it's not
16:29 you know a lot of this is trying to also
16:33 substantiate your ideas with data
16:35 whether it comes from your user or from
16:37 tests to try to see what really is the
16:40 problem you're solving and what the best
16:41 solution could be
16:43 so i guess this is a part of the second
16:46 step the second diamond is okay now we
16:48 have a couple of possible solutions we
16:51 can evaluate this vendor let's try and
16:54 build a proof of concept right or we can
16:56 see how to solve it without any machine
16:58 learning let me just sit down and label
17:00 all this data myself and see what
17:02 happens right how can i actually make a
17:04 decision based on that and then i don't
17:06 know the third option could be something
17:07 else
17:08 let's just i don't know open my jupiter
17:10 notebook and try in a simple model
17:12 myself right yeah and then we do this in
17:14 parallel we have three proof of concepts
17:16 and then at the end of this uh
17:18 step we can also see okay this seems to
17:21 be more viable right it's also part of
17:23 the growth source
17:25 exactly yeah yeah and then usually along
17:27 the way you have different criteria
17:29 right because sometimes you know you
17:30 just have budget constraints so that's
17:32 when you eliminate one of the options um
17:34 sometimes you have an idea but
17:37 you put it in front of a user and it
17:38 turns out they don't like it at all so
17:40 maybe that's why you eliminate another
17:42 idea or as you said you know you tried
17:45 that these billets but you figure out
17:46 you really have none of the data to get
17:49 started and to collect the data would
17:51 take you so long that that idea is not
17:53 feasible so then for different reasons
17:55 you can start to eliminate some of your
17:57 potential solutions but at least
18:01 i like it because it's a very conscious
18:03 uh approach to make sure that you are
18:06 really doing the right thing and you're
18:08 not spending your whole team's time
18:10 solving a problem that could have been
18:11 solved in a much easier way or maybe
18:14 that's you know wasn't the most
18:16 important step to actually solving that
18:17 fraud issue that you started off with
18:21 and at the beginning when you were
18:22 describing this it looked like a thing
18:24 that could be done in a couple of days
18:26 but now when we talked about creating a
18:28 proof of concept multiple of them then
18:31 also showing to the user and then at the
18:32 beginning you also mentioned i think
18:34 user research i think which involves
18:37 talking to actual people and then
18:38 showing them some things right so how
18:41 long does it actually take from
18:43 the
18:44 moment let's say when a management
18:46 management comes in and says hey we have
18:48 a lot of fraud let's solve it to the
18:50 moment when you you know finish the
18:52 second diamond
18:55 well ideally this takes two days right
18:57 you have an amazing team you put all of
18:59 them together in the room and solved it
19:01 today so yeah it depends a lot on how
19:05 many customers you have how elaborately
19:08 you want to do user research or how much
19:10 of that is already there in the
19:11 organization uh it also depends on how
19:14 complex you want to make the solution um
19:16 and when you would say it's done right
19:18 is it double you have some sort of a
19:20 small proof of concept or is it done
19:22 when you've actually scaled that across
19:23 all of your customers and all of the
19:25 countries that you might be operating in
19:28 so
19:28 you know that
19:29 i think it's it's difficult to say how
19:31 long a process like this takes but
19:34 you know i do want to stress that when a
19:37 team that includes data scientists
19:39 starts to work with these type of let's
19:41 say design techniques doesn't mean that
19:43 they'll spend 90 of their time in
19:45 brainstorming sessions right
19:48 i think it's very nice to
19:50 use some of these techniques to try to
19:53 make sure that you as a team and as an
19:56 organization you keep working in a
19:58 user-centered problem-centered
20:00 way you don't go to a solution too
20:03 quickly also doesn't mean that you have
20:05 to kind of throw away
20:07 all of the ways of working that you had
20:09 before i think it can be very nice
20:11 add-on
20:12 to some of the the ways of working that
20:14 teams already have
20:16 so i heard this devil diamond in context
20:20 of another thing another term called
20:21 design thinking
20:24 so what is design thinking and how these
20:25 two things are related
20:28 so you know if you maybe thought that
20:30 this concept of double diamond was
20:32 really a little bit fluffy then i would
20:34 say that you know design thinking is the
20:37 overarching term even above some of
20:39 these different uh processes
20:42 i would say that
20:43 design thinking is
20:45 you know the overall ambition to make
20:47 sure that whatever products you build or
20:50 features or
20:51 models that they again take into account
20:54 um this this user and then
20:57 as an organization anyone could do that
21:00 in different ways maybe
21:01 what's actually a nice example is how
21:03 google does this
21:05 so if you're interested to know a little
21:07 bit more of what some of these design
21:08 processes
21:10 might look like to make sure that you're
21:13 working in a
21:14 kind of design thinking way
21:16 you could look up pair
21:18 with google i think it stands for people
21:21 in ai research
21:23 they have a whole set of tools
21:26 from kind of scoping exercises to more
21:30 ways of prototyping and very nicely
21:33 combined
21:35 making sure that you understand the user
21:38 but also working towards actual ai
21:40 machine learning solutions for some of
21:42 the problems that you might identify so
21:44 yeah it's a whole i would say it's a
21:46 whole set of processes to make sure that
21:48 you are your user centered but as i said
21:51 it's not
21:52 yeah i think it's a nice add-on and a
21:54 nice tool uh to have but it's not you
21:57 know something that makes you have to
21:59 throw away everything that you've
22:01 learned before right
22:03 so it's paired like
22:05 p
22:06 a
22:07 i r right yes
22:09 people ai research yes
22:12 and you said it covers things like
22:15 scoping i guess this is the first
22:16 diamond we talked about yeah right so
22:18 how
22:19 do we make sure that we're solving the
22:21 right problem and then prototyping i
22:23 guess this is the second part right so
22:25 when we
22:26 try to find the the solution and there
22:30 they
22:31 talk about different tools how to
22:32 actually work processes how to actually
22:35 organize
22:37 our
22:38 sessions in such a way that we
22:40 successfully
22:41 find the problem and then we
22:42 successfully find the solution right yes
22:45 yes and it's just more i would say an
22:48 overview of some
22:49 inspiring resources
22:52 because i would say as a team that has
22:56 an ambition to do work more with ai
22:59 it's not as if there's already you know
23:02 a very strictly written handbook of
23:04 exactly how you should be doing it so
23:07 it's typically up to a team to just find
23:09 inspiration
23:11 in how others might have approached this
23:13 and then this research from google might
23:15 be an interesting one
23:17 and what is the design sprint is it
23:18 related to what we talked about
23:22 yeah sometimes uh teams go
23:24 through a process like this kind of
23:27 centered in i don't know a one week for
23:29 example so before they might have
23:31 already done a lot of interviews so in
23:33 one week with a team they'll try to
23:36 analyze those interviews and say hey
23:38 here is really the problem that we want
23:39 to solve they'll do a whole bunch of
23:42 creation ideation type of workshops
23:45 and they'll come to a prioritized list
23:46 of solutions that they might want to be
23:49 trying so sometimes teams try to really
23:53 um put a lot of brain power together and
23:56 try to answer a whole bunch of these
23:58 type of questions around the problem in
23:59 a solution together in one week but yeah
24:02 that again depends a little bit on
24:05 what it is that you're trying to solve
24:06 sometimes you know hey let's solve
24:08 frauds that might be such a big
24:10 challenge that the design sprint is not
24:12 really the right method for a team to do
24:14 that
24:15 so this is one of the tools of this
24:17 design thinking thing right
24:19 yes
24:20 yes okay so i guess
24:22 from what you mentioned what you talked
24:24 about
24:26 so let's say when we understand the
24:27 problem when we did a bit of homework
24:29 already when we interviewed the user
24:31 users about the problem that they have
24:33 with our product
24:35 and we already have some materials and
24:38 then in order to go through this
24:40 material and process it as effectively
24:42 as possible we
24:44 put together a team like a bunch of
24:46 people together
24:48 and
24:49 tell them okay you have a week
24:51 now figure out what to do with right and
24:53 this is called the design sprint
24:55 yeah i think that's how you could say it
24:57 yes
24:58 okay and what are the things that we
25:01 need to do in this design spring to to
25:03 be able to
25:04 do this successfully and
25:06 who do we need that
25:08 there in the sprint like i think we need
25:10 to
25:11 data scientists right do we need
25:13 designers product managers like who
25:15 should be there
25:18 well um definitely if if you're thinking
25:21 about solutions that involve data
25:23 science
25:24 definitely helpful to have data
25:26 scientists in the room and i also what i
25:28 appreciate about these ways of working
25:31 is that you'll try to involve
25:34 the full team in the whole process right
25:37 so it's not just like oh okay i think
25:39 there's a solution with ai here this is
25:41 the moment that we call the data
25:42 scientist no i always really appreciate
25:44 it if data scientists maybe sat in on
25:47 some of the interviews with customers or
25:50 helped at least really also understand
25:53 and own the problem that you're trying
25:54 to solve so that's why
25:56 everyone that you might need for the
25:58 solution i really appreciate it if all
26:00 of them are there in the whole process
26:02 um but yeah for these type of sessions
26:04 it is very helpful to have someone that
26:06 can facilitate them so that understands
26:08 okay when should we be
26:10 diverging and we keep researching and
26:13 trying to keep understand the the
26:15 problem space and when do we start to
26:17 diverge and say like hey this is really
26:19 the core problem that we're trying to
26:20 solve for example
26:22 and it could be a designer or a surface
26:25 designer who are very well equipped to
26:27 facilitate processes like that but it
26:30 could also be a product manager who has
26:32 done that before right this is i would
26:34 say
26:34 a skill that is definitely uh you can
26:37 definitely learn this data scientist
26:39 could also be the ones that facilitate a
26:41 session like that
26:43 um
26:44 and
26:45 yeah whoever you might need
26:47 on the on the solution side uh you would
26:50 want to involve them in a session like
26:52 this all the engineers right front-end
26:54 engineers back-end engineers
26:56 much like if we need machine learning
26:58 and data scientists then the facilitator
27:01 could be product manager could be
27:02 designed or both right
27:04 yes so basically the entire team the the
27:08 product team that will work on
27:10 implementing this they all should be
27:12 there right
27:13 ideally i would say so yeah and this is
27:16 something that i haven't always seen i
27:19 do see often a tendency you know that
27:22 the user research is done by user
27:24 researchers or designers
27:27 and then
27:28 you know once you actually start
27:29 building that's when you call the
27:31 engineers
27:32 but
27:33 i would say that you build better
27:35 products or better features or better ai
27:37 applications if you have all of that
27:39 brain power involved both in the
27:42 problem definition and in the solution
27:44 uh definition as i mentioned before you
27:48 know this idea of seeing
27:50 as an algorithm and maybe optimizing the
27:53 interaction with the user in such a way
27:55 that you really collect these laser
27:57 sharp signals that you might meet in
27:59 your models
28:00 well that is quite a complicated task so
28:03 you probably need
28:05 all of the different
28:07 you know
28:08 mindsets and all of the different
28:11 specialties
28:12 involved in the whole process there to
28:14 be able to achieve something like that
28:17 the the next question i had was about
28:20 like as a data scientist why should they
28:22 care about all that sounds like
28:25 my job but i think you answered that
28:27 right so in this process
28:29 when we talk about potential solution
28:31 as a
28:32 well even before that uh first data
28:35 science for a data scientist it's very
28:37 helpful to understand the problem right
28:38 because it will influence how exactly
28:40 you solve it but then also during the
28:43 sort of solution phase when let's say
28:46 we're thinking about the interface how
28:47 exactly the interface will look like
28:50 if
28:51 i as a data scientist not there
28:53 not in this meeting then i might not be
28:55 able to say hey what i mean how
28:59 oh wait a minute how exactly we're going
29:01 to collect this data that we actually
29:02 will need for solving this
29:04 exactly
29:05 exactly yeah and i would almost turn
29:08 around the question
29:09 and say you know as a data scientist
29:12 or a machine learning engineer or maybe
29:14 data engineer have you ever felt like
29:16 you were working on an initiative
29:18 where
29:19 it wasn't 100 clear to you why you were
29:22 even doing this and why this problem had
29:24 to be solved or why it was an important
29:26 problem
29:27 or you know as a
29:29 if you ever find yourself having this
29:31 tendency of just going to the solution a
29:33 little bit too quickly
29:36 um
29:37 then
29:38 you know that might be an indication
29:39 that there's definitely some value for
29:41 you and the rest of the team to get in
29:43 techniques like this
29:45 yeah definitely i remember being in this
29:47 situation and even worse than somebody
29:50 when somebody already comes with a
29:51 solution like let's say a manager comes
29:53 and says hey like we have this problem
29:55 and i read an article and the article
29:57 says you can solve this with deep
29:59 learning so here we go
30:01 go figure out how to do this yes and
30:04 then you spend a couple of months then
30:06 you come up with some solution it turns
30:09 out that the problem is wrong and the
30:11 solution to this problem is uh
30:14 yeah not correct and then it's just two
30:15 months wasted
30:17 yes exactly
30:19 yes
30:20 yeah and of course
30:21 you know some of those dynamics you
30:23 cannot just solve by having a
30:25 brainstorming session right if you work
30:27 in a company where that's the tendency
30:29 that the
30:31 product direction is established
30:32 somewhere by really high management and
30:34 you're really in an execution
30:36 focused role
30:38 then of course there might be some some
30:40 bigger challenges to solve
30:42 but
30:43 at least this way of working with your
30:45 team might allow you to
30:48 um
30:48 well start the conversation with
30:51 managers about these type of topics
30:53 where you say hey but look we did the
30:54 research and this is not even the right
30:56 problem to to be solved
30:59 you know anyway or this might not be the
31:01 right technique to to solve it
31:04 let's imagine we have this situation a
31:06 manager
31:08 comes to me or to team to product
31:11 manager and says hey this is the problem
31:13 we think we
31:15 have
31:16 let's solve it with a neural network so
31:18 how do we challenge that person how do
31:20 we challenge the
31:22 uh let's say dct or
31:24 whoever
31:25 um to say no no let's let's
31:28 first figure out
31:29 um what the problem is like how do we go
31:32 about that
31:34 yeah so you know i
31:36 also my times working as a consultant i
31:39 think i've i've gained some experience
31:41 with you know saying no but maybe a
31:43 friendlier way of saying no is asking
31:45 why
31:46 so it's almost as if you know your
31:49 manager or the cto whoever gives you
31:50 this assignment is also kind of like a
31:53 customer
31:54 right
31:54 so just like you want understand the
31:56 customer and their needs and their
31:57 priorities
31:59 you might also want to do the same with
32:01 whoever is giving you these type of
32:03 assignments and then it turns out you
32:05 know they say to you build solution x
32:08 but what it really is again you know
32:10 maybe what they're actually trying to do
32:11 is solve fraud or
32:13 improve their monetization or improve
32:16 the engagement between the
32:18 different users on the platform so
32:20 that's maybe the underlying why that you
32:22 also need to get to with whoever gave
32:24 you that assignment
32:26 and then but your team you can try to
32:27 figure out okay if that's what we're
32:29 trying to do again trying to solve
32:32 frauds then i can go back and understand
32:35 what's within fraud is really um
32:38 the best thing to be addressing
32:41 so yeah i would also see that as kind of
32:44 an engagement where you want to get to
32:46 the bottom of why someone is giving you
32:48 a certain task
32:50 so i guess then
32:52 in this situation
32:54 i or somebody else would ask why right
32:57 and then understand and where this is
32:59 coming from like why do you come to me
33:01 with this problem who yes uh where did
33:03 you hear about this and then maybe it's
33:05 better to also
33:07 involve that person right or maybe if
33:09 this is something that comes directly
33:11 from cto and then i don't know do we
33:13 start a design sprint or how do how do
33:16 we do this or double diamond um
33:18 or like maybe there is even a bigger
33:20 problem and we need to think about
33:22 something else before we start
33:25 yeah
33:26 yeah so
33:27 large organizations maybe others might
33:29 not recognize this if they work in
33:30 smaller teams where you know the cto
33:32 just sits at the desk right next to you
33:34 then you might not have these situations
33:35 but yeah large organizations sometimes
33:38 remind me of this you know chinese
33:40 whispering game where you all sit in a
33:42 circle and one person you know whisper
33:45 something in the air of the person
33:46 sitting next to them and the story keeps
33:48 traveling throughout the room and by the
33:49 end the time that it ends up at the end
33:51 of the line the story has completely
33:53 changed uh i think this sometimes does
33:56 happen in organizations so you know
33:58 everyone gives their own interpretation
34:01 of what it actually is that
34:04 we're trying to achieve um so you know
34:07 again that's one of the bigger
34:08 challenges that you can't necessarily
34:10 solve through
34:11 design thinking or any sort of design
34:13 process and that is probably just you
34:16 know an organization that's in need of a
34:18 very clear
34:19 product
34:20 definition or a very clear strategy
34:22 statement
34:23 but yeah again you go into these
34:25 meetings like that you try to understand
34:27 what the bigger problem is that you're
34:29 trying to solve
34:30 and then whoever
34:32 is responsible for building a solution
34:34 needs to make sure that he or she has
34:36 the team
34:37 that they need to try to
34:40 achieve that so involve their product
34:42 teams usually and then facilitate the
34:45 right process could be a design sprint
34:47 the problem is slightly smaller and
34:50 slightly more clear-cut could be also
34:52 launching news research if you don't
34:54 have that understanding yet of the
34:55 problem that you're trying to solve or
34:57 maybe it's just a lot of analytics
34:59 because sometimes that can be what you
35:01 need to understand
35:02 what it is that that we're actually
35:04 talking about
35:05 yeah and then combining those into an
35:07 insight about the problem to solve and
35:08 then taking it from there
35:11 yeah this changes whispering
35:13 i think it happens quite often
35:14 especially in larger companies where we
35:17 have
35:17 multiple uh
35:19 or say multiple departments and then the
35:22 problem is coming from one department
35:24 then somebody tells to their manager to
35:26 their manager to their manager and then
35:28 the manager at the top talks to other
35:30 manager and then you know there could be
35:32 like i don't know uh
35:34 five six hops before it reaches the data
35:37 scientist who actually needs to solve it
35:39 yes
35:40 and then i guess here the right way to
35:42 do this would be to try to backtrack
35:45 right and then
35:46 get together and try to solve it
35:49 yeah and and also as a data scientist
35:51 don't be afraid to
35:53 you know okay i'm just going to write
35:54 down what i think the scope is and what
35:57 i think the problem is that we're trying
35:58 to solve and why we're doing this so
36:00 just put together a very short scoping
36:03 documents can even be an email
36:05 and just send it back to
36:08 you know all the way
36:09 up to where you think it actually came
36:11 from and see how they how they respond
36:13 to that so just kind of those basic
36:16 projects management tools sound kind of
36:19 boring but they can maybe save you a few
36:21 months of work uh if
36:23 if it helps you focus on the the right
36:25 topic
36:27 so when somebody comes with a solution
36:29 you need to find out why and then you
36:31 need to document this and then you need
36:33 to say okay this is what i think the
36:36 problem is and this is the solution i
36:38 propose right or even without the
36:40 solution just the problem right and then
36:42 you have this document and you you send
36:43 it and then you want to get an
36:46 okay saying okay yes this is indeed the
36:48 problem
36:50 in an ideal world this is how the work
36:52 is
36:54 do we need to have some sort of
36:56 processes in our uh organization like
36:58 and how exactly this should happen like
37:00 i imagine if there are such ad hoc
37:03 requests coming in i guess there should
37:05 be some process like how exactly we
37:07 define the roadmap and how we prioritize
37:10 things right
37:12 like how should it work
37:15 yeah so that's maybe
37:17 you know a broader question also around
37:20 who
37:21 defines priorities in an organization
37:24 and maybe specifically we talk about
37:25 data science
37:27 who defines what the roadmap looks like
37:29 and what you know models the teams are
37:32 going to build
37:33 and who leads that
37:35 and
37:36 i think what you're describing is one
37:38 situation that i've sometimes seen that
37:40 it's you know decided by management okay
37:42 we need deep learning here and then
37:44 let's build it without maybe detailed
37:47 understanding of the problems or the
37:49 total solution space
37:51 um but what i've also seen is maybe more
37:53 the opposite that it's data scientists
37:55 that pitch ideas maybe mostly from a
37:58 technical perspective like hey we have
38:00 this data available in the company and i
38:02 see that we can build
38:04 i don't know a computer vision model
38:06 here based on whatever data we have
38:08 available let's build it
38:10 and
38:11 that's not always wrong but i do
38:15 sometimes see a missed opportunity also
38:17 for product managers to play a larger
38:19 role in defining
38:20 um these ai and data science
38:23 applications
38:24 simply because
38:26 you know the data the product managers
38:28 are usually the ones that kind of at the
38:30 center that have an overview of all of
38:33 the different problems that are out
38:35 there that need solving
38:36 and they could be the ones that very
38:38 nicely say like hey here i see a
38:41 problem area that my customers are
38:43 struggling with and
38:45 you know maybe we can use ai machine
38:46 learning as a as a tool for solving it
38:49 so when it comes to priority setting
38:52 it's of course a very difficult
38:54 broader topic and also
38:56 organizations have different uh
38:57 processes in place for that but
39:00 sometimes i see that maybe product
39:01 managers can also take a leading role in
39:05 at least understanding where ai might
39:08 make sense
39:12 yeah
39:13 and one other thing we wanted to talk
39:15 about here so we talked about design but
39:18 we also wanted to talk about innovation
39:20 right
39:21 and
39:22 how is it all that we talked about
39:24 design and all that communication
39:27 between different people
39:29 how is it related to innovation
39:32 and what are the generations from
39:35 and again you know we just discussed a
39:37 very broad topic they really decided now
39:39 and even part of one called
39:41 um
39:44 maybe again from my perspective how i
39:46 see this topic so
39:48 especially in large organizations a lot
39:50 of the data scientists work on
39:53 uh they work typically on on
39:56 goals for the next three months right a
39:58 lot of large tech organizations have
40:00 these things called okrs objectives and
40:02 key results that are established on a
40:04 three monthly basis which means that a
40:06 lot of the day the science work
40:09 usually fit in a three-month block which
40:11 means that
40:13 they are not huge crazy endeavors but
40:16 there's something that's quite tangible
40:18 and just has a direct impact on a
40:20 certain metric let's try to you know
40:22 tweak the the search recommendation
40:24 models for
40:26 this particular part of the product to
40:28 try to get the performance from a to you
40:30 know five percent better than that and
40:32 of course if all of the data scientists
40:34 working on an organization all have
40:36 these sort of projects where they get
40:39 you know from a to b in a kind of
40:40 incremental way at scale that has a
40:43 large impact
40:45 but
40:46 i do think that that can be a bit of a
40:48 missed opportunity because there might
40:50 be completely new business opportunities
40:52 or completely new product opportunities
40:55 that just don't fit into a three-month
40:58 time period they will take more time
41:00 so innovation for me is trying to find a
41:03 way of working
41:04 to spend part of your organization's
41:07 resources and also figuring out you know
41:11 where might the product or the business
41:12 go in six months or a year from now and
41:16 how might ai machine learning actually
41:19 enable a huge opportunity that that
41:21 wasn't there before
41:24 like you know when you talk about lx how
41:26 can we
41:28 try to do personalization to an extent
41:31 that we don't do right now how can we
41:34 really in detail understand your
41:36 particular
41:38 taste when it comes to furniture in such
41:41 a way that we can find the perfect couch
41:43 that fits into your house
41:45 or when you talk about maybe food
41:47 delivery how can i
41:49 really understand
41:50 you know
41:51 exactly what type of flavors you like in
41:54 such a way that i can find this dish
41:56 that you've never tried before but i'm
41:58 sure you're absolutely going to love it
42:00 those could be really cool and powerful
42:03 ideas that very much change the business
42:06 but it's hard to do them you know
42:07 three-month project so that's why
42:09 sometimes at least i would say you need
42:12 a bit of space uh in an organization to
42:16 try out the more radical ideas and that
42:18 for me is innovation
42:20 yeah because i think we usually work in
42:22 this uh
42:24 i think increments in these sprints in
42:26 this time block i think when like the
42:28 whole agile thing is about
42:30 moving in small things right so you
42:32 don't i don't know spend
42:34 a lot of time working on something
42:37 ambiguous when you can work
42:39 on something tangible that will bring
42:41 results right now right so this is the
42:44 usual way of working right so this is
42:46 the whole agile thing
42:48 but as i understood you correctly maybe
42:49 i'm wrong innovation is not about making
42:52 these small incremental steps but
42:54 instead of you know backing up and
42:56 saying okay
42:58 let's instead of
42:59 doing the next thing we need to do for
43:01 this project let's just
43:03 start from the beginning and see
43:06 if there is a different way of solving
43:07 this problem
43:09 not something that we built on top of
43:10 what we already have but maybe there is
43:12 something like a completely different
43:14 road that will take us
43:16 closer to what people actually need
43:18 right exactly yeah
43:20 yeah so you know as an example
43:24 secondhand cars right
43:27 within all x or a lot of platforms where
43:28 people can buy and sell second-hand cars
43:31 but the core problem when you're buying
43:33 a second-hand car is trust not very
43:36 surprisingly number one emotion when you
43:38 buy a secondhand car sphere
43:40 that you know the seller is lying to you
43:43 the car has been in a huge accident and
43:45 uh they're trying to to hide that or
43:48 actually the car has driven
43:50 twice the amount of kilometers that the
43:52 seller it says it has
43:54 so
43:55 of course you can try to solve for some
43:57 of these issues in kind of an
43:58 incremental way so you'll try to make
44:01 you know
44:04 a model to
44:05 test the reliability of the seller for
44:07 example
44:08 but you can also say okay can we use ai
44:11 machine learning or any other technique
44:13 to solve that trust issue in a radically
44:16 different way you know maybe we should
44:18 build this huge scanner where you drive
44:20 the car into the scanner kind of like a
44:22 car wash it has a lot of ai machine
44:24 learning and at the end of it you know
44:26 every possible detail about the car that
44:29 would be a radically different way of
44:30 solving it i'm not saying that that is
44:32 the way to solve it because then i would
44:34 be going immediately to you know a
44:36 solution that's not what you try to do
44:38 in innovation again it's the same type
44:40 of design thinking where you really try
44:42 to understand the problems and all of
44:43 the solutions before you pick one but at
44:45 least
44:47 you know if you just have three months
44:48 no one is going to try to think about
44:50 building that scanner because it just
44:52 wouldn't fit into the roadmap and of
44:54 course you also don't want everyone in
44:56 your organization thinking about this
44:58 kind of crazy and out of the box idea
45:00 all the time because then of course you
45:01 know nothing actually gets built on a
45:03 daily basis but you might want a few
45:06 people that worry about
45:09 completely different directions
45:12 i heard this story many times that let's
45:15 say somebody works at amazon or facebook
45:17 or some other company
45:19 and they have a problem with something
45:22 but
45:23 they because they don't have freedom of
45:25 solving this like
45:28 in in
45:29 innovative way so they need to make
45:31 these incremental steps so what they do
45:34 is they leave and start a startup right
45:37 then
45:38 they try to in a way to sort of rethink
45:41 exactly how they do things
45:43 and then there is a startup which tries
45:46 to take one of the things that this big
45:48 company does but do it differently and
45:51 then some of these startups become
45:54 bigger right because they solve it in a
45:56 more original way in a better way that
45:59 users like more right i heard many
46:02 stories like that
46:03 not necessarily amazon the reason i
46:05 brought up amazon is because
46:08 i remember
46:10 hearing listening to an interview
46:12 somebody who had a problem with amazon
46:14 with some internal machine learning
46:15 platform and then they thought okay this
46:18 seems like a problem that many customers
46:19 have so they left amazon and they
46:21 started the company
46:23 so this is innovation right so when you
46:26 see a problem and to try to find a new
46:28 way of solving this problem
46:30 yeah yeah i definitely think a lot of
46:32 startups started that way it starts from
46:35 a personal frustration something that
46:37 you've encountered and maybe couldn't
46:39 solve in the job that you have or you
46:41 say okay let me just do this myself
46:44 uh though you know i don't think you
46:46 always have to leave a company
46:48 to
46:49 solve a problem like that
46:51 yeah and maybe my recommendation would
46:54 be
46:55 of course you know if you have a
46:58 you see a big problem and you think of
47:00 an innovative solution um it's not as if
47:02 you know when you pitch it to the
47:03 manager that he immediately says perfect
47:06 here you have a million and a team and
47:07 then let's build it but what you can do
47:10 is try to again
47:11 it would be nice
47:13 but again what you can try to do is in
47:16 that similar way is what i just
47:18 mentioned about the double diamonds you
47:19 can try to collect evidence so you can
47:21 try to collect evidence about why this
47:23 is really an important problem to be
47:25 solved
47:26 and also evidence about why your
47:28 solution might be the right one so if
47:30 you just have a little bit of time and
47:32 opportunity to for example experiment
47:35 and collect some data in that way
47:37 then that might help you build a really
47:39 cool business case and then you pitch it
47:41 to your manager then you get that
47:42 million then you can solve the problem
47:45 i guess if you're in amazon you don't
47:48 want your people
47:49 to live and start a startup right so you
47:51 want to keep them in the company and you
47:54 want to
47:55 encourage them to do in a
47:57 because you know
47:58 you don't know maybe this will become a
48:00 new
48:01 uh big thing right so you probably want
48:04 to this to happen inside your company
48:06 and not have a competitor who is doing
48:08 something similar ideally yes yeah and
48:11 it's you know i hope that that is also
48:15 part of an interesting culture for
48:16 people to work in when i feel like hey
48:18 if i have an idea and i'm actually able
48:21 to prove that it might be working
48:24 and i have the opportunity you know
48:26 maybe to experiment a little bit
48:28 uh and my idea actually makes it so i
48:31 can prove that maybe my idea is even
48:32 better than the one that's already on
48:34 the roadmap that they get that
48:36 opportunity to go and further develop it
48:39 it's kind of also that maybe
48:41 back in the days from facebook that
48:43 whenever you had a good idea you would
48:45 get you know 10 000 users to test that
48:47 idea on that sort of mindset i think is
48:50 very interesting for tech companies to
48:52 have
48:53 if there is a good idea that somewhere
48:54 and the organization pops up that people
48:57 get a little bit of opportunity to um
49:00 to to run with it
49:01 and sometimes it works within existing
49:03 product teams
49:05 sometimes you maybe need a separate team
49:07 to to work on it because
49:09 you know otherwise it interferes too
49:11 much whatever whatever needs to be
49:13 delivered in a certain quarter
49:16 i heard there's a company in germany i
49:18 think it's called europe called zalanda
49:20 they're selling
49:21 [Music]
49:22 clothes
49:24 and what they have
49:25 is
49:27 they have some freedom to work on small
49:30 projects so they
49:31 see a problem and they form a small task
49:33 force so the person who sees a problem
49:36 they pitch to i don't know to who they
49:39 picture hey we see this problem
49:41 give us a couple of one month to try it
49:45 out to see how we can solve it and then
49:48 they would get some people they would
49:50 work only for this limited amount of
49:52 time on this problem and then they would
49:55 show the results and then based on that
49:57 they will decide if they want to keep
49:59 doing what they do and then actually
50:01 build a team around that or just
50:03 everyone goes back to their team so i
50:05 think this is a pretty cool concept i
50:06 don't know how popular it is but i
50:08 remember recruiter from zolander was
50:10 pitching me this concept and i found it
50:12 pretty interesting
50:14 do you do you know if this is something
50:16 companies often do
50:18 and do you think this is a good thing
50:21 yeah so as i mentioned you know there's
50:23 different ways in which you could
50:25 make sure that you spend part of your
50:27 resources on looking a little bit
50:29 further than just next three months and
50:31 maybe going to
50:32 ideas that are a bit more radical one of
50:35 them would indeed be these ad hoc task
50:38 force like teams i've also worked with
50:41 ifood food delivery company from brazil
50:43 and they have a similar setup and they
50:45 call it jet skis so imagine that you
50:48 know most of the company is this huge
50:51 oil tanker that's very difficult to
50:53 steer and if you steer it it's just
50:55 going to make a small movement and then
50:57 you have sort of the jet ski and it can
50:58 go anywhere and it can try out new and
51:00 different ideas and
51:02 that definitely works but it sometimes
51:04 depends a bit on the organization and
51:06 also the type of challenges that you
51:08 have sometimes it makes sense to really
51:10 have a dedicated team instead of these
51:13 kind of ad hoc task force like kind of
51:16 oh we'll spend a few hours a week on
51:18 this type of teams because sometimes you
51:20 really want to make sure that you have
51:23 um enough resources that um that there's
51:27 really
51:28 a dedicated team with kind of a
51:30 structured way of working that always
51:32 takes into account you know the user
51:34 side to make sure that your ideas are
51:36 linked to the overall ambitions of the
51:38 company rather than leaving it kind of
51:40 up to these smaller teams to hopefully
51:42 come up with something cool
51:45 so
51:46 let me try to summarize innovation is
51:48 about finding new ways of solving
51:51 the problem you already have or maybe
51:53 new problems but not in small steps but
51:56 trying to maybe radically
51:58 trying to find a radically different way
52:01 of doing this but then you need to
52:03 give people
52:04 some
52:05 time to actually work on this because if
52:07 all 100 of their time is taken by these
52:11 incremental things by these okrs and
52:13 other things they will not have time to
52:15 work on innovation right so you need to
52:17 somehow
52:18 give them the space to work on this
52:21 and this is how you innovate and then as
52:23 a tool for innovation you use these
52:25 things that we discussed in the first
52:26 half
52:27 like all these double diamonds design
52:29 thinking design sprints to actually make
52:31 sure you understand the problem
52:34 build a user build a business case and
52:36 then present this business case to
52:37 whoever decides if you want to
52:40 invest in this team right and then based
52:42 on that
52:43 the company decides what to do is it an
52:45 accurate summary
52:46 definitely a very nice summary
52:51 no
52:52 no i think that that makes a lot of
52:53 sense yeah and the way in which
52:56 companies do this could be different
52:57 could be a dedicated team could be you
52:59 know as you say something that people
53:01 spend their friday afternoons on in kind
53:03 of a
53:04 project by project um
53:07 team
53:08 but
53:09 you know my encouragement would
53:10 definitely be try to discuss these type
53:12 of ideas with your direct team or maybe
53:15 the broader data science community if
53:17 you see an opportunity for more
53:20 innovation or more
53:22 user-centered product development or ai
53:25 application development try to get
53:27 together and try to see if there are
53:29 techniques out there that you want to
53:30 try out
53:33 and i imagine a different challenge so
53:35 let's say i'm a manager and they have a
53:36 lot of fires to fight
53:38 and then now somebody comes to me and
53:41 says hey i have this amazing idea we can
53:43 completely revolutionize the way we're
53:45 solving this problem
53:47 but i have these fires and i don't have
53:49 enough people
53:50 so
53:51 how can i tell them because i probably
53:53 need to tell yeah a cool idea but let's
53:55 go back back to work right so how do you
53:57 answer
53:58 with innovation do i need to talk to my
54:00 manager to ask for i know more people in
54:02 the team or i need maybe to rethink
54:04 priorities
54:06 um is there any a solution to this at
54:08 all
54:10 but if you're always putting out fires
54:12 then yeah there might be some underlying
54:14 bigger issues to fix maybe around the
54:17 resourcing because you know that that's
54:20 not an ideal work that way of working
54:21 for anyone
54:23 but i do believe that in order to keep
54:26 teams motivated they need to have a
54:28 little bit of space and a little bit of
54:30 a say in what they actually work on and
54:32 what the broader team works on so
54:34 hopefully there is definitely an
54:35 opportunity for for a conversation um
54:39 there
54:40 uh but i'm not sure if there's you know
54:43 a a
54:44 one-size-fits-all solution to be able to
54:47 both
54:47 work on really cool and often projects
54:50 and make sure that whatever the team
54:52 needs to deliver actually um
54:54 gets delivered
54:56 a solution direction that i believe in
54:59 is definitely experimentation
55:02 so
55:03 you know if i pitch a really cool idea
55:05 but my proof point is uh me talking to
55:07 another colleague over beer that that's
55:10 not a very convincing idea if my proof
55:12 point is hey i actually very quickly ran
55:14 a survey and it turns out that 90 of the
55:16 customers deal with this issue or hey i
55:19 very quickly changed this i don't know
55:21 button from a to b and through that i
55:23 could actually measure that people
55:25 really clicked on it so they're really
55:26 interested in it then you have a much
55:28 better story but in order to be able to
55:30 quickly do that type of data collection
55:33 or that type of experimentation you need
55:35 quite advanced capability so that you
55:38 know it's not um
55:40 possible anywhere
55:41 but i believe it's definitely a
55:43 direction that more mature tech
55:45 companies need to move into in order to
55:48 be able to together prioritize all the
55:50 resources the right way
55:54 yeah i imagine there could be a station
55:56 when you come to a manager with a good
55:57 idea and the manager says yeah cool idea
56:00 but i don't like it right but if instead
56:04 well maybe not in this direct manner but
56:06 i can imagine that something like this
56:08 happens and we have other priorities and
56:11 we have more things to do than we can
56:12 fit in the sprint in uh in three months
56:15 um
56:16 but i guess the other story would be if
56:18 you use a bit of i don't know friday
56:21 afternoon to
56:23 get
56:23 heart evidence that this is a good
56:25 solution through maybe experimentation
56:27 or something else and then you bring
56:29 this evidence
56:30 to your discussion and it's very
56:32 difficult to say uh yeah work on
56:35 something else right yes yeah and maybe
56:38 one more recommendation for an article
56:40 to read is a recent article by stitchfix
56:43 they very nicely describe as
56:45 organizations are moving towards using
56:47 more data science ai machine learning
56:49 everything becomes more measurable as
56:51 well
56:52 and then
56:53 people find out that their ideas for
56:56 product directions you know if they're
56:59 based on intuition sometimes they work
57:00 but sometimes they also don't work so
57:03 actually if you do work with exercise
57:05 and everything becomes measurable you
57:06 have a much better tool to
57:09 prioritize based on actual facts and
57:12 data rather than someone's gut feeling
57:15 which can also be a bit confrontational
57:17 it turns out that your ideas don't
57:19 always work but for the overall impact
57:21 that you're trying to achieve i think
57:23 that's that's the direction that you
57:24 need to be going into
57:26 it it's a an interesting feeling when
57:29 you think that idea works but then you
57:31 look at the data and it doesn't
57:34 at first it's hard to accept but then
57:36 over
57:37 time you learn
57:38 from that yeah i think it's a good thing
57:40 to
57:42 you know even when you put some time
57:43 into this thing to accept that this
57:45 thing is not working instead of trying
57:47 to
57:48 uh let me tweak this thing like let me
57:50 tweak that thing to just accept okay
57:52 it's not working let's say something
57:53 else let's move on it also it also helps
57:56 to see hard evidence that okay this is
57:59 uh
58:00 don't
58:01 don't try to keep working on this thing
58:05 okay i think we should be prepping up
58:09 i didn't ask you all the questions i
58:11 wanted but is there anything you want to
58:13 say before we finish maybe something you
58:15 wanted to bring up but we didn't have a
58:17 chance to talk
58:19 yeah plenty of interesting topics right
58:22 i guess if you work on the design and
58:25 innovation you can keep talking about
58:27 them for a long time but maybe one um
58:29 encouragement for anyone listening in
58:32 and
58:33 you know they're kind of interested but
58:34 they maybe don't know where to get
58:36 started i would just say these type of
58:39 skills are definitely very learnable so
58:43 you know three years ago when i joined
58:45 process
58:47 maybe the playbook for a successful
58:49 machine learning project hadn't really
58:51 been written and now it's three years
58:52 later i think there's so much
58:54 documentation and so many podcasts like
58:56 this one and so many courses out there
58:58 that exactly describe
59:00 how to do a successful machine learning
59:02 project and it's maybe the same with
59:04 these topics right into design and
59:06 innovation
59:07 at the moment there's no playbook
59:09 written yet but you can definitely
59:12 read about it
59:13 see if you can find a course get
59:15 familiar with the topic and maybe you
59:17 might be the one that's later on writing
59:20 the playbook for how to incorporate
59:22 design into a data science team so
59:25 definitely encouraged to keep learning
59:27 about topics like this
59:29 and a good starting point i think you
59:30 mentioned is this pair from google all
59:32 right yes definitely yeah
59:35 okay so if somebody has questions what's
59:38 the best way to reach out to you
59:41 uh linkedin might be the best channel
59:45 okay
59:46 oh yeah the
59:49 there is a comment can we have links to
59:51 learn more maybe you can send us uh
59:54 these links there that these articles
59:56 you mentioned i think you mentioned
59:58 um about this tick tock and um yes
1:00:02 instagram see you as an algorithm yeah
1:00:04 yeah then the other one about was stitch
1:00:06 fix right yeah there's something else
1:00:09 and then this peer and a pair of google
1:00:11 yes
1:00:12 maybe if you can understand i will
1:00:15 put this in the description
1:00:17 and i think that's uh
1:00:19 all for today so thanks again elizabeth
1:00:21 for joining us today thanks for sharing
1:00:23 your knowledge thanks everyone for
1:00:25 joining us today as well for listening
1:00:27 in and
1:00:29 yeah
1:00:30 that's all for today
1:00:32 thank you thanks for joining