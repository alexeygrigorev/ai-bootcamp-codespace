0:00 hi everyone Welcome to our event this
0:02 event is brought to you by data talk
0:04 club which is a community of people who
0:05 love data we have weekly events and
0:08 today is one of such events if you want
0:10 to find out more about the events we
0:11 have there is a link in the description
0:13 go there check it out then do not forget
0:16 to subscribe to our YouTube channel this
0:18 way you will stay up to date with all
0:20 our future streams like the one we have
0:22 today and last but list we have an
0:24 amazing slack Community where you can
0:26 hang out with other daa indas so check
0:29 it out too
0:30 and during today's interview you can ask
0:32 any question you want there is a pinned
0:34 Link in the live chat click on the this
0:37 link ask your questions and we will be
0:39 covering these questions during the
0:41 interview and I just noticed my nose is
0:43 a bit blocked I hope uh nobody can
0:47 notice it so uh and uh La you'll be
0:51 doing talking most of the time so I'll
0:53 just need I'll just need to ask you
0:55 questions so I hope it will not be a
0:58 problem okay and now I am asking I'm
1:01 opening the
1:04 questions and if you are ready we can
1:08 start yeah
1:11 sure
1:13 so
1:17 yeah so this week we'll talk about
1:19 building a strong career in data and we
1:22 have a special guest today laa laan is a
1:25 Cari melon University
1:27 alumni uh she currently works as a
1:29 senior your applied associate at JP
1:32 Morgan Chase in their specialized
1:35 machine Learning Center of
1:36 Excellence her latest latest research on
1:39 Lo long context evolation of llm was
1:42 published in
1:44 emnlp
1:45 2024 emnlp I guess it's a conference
1:49 right yes great
1:52 so NLP tell us more yeah about that so
1:56 welcome uh welcome to the show laia
1:59 thank you great to meet all of you yes
2:02 yeah um so the questions for today's
2:05 interview are prepared by johana B
2:07 thanks Yana for your help as always and
2:10 before we go into our main topic of uh
2:13 building a strong career in data let's
2:14 start with your background can you tell
2:17 us about your career Journey so far yeah
2:20 U happy to share my journey I think I'll
2:22 keep it short for now uh so I
2:26 after yes we can Del deeper into the
2:29 interesting parts but overall um I
2:32 finished my undergrad back in India in
2:34 2016 I worked for a couple of years as a
2:38 software engineer until I got my
2:41 interest peaked in AIML or data science
2:44 back then it was more of data sciency
2:46 stuff so in 2018 I decided to make a
2:50 switch from uh my S SD profile to an ml
2:54 profile worked for a year or so back in
2:57 India again until I decided to change my
2:59 mind again and pursue my masters so then
3:03 I came to the US uh did my masters from
3:06 Cari melon and here I'm now working with
3:09 uh JP Morgan uh in their ml vertical
3:13 yeah that's quickly a short overview
3:16 about my story but we can talk more of
3:18 the yes yeah because today's so this is
3:21 a typical question we asked um and for
3:25 me it's like I don't know I need your
3:26 reaction so every time we start I start
3:28 with this question but since today's
3:30 interview is mostly about your career so
3:32 this question is kind of like what the
3:35 entire interview but U how did you
3:37 actually become interested in machine
3:39 learning and AI was there specific
3:42 moment or specific project that sparked
3:44 your curiosity yeah definitely the
3:46 interesting part is that while I was
3:48 doing my undergrad I had zero zero
3:52 courses in ml or data science this is
3:54 back in 2016 so uh I think there were
3:58 like very few specialized people doing
4:01 ml that time um and then I I joined my
4:05 regular SD role um with HSBC uh until I
4:09 participated in haakon so it was just
4:11 coming up and uh my interest peed
4:13 through vision although now I'm deep
4:16 into NLP but I started my journey in ml
4:19 with vision models so you know image net
4:22 and those kind of things back in
4:24 201617 they were really hot and popular
4:27 um so I participated in a lot of
4:29 hackathons within my firm um wherein we
4:33 were given this Liberty of experimenting
4:35 with new things that were coming up
4:37 apart from my regular role so that's I
4:41 think definitely a clear moment for me
4:43 where my interests got picked over a
4:45 period of two or three years
4:47 participating in hackathons yesh do you
4:50 remember any of the projects you did
4:52 there yeah uh I think the one project
4:55 that has stood out or in fact also got
4:57 converted into one of the project uh
4:59 product in the bank was like a OCR model
5:03 so uh the bank receives when they
5:05 onboard customers they used to receive
5:08 um something that we called organization
5:10 charts so it's typically like a
5:12 flowchart kind of diagram you can
5:14 imagine but pretty complicated also back
5:17 in 2016 right so um no fancier llms back
5:21 then so we had like really specific
5:23 models first which could parse this
5:26 complicated orc structure to find out
5:29 the relationship between entities and
5:31 then kind of autop populate this form
5:32 you know like what is the hierarchy of
5:35 that organization the board directors
5:37 the people working under who is
5:39 reporting to who so this complicated
5:41 flowchart we had Vision models which
5:43 could first extract like these boxes
5:46 then some Arrow connections so we used
5:48 um depth for search BFS DFS kind of
5:52 algorithms to find those connections and
5:54 then finally some OCR through um Google
5:57 AP Google Cloud I think we were using
5:59 that time
6:00 to extract out the text within these
6:02 boxes and then finally put all of it
6:04 together to you know interpret what that
6:06 diagram looks like so
6:09 um yeah looks pretty knife for today uh
6:13 five 10 years down the line but very
6:16 very interesting stuff back then I mean
6:18 like if you ask me now how to solve this
6:21 problem I wouldn't of course like you
6:23 can just send it to chpt or like I don't
6:27 know any llm and say hey like give me
6:29 the uh orc chart and probably it will
6:32 just work but uh apart from that like I
6:36 have zero idea of how to actually
6:39 approach
6:41 that you were a developer so like you
6:46 even probably had if I have zero idea
6:49 now you probably even had less uh yeah
6:53 IDE approach that
6:55 I yeah it's like a SD approach like just
6:58 hack you know parts of it that's what we
7:00 did last um that time because none of us
7:03 had like any ml background so what is
7:06 the most basic thing that you can do is
7:08 what we did so yeah it was like a
7:09 rewarding experience but definitely
7:11 something that piqued my interests in
7:13 this field I can imagine like if you
7:16 have zero knowledge about that but then
7:18 you manage to hack something together in
7:20 a couple of days just by Googling and
7:22 trying different tutorials and then it
7:24 works at the end like be quite rewarding
7:27 yeah exactly I think also so uh
7:30 currently we have like lot of
7:32 information all around the web that time
7:34 the information was also limited so
7:36 easier to kind of go evaluate all your
7:39 four five 10 options and then just go
7:41 ahead with one instead of hitting your
7:43 head against 100 possible solutions
7:46 right um that is another
7:49 downside so you're saying that before it
7:52 was easier because there were like only
7:54 handful of Articles yes and anything
7:57 that you do would come out as innovative
8:00 because um not many people were you know
8:02 into this field um
8:05 yeah interesting and uh so probably
8:08 these days people wouldn't worry about
8:10 these things right they would just um
8:13 you know I guess I think the first thing
8:15 people would do is just as you mentioned
8:17 asked um any of the vision llms to do
8:20 this for
8:21 you is it something you do now these
8:23 days like do you you're doing something
8:26 with llms right now right or yeah but as
8:29 said I started with vision but now I'm
8:32 deeply into NLP so not U yeah I wouldn't
8:37 say I'm doing anything much on the
8:38 vision side what do you do
8:43 now currently I'm more into um my team
8:46 is specifically focused on benchmarking
8:49 llm so uh my team at JB Morgan we talk
8:53 to a lot of model providers vendors like
8:56 open anthropic we have regular meetings
8:58 with them meta
9:00 we take their models we Benchmark those
9:01 models on our internal data sets so our
9:04 team is like the first entry barrier
9:06 into any model that the bank wants to
9:08 ingest we thoroughly Benchmark on
9:11 quality as well as on you know
9:14 deployment um like latency throughput
9:16 stuff and then we publish um developer
9:19 guidances blogs do webinars for form
9:22 wide uh developers data scientists to
9:26 share our experiences with the model
9:28 publish best practice is you know
9:29 there's these models are really
9:31 sensitive to prompting things keep
9:33 changing there are new model releasing
9:36 almost every week now so how do you keep
9:38 up with these we also negotiate with
9:40 these model providers to give us like
9:42 the best rates uh because there's a lot
9:44 of usage within the form so yeah I'm
9:47 heavily into like benchmarking
9:50 llms that's my I don't know yeah I
9:52 didn't ask you before but this is
9:53 something you can talk about like can
9:55 you maybe tell us more about the
9:57 projects or this is cuz I know in Banks
10:00 like uh some banks take these things
10:03 very seriously and you are not allowed
10:05 to discuss to talk about the details of
10:08 your work like I'm wondering if it's the
10:10 case or you can tell us we can talk
10:12 about the published work that we have so
10:15 that's also a similar domain uh so you
10:17 know benchmarking can be done on
10:19 different aspects like you can do on the
10:21 traditional nlu stuff you can do on Long
10:24 context you can do on code generation
10:26 capabilities math capabilities go into
10:29 multimodal stuff uh so my focus for the
10:32 published work was on uh the long
10:34 context capabilities so you know these
10:36 models claim that they have this huge
10:38 context window of 128k but can they
10:41 really you know read a 200 or 500 page
10:44 book and give you the correct answer as
10:46 they claim to be so delving deep into
10:48 that so yeah we can definitely talk
10:50 about that since it's
10:52 published so short answer can they read
10:54 a 500 page book and give an answer uh
10:58 I'll I'll say yes and no yes on easier
11:02 data sets and no on real data sets so if
11:06 you Tre the published work or you know
11:08 the public benchmarks they are really
11:11 positive they really uh say the models
11:13 are getting better with time but there
11:15 is this um concept of artificial tasks
11:19 or not like simp artificially simplified
11:22 tasks uh which makes it kind of easier
11:24 for the model but when you just use the
11:26 same model in real world especially on
11:29 you know specialized domains like maybe
11:31 Healthcare or Finance you start to see
11:33 pitfalls of these models uh at longer
11:36 contexts so yeah yeah I think so let's
11:40 say now after we finish recording this
11:43 podcast I get the transcript and put it
11:45 to chpt and say hey this is the
11:47 transcript of our conversation can you
11:49 answer the questions uh my questions
11:52 based on the transcript I gave I'm
11:55 pretty sure it will work fine cuz like
11:57 it will be only one hour I don't know
12:00 how to how many pages it translates but
12:02 I'm wondering if I let's say take any
12:05 Andrew huberman podcast like I was
12:07 listening to one of the episodes the
12:09 other day it's like four hours long yeah
12:13 like it's probably the book so and then
12:16 it's like the topics
12:18 are like they go
12:21 deep sometimes like sometimes they talk
12:23 about research like latest research so
12:26 I'm wondering if the model will be able
12:29 to actually correctly answer questions
12:31 there what do you think yeah I have two
12:33 opinions on that so one is uh in our
12:36 studies because it's hard to say you
12:38 know at what um range in the context
12:42 window do the models fall so we kind of
12:44 split it into like less than 32k tokens
12:46 and then greater than 32k tokens there's
12:49 a clear dip like around that um so 32k
12:53 tokens is typically we decided that
12:55 range because at least in the bank we
12:57 have all our use cases usually falling
12:59 within 32k th000 tokens so as you should
13:02 said maybe this transcript uh in my
13:04 experience would be way lesser than that
13:07 uh but maybe the huberman uh podcast
13:10 would be much uh greater so that is one
13:12 thing like uh uh maybe if it's a shorter
13:16 input that you give to the model it's
13:18 better and as you go like you really
13:20 push the boundary up to 128k that's
13:22 where you start to see these uh Falls uh
13:25 in the model capabilities other second
13:27 thing is um the natural language aspect
13:30 of things so if you give these models
13:32 like to summarize or ask some questions
13:35 it's easy for it to kind of you know
13:37 just make up stuff and it's hard for you
13:39 also to uh objectively verify whether
13:41 it's correct or not versus the tasks
13:44 that we evaluate the models on they are
13:46 very objective you know something like
13:48 you can do auto evals on so there's no
13:51 concept of subjectivity there or like
13:53 string you know for this one you would
13:55 do like a roou score or something like
13:57 that but what we do is very specific
14:00 like Precision recall on specific um
14:03 yeah we can talk more about the task but
14:05 uh I think that's another aspect of it
14:07 like that's the realistic task that I
14:10 was saying like just doing it in natural
14:12 language the model will definitely give
14:14 you something and you might think oh
14:15 looks nice you know um because it will
14:18 start reading it will still be grounded
14:21 somewhere in the context but might not
14:24 be exactly correct like should we like
14:29 is rck that like I I'm just wondering so
14:33 the approach before this long context
14:36 models were available would be to chunk
14:39 like every page is a document you chunk
14:43 it you index it you ask a question then
14:45 you retrieve relevant Pages then llm
14:48 summarizes these pages and you get the
14:50 answer right so even when uh we try to
14:54 use it in our bank because now we know
14:56 that these models fail at you know
14:58 around this 64k context although we are
15:01 using these fancy 128k models we do the
15:04 same thing that you said we chunk it
15:06 because we know up to this point the
15:07 models don't fail usually so we chunk it
15:10 and then you know do whatever is the
15:12 downstream
15:13 processing and um this is related to the
15:17 paper you published right yes it's uh
15:21 this was the paper that we talked about
15:23 this
15:25 emnlp uh the name of the paper is
15:28 systematic evaluation of long context
15:30 llms on financial Concepts so that's
15:33 exactly the paper and this is what we
15:34 are talking about right now right yes
15:36 yes this is exactly
15:38 yes did you like writing the paper yes
15:42 in fact it was like my four I mean I had
15:45 some Publications back uh when I was
15:47 studying but um they were
15:50 not uh core into NLP they were all over
15:54 the place because as I said I started
15:55 out with data science spent some time in
15:58 VIs so all my papers were like all over
16:01 the place so this was something that uh
16:03 was really rewarding and also it's like
16:05 a ACL conference which is um really
16:08 reputed in the community so um that was
16:12 that was really nice
16:14 yeah ACL is U something computer
16:18 Linguistics yes Association for
16:20 computational Linguistics yes I did my
16:22 masters also it was related to NLP so I
16:26 was working with mathematical formulas
16:28 in Wikipedia and I remember this
16:31 abbreviation from the days sale yeah
16:35 it's like one of the coolest uh people
16:37 right yeah it's really top tier coed
16:41 highly filtered and yeah really smart
16:45 and brilliant people right like um
16:47 Andrew and G all of these people attend
16:49 these conferences so you can imagine
16:52 yeah cool and um this is pretty
16:56 rare sorry what see uh like this is
16:59 pretty rare from what I see to work on a
17:01 paper working at a company so typically
17:03 when you work at a
17:05 company um like you are concerned uh
17:09 about other things like at most maybe
17:11 you just write a blog post or give a
17:13 talk conference but here you brought a
17:16 paper a scientific
17:18 paper uh for a scientific conference
17:21 like um yeah I'm wondering why like is
17:24 it uh common for people at JPM JP Morgan
17:28 ch to do this
17:30 or um yeah I think so uh the vertical
17:33 that I worked with is uh mlco as I was
17:35 saying so we are um a group of like
17:39 about 150 odd people so we are not tied
17:42 to a single use case or product we do
17:45 work on products uh um we are heavily
17:48 tied to those but we also have this
17:51 Liberty of um some uh creative Liberty
17:55 because we do a lot of new stuff so if
17:58 anything new comes up we do publish Al
18:00 although it's in the industry track and
18:02 we are not allowed to release the data
18:04 but a lot of us keep coming across like
18:07 new findings all across so our teams are
18:09 divided into NLP graphs and then
18:12 multimodal Vision speech we have like
18:15 specialized teams for all of these so
18:18 very active and smart people in each of
18:20 these groups so if anything new comes up
18:22 there's always this um encouragement I
18:25 would say to uh go ahead and publish
18:27 your work but it comes on addition to
18:29 your um you know regular work like it's
18:32 like an extra effort yes okay no focus
18:35 on that but uh it's always
18:38 encouraged extra effort because I know
18:41 that writing a paper especially for a
18:43 top tier
18:44 conference is not easy yeah right and
18:47 then if it comes as an extra as extra
18:50 effort to what you already do let's say
18:53 you work 40 hours per week and then you
18:55 probably need another 40 hours per week
18:57 for a paper like how do you managed that
19:00 yeah this one was uh definitely
19:02 something that me and my manager we were
19:04 excited about because when we were
19:06 trying to like look up things um look up
19:08 the existing research we found limited
19:10 resources so we kind of knew this would
19:13 be like a unique contribution so we uh
19:17 yeah I think it was a lot motivated by
19:20 knowing the scope that this is a
19:22 underexplored area um maybe like for
19:26 things that are welld developed it needs
19:28 a lot more
19:29 um thinking through and um stuff like
19:32 that but for us as soon as we found out
19:35 something in our experiments we were
19:36 like okay this thing needs to go out in
19:38 the public people are not aware of this
19:40 you
19:40 know yeah and um so since our
19:44 conversation is about building strong
19:45 career and data yeah even though we talk
19:49 specifically about your career but I
19:51 think this is really good for a career
19:55 to have Publications because most of us
19:58 we just go to work um I don't know do
20:00 some stuff that things that are related
20:03 to work but then at the end we don't
20:05 share the results and even if we do it's
20:07 typically not top tier scientific
20:10 conferences right so how how how can we
20:13 force ourselves to go this extra mile
20:16 when it's like when it's extra work and
20:18 it's not simple work it's complicated
20:20 work so how how do you motivate yourself
20:24 so you said that there is unique
20:25 contribution but still like I imagine
20:27 that you could be pretty drained after
20:30 work MH then now you also cuz writing
20:33 papers is super difficult at least for
20:37 me like I remember being a master
20:39 student and like I really hated working
20:42 on my thesis like yeah that was terrible
20:46 yeah I when I chose my masters I had
20:48 this option of doing a thesis and I was
20:51 although in undergrads much easier but
20:54 uh I I was so bad at it I was like I
20:56 don't want to do like a thesis of option
20:58 you know like it it so much from you uh
21:02 but um yeah as I said I think um for me
21:05 my manager was uh a really good
21:08 motivator since he had more experience
21:10 in this field so he was pretty certain
21:13 that we should definitely put this out I
21:15 had my times and you know moments of
21:18 Doubt where you feel oh is this worth it
21:19 I'm putting so much effort um but having
21:22 like some guidance or some motivator um
21:25 apart from you yourself is uh really
21:28 helpful um and Al also like published
21:31 work is always um even if it's not
21:33 accepted to any conference we we were
21:36 okay with just like you know putting it
21:37 out on archive um uh because unique
21:41 contributions are always they will shine
21:43 out if nothing else so we we were
21:46 certain of like just putting it out on
21:48 uh putting a good quality paper on
21:51 archive if they get accepted great if
21:53 they do not it's fine but um yeah I
21:57 think uh having the m ation to just
21:59 share it to the community is is nice
22:03 it's hard but it's
22:05 nice yeah so I remember I took part in a
22:08 competition and then after the
22:10 competition I wrote summary and then
22:13 without thinking too much just upload it
22:15 to
22:16 Archive and then people cited like they
22:19 keep cting it even now and the
22:21 competition was something like uh fake
22:23 news detection something like that yeah
22:26 even now even today like I recently
22:30 discovered that there's one more cation
22:31 it's not like overly cated it's yeah
22:34 yeah like 10 or maybe but it was
22:37 something I didn't really think about I
22:38 was just yeah okay I have this uh piece
22:41 of writing let me just quickly put it to
22:44 latch generate PDF and upload it to well
22:48 in archive you upload the latch file
22:50 anyways you just you know I didn't
22:52 really think much about this I just
22:54 uploaded this and then forgot and then
22:57 discovered that people cited and was
22:58 like wow yeah I think that's what I did
23:01 in my undergrad like um when I was first
23:04 into this field so people were like oh
23:07 let's just upload it there I'm like who
23:08 is even going to you know look at you
23:10 know there are hundreds and hundreds of
23:12 Publications out there uh people do want
23:14 to trust credible sources so I
23:17 understand but even archive they have
23:18 like really high standards of you can't
23:20 just upload anything you know they um
23:22 they do see that your paper is well
23:24 formatted Etc and then you need to be
23:26 endorsed by existing me M so it's not
23:30 it's still I would say it's still a
23:31 regulated community so um anything even
23:34 if you upload to Archive it's it's nice
23:36 so yeah I think yeah now I remember that
23:40 I needed to ask a friend to actually
23:43 endorse me yes yeah there were some
23:46 categories where you didn't need
23:47 endorsement like I think uh so for
23:49 machine learning you needed endorsement
23:51 but for information retal back then you
23:53 did not maybe now things changed it so I
23:56 think
23:57 I I just
23:59 my earlier work like my my thesis for
24:01 example I just uploaded toal because
24:04 like it's kind of related to NLP to some
24:07 extent correct correct yeah I uploaded
24:09 to um I think computer science and that
24:12 required endorsement yes okay yeah but I
24:15 think so from what I understood by
24:18 talking to you right now you we're
24:23 always looking
24:25 for you were looking to go the extra
24:27 mile so when you you were working as a
24:30 software developer a software engineer
24:33 you were taking part part in these Hons
24:36 right so now you prior to search papers
24:39 even though it's extra work uh it's on
24:42 top of what you do but you still do this
24:45 um so what it means to me that you
24:49 engage in all these uh other activities
24:51 extra activities in the space so how do
24:55 you find motivation to do that and how
24:57 it was uh beneficial for you yeah I
25:01 think um in the hackathon space like
25:03 going back to that era 10 years back um
25:07 they were as I was saying they were very
25:08 limited resources and I was pretty
25:10 active on the web that time to you know
25:13 uh self learn I think at that time self-
25:16 learning was really crucial for me I
25:17 just gotten out of my undergrad um and I
25:21 I'm not sure if you remember but we had
25:23 this conversation back in
25:26 2020 I reached out to you
25:29 randomly on LinkedIn and it right now
25:31 yeah it was on LinkedIn yeah yeah we
25:34 talked for a while it's it's it's very
25:36 long yeah what did we talk about
25:39 completely random uh I think I was
25:42 looking out I don't know I was doing
25:44 some pet project and I wanted some help
25:46 around I think Docker AWS Lambda
25:49 something in in
25:50 mlops um and you you were doing these
25:53 Zoom cams back then right and there was
25:55 as I was saying very limited resources
25:57 and I wanted
25:58 quick easy trusted credible help so I
26:02 went through a tutorial found it
26:04 interesting I think I maybe I halted at
26:06 some step because I wanted something
26:09 different from what you had shown in the
26:11 tutorial so I I just like reached out to
26:13 you saying that hey can you help me on
26:14 this I'm not able to figure out and U I
26:17 think that's what I used to do then like
26:21 quickly seek help self-learn and it's
26:24 very interesting because I completely
26:26 forgot about it after reaching to you
26:28 for help we talked about a little you
26:31 gave me plenty of resources I got done
26:33 with it I thanked you and I forgot about
26:35 it until this session came up and I
26:37 realized oh my god I've spoken to alexie
26:40 back then and I remember talking in
26:42 detail about uh mlops and then at that
26:46 time we also discussed about my roles as
26:48 you know I was doing some mentoring
26:50 stuff and instructor I was also
26:52 developing a course with data Camp so
26:55 yeah we talked a lot all over the place
26:56 but um that that's Al that also keeps me
26:59 motivated because I keep seeing people
27:02 you know doing so many new things uh you
27:05 were doing uh this Zoom Camp I was doing
27:07 my own stuff as my ped project so yeah
27:10 it was
27:11 great it's cool so yeah I now went
27:14 through our conversation and I have this
27:17 feeling of
27:18 nostalgia uh like all these uh things
27:20 that we were doing like we were talking
27:22 about deploying with a Lambda and back
27:26 then I think in a yes they just uh there
27:29 was a new feature that you could put
27:30 Lambda inside a Docker container yes and
27:33 serve it and like it was uh I remember
27:35 making a post about this on LinkedIn and
27:37 received like I don't know 500 likes in
27:39 a minute well I'm exaggerating but uh
27:43 yeah like people really like this stuff
27:45 back then yeah yeah cool uh so uh what
27:49 you say is you try to be active you
27:51 self-learn and then when you stuck you
27:53 reach out to people you make connections
27:55 and like all this stuff motivates you to
27:57 do even more and then also give back to
27:59 the community yeah sometimes I feel I'm
28:01 doing uh too much like I get too much
28:04 information out on the web I feel oh I
28:06 want to do this oh this guy is so cool I
28:08 want to also contribute here you know
28:10 and that time uh since I was I told you
28:13 I was in a SD profile my aim was to
28:16 become like a at that time this term of
28:18 full stack Emily was really hot to know
28:22 everything from data processing to data
28:25 modeling like doing your actual modeling
28:27 stuff and then also deployment I was
28:30 really interested in that so you know I
28:32 used to as I said for you you were
28:34 publishing out uh publishing stuff on
28:36 mlops a lot so you were like my go-to
28:39 resource for all of this and then I had
28:41 people to look into more on the data
28:44 processing side I would really speak to
28:46 them a lot uh to get pipeline built um
28:50 yeah that time yeah I was really
28:53 inspired by this full stack Mily role MH
28:58 and today are you still inspired by this
29:00 role or maybe we don't need this role
29:03 anymore yeah I mean uh I don't know I
29:07 I'm not sure if we don't need this role
29:09 anymore but I think now I'm uh I found
29:12 my piece more into NLP um long forgotten
29:16 ruths of SD I do I do work on some SD
29:19 stuff uh but not so much I would say um
29:24 yeah I think I found my calling more in
29:27 NLP
29:28 yeah for me the main idea behind this
29:30 so-called fullstack Engineers or data
29:33 scientist whoever is not be not being
29:38 afraid of things you don't know yeah
29:41 like there are things you need to do and
29:44 who else if not you are going to take
29:46 them right so you can just take
29:49 responsibility and say okay I'll try my
29:51 best to actually do them and if you're
29:54 not afraid of doing everything like from
29:56 back end to front end even if you have
29:58 no clue how this thing actually work but
30:00 you're not afraid to figure this out
30:03 then you are full stack and I think
30:05 people like that even today are still
30:08 like especially today because you have
30:09 ai tools that can help you right yeah
30:12 100% I think and also early in your
30:14 career it's uh it's hard but it's nice
30:17 to get your hands dirty with everything
30:19 so then you don't feel you know that oh
30:21 this is something that I'm not I'm
30:23 unaware of or this does not fall into my
30:26 perview uh and I think now uh at least
30:29 in my role we use stream lit a lot
30:31 because as soon as you develop something
30:33 you don't want to wait for you know and
30:36 how do you pass it on to the leadership
30:37 to try it out Etc or gather feedback you
30:40 don't want to be dependent on the
30:41 engineering team so streamlit I feel at
30:44 least for me it has been a really
30:46 valuable quick spinup tool to just um
30:49 pass it around to the people to see um
30:52 and gather feedback on whatever You'
30:53 have built yeah I remember creating
30:56 react applications
30:58 M which like with stream lead would be
31:00 much much would have been much easier
31:03 yeah instead of spending one hour on
31:06 figuring out how to do like react stuff
31:09 just stream lead but now these days like
31:12 with all this AI it doesn't really
31:14 matter I guess
31:16 yeah so but from what I hear hear from
31:20 you so you've been you were interested
31:22 in the full stack mle role which let you
31:27 discover like let you play with
31:30 different areas yes and um you were able
31:34 to do some things everywhere and then
31:37 eventually you realized okay like it's
31:40 interesting but the thing for me that I
31:42 like the most is NLP right is what NLP
31:47 is NLP yeah I think it's also like a
31:50 consequence of my master so when I went
31:52 into Masters I definitely didn't have
31:54 this thing in my mind that uh I'm going
31:56 to like go deeper into NLP uh but
31:59 because the way my course was structured
32:02 or the people I interacted with uh it
32:04 really influenced me um and also I was a
32:07 part of my program was a part of LTI So
32:10 within CMU language Technologies
32:12 Institute is uh highly coed um Institute
32:16 uh they work a deeply on language and
32:19 speech Technologies so both of them are
32:22 related so because of like the people I
32:25 was surrounded with I think that also
32:26 influenced uh so yeah um going into
32:31 Masters I did not have this thought but
32:33 at the end of two years that's what I
32:35 realized yeah and U so when we started
32:38 this conversation this interview we
32:41 talked about the
32:42 hakon and then we jumped I asked you
32:45 question about what you do CU I think
32:46 you were talking about something so
32:48 previously it was Vision now it's n LP
32:50 so I asked you what do you do and we
32:52 come to kind of jumped like 10 years or
32:54 something like that yes so but uh as we
32:57 continued talking you mentioned that you
33:00 also did
33:01 mentoring uh you were an instructor in
33:04 data Camp can you tell us more about
33:06 these extra activities you did in
33:09 addition to your main uh I don't know
33:13 like main activity because you worked
33:16 you studied but then you did some extra
33:18 work so what was this work yeah sure uh
33:22 all of this work was definitely on the
33:24 side and I think it's just like
33:26 connecting the dots as I was saying I
33:28 was um in my uh sometime when I used to
33:30 be free I used to attend Zoom camps like
33:33 yours and then different others um one
33:36 thing that I attended was like a web
33:38 scraping tutorial and um it was always
33:41 fascinating back then to like web scrip
33:44 you had these restrictions on websites
33:46 who would block you Etc so for some
33:49 reason I was looking up data sets on
33:50 kaggle uh and I realized there was
33:53 nothing there was a lot there were a lot
33:55 of data sets on App Store like Apple App
33:57 Store
33:58 but um not comprehensive ones on Google
34:01 Play Store I don't know why but uh at
34:05 that time at least app store was a
34:07 little I'm not very sure but it looked
34:09 at least it's UI looked much simpler
34:11 than um the Google Play store which had
34:14 like Dynamic loading as you scroll more
34:16 apps show up versus for Apple it was
34:19 like a static thing you do like show all
34:21 and everything shows up in the UI uh so
34:24 I figured maybe that's like the reason
34:26 which is creating such challenge in like
34:28 web scraping the stuff um again this
34:31 dates back to 2019 so I'm just speaking
34:33 from memory uh but that's what I just
34:36 tried and I just put out the data set on
34:39 kaggle after scraping it and it became
34:42 at one point it was like the highest
34:43 voted data set on kaggle after the covid
34:46 data set because
34:47 2019 uh it had like 10 15K upvotes and
34:52 currently I think it's 10th highest I
34:53 just checked it before this webinar but
34:56 um at some point it was trending at the
34:58 second highest
35:00 so yeah just stuff like that right like
35:03 um I think maybe it's luck I think we
35:07 started our conversation with that but
35:10 definitely some motivation there uh to
35:12 do something unique uh and then u i
35:16 published it out on uh kaggle and uh I
35:19 just wrote out a simple notebook doing a
35:21 basic Eda some modeling on that and then
35:24 I had someone from data Camp reach out
35:26 to me saying because it was trending so
35:29 there was it was garnering a lot of
35:31 interest so I had someone from data Camp
35:33 reach out to me saying that they would
35:35 want to you know onboard this as a paid
35:37 project for Learners and at covid self-
35:40 learning or this virtual online learning
35:42 was at its peak everyone was sitting at
35:44 home and doing this stuff so I I got a
35:47 lot of attention on that project I
35:49 firstly made like a guided version of
35:51 that which had some guidelines to guide
35:54 the Learners through it then I also
35:55 created an unguided version of it and
35:58 things just like connected one after the
35:59 other so and then once I got this
36:02 opportunity I had like more people
36:04 contact me to do like some mentoring
36:06 stuff um coaching for young um aspirants
36:12 I was also young I I I wouldn't call I
36:14 was like an established person in this
36:15 field but yeah whatever I could do to
36:18 help uh people get into it yeah it's
36:21 really amazing and yeah so I I we were
36:26 talking about luck and maybe yes we can
36:28 attribute this to lck uh but I remember
36:31 talking to Eugene Yan um yes had an
36:35 interview with him um a while ago and
36:39 what he said about luck I still remember
36:41 he said
36:43 um like imagine you want to shoot an
36:46 arrow like you have a bow and you want
36:48 to shoot an arrow and then you have some
36:49 goal some think where you want to do it
36:52 if you don't shoot any arrows arrows
36:55 then you're not going to hit the target
36:57 right yeah but then if you shoot 100
37:00 then maybe at least one of them
37:02 will you know reach the target yes but
37:05 you need to be shooting right
37:08 yeah yeah I call about you know this
37:11 opportunity like there are opportunities
37:14 standing at your door you just need to
37:16 like open and peek out and for you that
37:19 was like this
37:21 um tutorial that you attended you
37:23 scraped Google Play store and then it
37:26 had the Snowball Effect of bringing more
37:29 and more opportunities yes definitely
37:32 yeah I think data Camp was something
37:34 that really highlighted my contribution
37:37 to this and then of course I got more
37:39 confidence that maybe you know I can
37:41 reach out to more people so I started
37:43 also getting involved I also reached out
37:45 to a lot of people on my own to have
37:47 these opportunities on the side I I got
37:50 interested in this community aspect of
37:52 things so I started with the women in
37:55 data science and then uh um small
37:58 communities like that um they have like
38:01 local Regional chapters and then I
38:03 reached out to more industry players um
38:06 to get more of this mentorship
38:09 experience okay yeah that's really cool
38:13 and then what did you do as a mentor
38:15 what kind of things you helped people
38:18 with oh I think all around the place so
38:21 starting with uh in one of the
38:24 organizations I was literally like an
38:25 instructor so I created my own modules
38:29 um to like uh do uh teach basic uh data
38:33 science Eda all the way up to modeling I
38:36 think we went from regression to
38:39 classification covering the I think the
38:42 last thing we covered was decision trees
38:44 random Forest boosting that kind of
38:46 stuff um but all of this was really
38:48 targeted at uh of course introducing the
38:51 candidate to the module but also with a
38:54 focus on interviews so um all of these
38:57 uh industrial organizations they they
38:59 target you know theyve offer these
39:01 packages that you collaborate with us
39:04 and then we ensure you we help you in
39:06 your interview prep so a lot of mock
39:08 interviews um along with teaching um and
39:12 then uh resume refinements
39:16 LinkedIn um LinkedIn reviewer reviews um
39:20 and uh with the other organizations like
39:23 women in data science it was uh targeted
39:26 at uh just just like open mentoring uh
39:29 you can come and talk whatever at
39:32 whatever stage you are in your life so
39:33 one time I had this really nice
39:36 conversation with a college student who
39:39 was really brilliant and smart but there
39:41 were a lot of things for me also to
39:42 learn from her she was applying to like
39:44 she had already Googled uh interned with
39:46 Google and stuff so I was also equally
39:48 curious you know about the interview
39:49 process and stuff so uh just open
39:52 mentoring sessions with these
39:54 communities so come and talk whatever
39:56 you want so it's like a two-way
39:59 exchange yeah yeah maybe you have an
40:03 opinion about that so some might say
40:07 yeah but you got lucky your data set
40:11 turned out to be trending and all these
40:13 opportunities happen to you like what
40:16 would you say like how would you um
40:19 encourage people to still try cuz like
40:22 it's unlikely if I just do a data set it
40:25 will become trending I actually upload
40:26 it quite a few data set on Goggle yeah
40:29 none of them were trending like uh I can
40:33 check how many upwards I had but like
40:37 even if it's I don't know 1020 like it
40:40 didn't really make that much
40:43 difference yeah so but still I feel like
40:47 my contribution was quite good so what
40:50 would you say like um how can we still
40:54 you know yes there is some luck uh maybe
40:58 how I say contribution of luck but still
41:02 like maybe even this even without that
41:05 you would have done many many other
41:07 things yeah so what do you think about
41:09 this yeah um definitely I mentioned luck
41:13 because as you said it it was also you
41:15 know at that time as I said covid time
41:17 was really hard people were really
41:18 active on kaggle that time maybe the
41:20 time factor is a lck but um I started
41:24 Say by saying that uh it was not a
41:26 random decision to just get up one day
41:28 and scrape uh you know Google Play Store
41:31 I had been looking into it uh um I'm
41:34 speaking from memory but I do remember
41:36 this thing from five years back that I
41:39 spent good amount of time to see what
41:41 data sets are existing on kaggle and
41:44 then why is this you know missing like
41:47 it stood out to me that it was missing
41:49 so it was um it was something that went
41:52 into my brain making me think that
41:54 evaluating all possibilities and then
41:56 fig figuring this out that maybe this is
41:58 a gap and then a gap in a way also
42:01 comparing it with the uh Apple App Store
42:04 that okay there are data sets for this
42:06 but not for this one so there's
42:08 definitely something and it was not you
42:10 know just like a matter of a code piece
42:12 that I wrote and it did the magic uh
42:15 there were Bots who were blocking me I
42:17 had lot of emails from people saying
42:19 that I I got some I don't know I don't
42:22 remember but I did get some
42:24 notifications uh saying that you need
42:27 something about you know blocking and
42:29 then this is not allowed etc etc I found
42:32 my ways out I spoke to a lot of legal
42:34 people there one of my friends told me
42:36 that you need to license it under this
42:37 otherwise you can be in trouble
42:39 something like that so it lot of work
42:43 yeah it was not just like scraping it
42:44 and uploading it uh yeah uh so there's
42:48 luck but there's also some thought that
42:50 I I would say analytical thought that
42:53 went into why I want to do this yeah
42:56 yeah I think this is even more important
42:58 than the result right m i mean yes it
43:02 got trending and a lot more people found
43:05 you but because of the effort you
43:08 put there like you would also put
43:11 similar amount of effort into a new
43:13 project and then one way or another yeah
43:18 and you can't predict these things like
43:19 you just you can't predict these yeah
43:21 you just have to do it if it happens it
43:25 happens yeah I just checked my kle kle
43:28 data sets so the largest one has
43:32 171 U uploades which I think is decent
43:36 so it was a data set with uh images of
43:39 different U clothings clothing like
43:42 pants shorts
43:44 t-shirts doing
43:46 some um image classification back then
43:51 yeah I I actually might have I don't
43:53 know because some at some point I was
43:55 also building a fashion recommender
43:58 system just as a pet project so I did
44:01 look into lot of um uh these cloth data
44:04 sets on kaggle I might be one of the up
44:07 voters because I did download a ton of
44:10 data sets from kagle on these clothes
44:13 yeah the reason I had this data set it
44:15 was also it's not like I woke one day
44:17 woke up and thought okay let me just do
44:20 a data set with clothing items no like
44:23 there was a reason and the reason was
44:25 that um so I was writing a book and then
44:28 I needed to actually include pictures of
44:30 the data set in the book and all the
44:33 other clothing uh data sets were scraped
44:37 yes and then okay like if I use a data
44:41 set let's say from Amazon or zand or
44:43 some other company then images belong to
44:48 them yeah you have if I print them in a
44:52 book then I might get into troubles
44:54 right so then I thought then then I need
44:57 to create a data set that has uh a good
45:01 license that allows me to actually do
45:03 this kind of stuff yeah one of my friend
45:06 was into a lot of open- source uh
45:09 contribution that time and he pointed me
45:11 out you know that there are these five
45:13 types of licenses you should go quickly
45:14 and put this on your data set otherwise
45:16 you could be on trouble something I
45:18 remember discussing with him in
45:21 detail yeah so we have a few questions
45:24 um how can a career
45:27 pivoter somebody who's changing career
45:29 career changer changer without a
45:32 computer science degree or the main
45:34 background break into
45:37 Data yeah um I think uh there's lot of
45:42 uh okay when we talk about data there
45:44 are lot of data roles at least in the
45:46 industry that I know uh I'm in Seattle
45:48 it's a hub of Microsoft and um Amazon
45:53 lot of people here are from non uh CS
45:56 backgrounds couple of them in my own
45:58 friend Circle um all working for these
46:01 big Tech Giants uh in data roles so I
46:04 think the main thing that comes to my
46:06 mind if you're completely non uh Tech
46:09 person a pro technical product manager
46:12 is something that people find easier to
46:14 get into because uh you have this
46:16 business Acumen uh on your hand you have
46:19 these those skills and all you need to
46:21 do is know a little bit about how
46:24 software engineering works not like
46:26 actual coding or stuff like that but
46:29 just how you know SQL works or things
46:31 like that uh that's a very good entry
46:33 point technical product manager uh my
46:35 friend who did like civil engineering
46:38 did his Masters also in civil
46:40 engineering and then got into senior
46:42 Tech uh product manager in uh Amazon so
46:46 um yeah I think product manager is one
46:49 thing and then if you are uh early in
46:51 your career I believe like bi roles
46:53 that's also quick dirty uh some skills
46:57 uh that you can quickly check like tabue
47:00 or um basic SQL stuff and then just get
47:04 going on learning on your own self-
47:06 learning is crucial but it I feel those
47:10 roles still require some effort because
47:11 you do need to spend sometime hardcore
47:14 learning those Technologies but product
47:17 manager I think it's easier to get
47:20 through MH yeah and um yeah so we talked
47:25 about Eugene and Eugene also lives in
47:27 Seattle
47:29 yes I didn't realize that you from the
47:32 same city and uh yeah but it's a big
47:35 city right it's not like you can
47:36 randomly St like running to people on
47:38 the street yeah I mean uh I'm very close
47:42 to the area where all these Amazon
47:43 offices are so yeah every day I do look
47:48 some I do come across someone familiar
47:51 either from my undergrad Masters or just
47:53 from LinkedIn yeah very very active
47:56 people yeah
47:58 so another question can you please
48:00 highlight me mentorship networking
48:01 opportunities which were
48:04 helpful um so is this about um getting
48:07 those opportunities for to be a
48:10 mentor so I the way I interpret
48:13 interpret this question is you had some
48:16 mentorship opportunities but then some
48:18 of them ended being helpful for you in
48:23 some way oh I see uh yeah I think so
48:28 getting those Mentor roles is uh
48:31 typically just through reach outs um
48:34 cold emailing LinkedIn messages um stuff
48:38 like that uh and once you build like
48:40 certain rappo with these people these
48:42 are huge communities then it becomes a
48:44 word of mouth so if you have done like
48:47 good networking putter good um good name
48:50 to your work um then it becomes a word
48:54 of mouth and that's how you kind of
48:56 excel in these community-driven uh
48:58 programs um because cold emailing or
49:01 these stuff can get you into it but like
49:04 to build you know a good uh trustworthy
49:07 uh relationships or networking as we
49:09 call it it's mainly through Word of
49:11 Mouth in my opinion yeah actually so now
49:16 again speaking of the our conversation
49:19 the earlier conversation um so I
49:22 scrolled up and saw that we talked about
49:24 mentorship and the reason I asked you
49:26 about that is because I was doing some
49:28 mentorship myself back then so I was
49:29 interested in your perspective on these
49:33 things and
49:35 um for me what was helpful is
49:40 uh maybe just structuring my knowledge
49:43 around things so people come to you with
49:45 requests and then you try to help and
49:48 then this way when you speak when you
49:50 say things out loud it helps to
49:53 structure these things in your head and
49:54 then it becomes easier to use this
49:57 information like in other settings that
49:59 was the main kind of highlight for me
50:01 and then like when
50:03 people one year after that right hey
50:06 like thanks because of the interview
50:09 because of the session we did together
50:11 like now I work at this company so
50:13 thanks a lot you changed my life and
50:16 like that's so cool yeah I think uh that
50:19 kind of exposure I only got with my U
50:22 open-ended mentorship experiences uh
50:25 since you work independently you had a
50:26 lot of this but I was tied to like more
50:29 uh formal organized structures with
50:31 these different that I was working with
50:33 so there were set expectations and goals
50:36 you know less open-ended stuff but there
50:39 is more fun in the open-ended stuff
50:41 because as you say like once you it's
50:43 just a normal conversation it's not like
50:45 oh I'm going to you know give out to you
50:47 some magic tricks that is going to
50:50 change your life but uh when you hear
50:52 back from people some years down the
50:54 line that it helped uh that's a really
50:57 nice feeling that mentorship uh rewards
50:59 you
51:00 with yeah thank you so um another
51:04 question in in relations to Alex's
51:07 question on successful data sets I
51:08 wanted to ask Lavania Lavania what she
51:12 considers to be the other important
51:13 element of a portfolio for data science
51:15 so basically the question is okay data
51:18 set is one option but what else or what
51:22 instead of a data set we can include in
51:24 our portfolio oh yes um I think the data
51:28 set itself I don't think it's even
51:30 mentioned on my resume because once you
51:33 go out for these roles uh these are like
51:35 your pet projects it's nice to show your
51:38 curiosity for this stuff or like just
51:40 talk about it at the end of the
51:41 interview but it's hardly not going it's
51:44 like sadly not going to get you the job
51:47 perspect job per se because those
51:50 requirements are different like uh so uh
51:53 to build your portfolio I feel um
51:56 so one thing is like you can stand out
51:58 in the community through these pet
51:59 projects and your extra stuff but if
52:02 you're talking about portfolio in terms
52:04 of targeting jobs uh I think that's a
52:07 different kind of effort that uh is
52:10 required so yeah maybe like this
52:13 requires a follow-up question U or a
52:15 clarification on what portfolio are you
52:17 talking about is it more like you know
52:19 just U standing out or building
52:22 networking opportunities or more
52:23 targeted at job application maybe let's
52:25 cover two like if we want both of them
52:28 so if we want to uh Target networking
52:33 how the portfolio should look like and
52:34 if we want to Target job opportunities
52:36 how it should look like yeah okay sure
52:39 let's talk about um just Community build
52:42 uh networking aspect of it so honestly
52:44 none of the things that I did give me um
52:48 any jobs or roles I've OB obviously just
52:51 in um worked with two companies but none
52:54 of them like got me anything from my uh
52:57 extra stuff that I did I definitely
52:59 talked about it at the end of my
53:01 interview because I wanted to Showcase
53:02 you know that I am uh genuinely
53:05 intrigued or interested in these pieces
53:07 so that's a good thing that you can add
53:09 at the end of your interview but um
53:13 portfolio building for this stuff uh I
53:15 think just start out as we were
53:17 discussing earlier in the session that
53:20 quick hands dirty self-learning doing uh
53:23 a lot of stuff on your own exploring
53:26 stuff and one of these things will click
53:28 something or the other will happen if
53:29 you are putting in genuine effort so um
53:32 portfolio building data set scraping was
53:35 cool back then I'm not sure how many
53:36 people are still doing it now uh but now
53:39 I think there's lot of uh focus on
53:42 people publishing tutorials on rag
53:44 agentic stuff uh lot of Buzz around
53:47 agents right right now so anything
53:50 unique that you can find out in that
53:51 space is definitely going to stand out
53:54 uh in this current uh world but this is
53:57 obviously limited to NLP I'm not sure
53:59 about other domains um portfolio
54:02 building um mentorship would uh help uh
54:07 some both ways like being a mentor or a
54:10 mentee you it's basically like a
54:12 brainstorming of ideas with like-minded
54:14 people so as you got from me that oh web
54:18 scraping data set Help Me Maybe from for
54:21 someone else something else worked out
54:22 so they could share that with you right
54:24 talking to multiple people just friendly
54:26 opinions suggestions guidance that could
54:29 be another thing um in terms of job
54:33 building I think that's a very different
54:36 um in my experience at least I'm talking
54:38 about the US economy it's very different
54:41 it's highly competitive so um job
54:44 building uh profile building for job
54:47 search targeted at job search is going
54:49 to be lot of lead code um and uh lot of
54:54 conceptual
54:57 drilling uh going deep into the concept
55:00 so that you are able to answer all
55:03 interview questions precisely there's no
55:04 beating around the bush um and then Mark
55:08 interviews yeah but this more for
55:11 Preparation to pass the interview but uh
55:14 uh I guess when it comes to portfolio
55:16 there are some projects that could be
55:20 yeah so I don't know let's say if I want
55:22 to work at Amazon then I can think what
55:26 are the problems that Amazon
55:28 solves and then do a project I know
55:32 maybe recommender system or I know
55:33 search maybe Amazon is a bad example
55:36 because it's so huge and there is
55:37 everything maybe there is a smaller
55:39 e-commerce company and you can get some
55:42 e-commerce data set and let's say do a
55:44 project on search right and then uh
55:47 especially if you want to work in this
55:48 area and when you have an interview with
55:50 them you can say okay yeah like I maybe
55:54 don't have to really professional
55:57 experience but I did this project on
55:59 search which we can talk about and then
56:01 all of a sudden you have things to
56:03 discuss on the interview for sure I
56:05 would just add to that that sometimes
56:07 like when I also interview people for
56:09 roles we do value um in uh like some
56:13 industrial experience more than like pet
56:16 projects because only because of scale
56:18 because when you are doing something on
56:20 your own there is no real testing or
56:22 like no feedback from actual users so
56:24 you can just come and tell me you know I
56:26 got like 90% accuracy but that's not
56:28 verified right versus so I understand if
56:31 you're starting out new it it's like a
56:34 vicious circle you want to get into the
56:36 industry and then I'm asking you to get
56:38 a industry experience to be able to get
56:40 into the industry so what I'm trying to
56:42 say is that uh you can still do like pet
56:45 projects but be associated with like
56:47 some or the other organization at my
56:49 time I was looking into this uh
56:52 organization called uh OMD om
56:56 so they do like small projects but they
56:59 are tied they have industry experience
57:02 uh industry experts looking over your
57:04 projects so they organize students and
57:06 uh people into like uh data scientists
57:10 project manager industry experts and all
57:12 of them work on a single project apart
57:14 from their regular roles whatever they
57:16 are working on so that's a good way to
57:19 like showcase that you are still uh
57:22 building your portfolio but it's not
57:24 completely your PR project right there
57:26 are others looking into it equally
57:28 invested into the same project and it
57:31 has some actual impact in the real world
57:33 so there are many organizations like
57:35 such who do these stuff so for
57:39 applications targeted at Job building I
57:41 think that's a nice way to build your
57:43 portfolio yeah amazing and on this note
57:46 I realized that I'm late for another
57:48 meeting so I need to run but that was an
57:51 amazing conversation laia so thanks a
57:53 lot for uh doing that for agreeing to
57:56 this interview and I had a lot of uh fun
57:59 talking to you and also some memories
58:02 from you know those times uh couple of
58:04 years ago so thanks a lot for finding
58:06 time to do this and uh yeah I wish you
58:10 success yeah thank you so much I'm sure
58:13 we're gonna stay in touch even more no
58:15 more Nostalgia chats yeah yeah and yeah
58:18 also to everyone thanks for joining us
58:20 today for asking questions for being
58:22 active um um enjoy the rest of your week
58:25 and