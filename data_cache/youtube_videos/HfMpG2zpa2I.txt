0:00 hi everyone
0:01 uh welcome this event is brought to you
0:03 by data talks club which is a community
0:06 of people who love data
0:07 and uh today we have um we have weekly
0:10 events and today is one of such events
0:13 so if you're interested in other events
0:16 we have in our schedule
0:18 go to
0:19 the description and there is a pin
0:21 linked and kaiten just
0:24 joined so let me let her in
0:28 hi i'm so sorry to be a few minutes late
0:30 i could not get
0:32 my monitors to work so okay
0:35 i
0:36 am doing already the introduction so you
0:39 are um
0:40 oh
0:42 yeah so i'm just saying that if somebody
0:44 wants to see what other events we have
0:46 at data talks club they should go to the
0:48 description
0:50 there is a link there click on this link
0:52 and you'll see other events and there is
0:55 this big subscribe button if it's a
0:57 subscribe for you you should click on
0:59 this and then you'll get subscribed to
1:00 our youtube channel and get our videos
1:04 in your feed and then finally join our
1:07 amazing slack community where we can
1:08 talk about all data related things if
1:12 you have any question during today's
1:13 conversation you can ask your question
1:16 in using slido so there is a pin linked
1:19 in
1:20 the live chat so just go there click on
1:22 this link and ask your question
1:25 i think that's it
1:27 so your monitor is working now
1:30 no i actually just gave up and um
1:33 i'm going monitor list and i don't have
1:36 an external microphone or camera so i
1:39 apologize for everything being the
1:41 default
1:42 but we'll survive
1:44 that's that's good
1:45 uh well your background is really cool
1:49 thank you i'm on my front porch which is
1:51 actually the source of this whole
1:53 problem that i've been working on my
1:54 front porch for like three weeks and i
1:56 haven't actually used my desk and so i
1:59 really should have planned more than
2:01 seven minutes ahead on that but
2:03 yeah that's why i usually send it in
2:04 white
2:05 five minutes before so maybe for the
2:08 next speaker i'll do this seven minutes
2:11 before
2:12 i know
2:14 uh i should really know better so what i
2:16 also usually do is i take a picture and
2:18 share it on twitter so
2:21 [Music]
2:26 okay i took the picture so let me
2:29 quickly shade um
2:33 just one
2:34 moment pictures
2:37 you're not on twitter are you i'm not
2:41 so too many social networks
2:43 i don't use any social networks but too
2:46 much
2:47 conversation to be had yeah you probed
2:50 enough uh it's like it's enough right
2:53 for you
2:54 yeah okay
2:55 um so let me just pull uh the notes so i
2:59 have the notes near me are you ready to
3:01 start as well yeah
3:04 okay then let's start
3:07 so this week we'll talk about the last
3:10 mile of data and we have a special guest
3:11 today caitlin caitlin is the vp of data
3:14 and business operations and troll
3:17 recommends where she helps brands buy
3:20 back and resell their products at scale
3:22 previously she led data teams in
3:25 crowdfunding companies and in
3:27 self-publishing she's also an admin and
3:30 a co-founder of amazing slack community
3:33 locally optimistic
3:34 and actually there is a funny story so
3:36 when i just started data talks club
3:39 one of you of the few first members he
3:42 was also on this podcast a while ago
3:44 arbit he asked me hey why did you create
3:47 this community your community if it is
3:49 all locally optimistic i thought locally
3:51 what
3:53 and then he basically invited me to look
3:55 at optimistic and then i found out about
3:59 this slack community so if you're into
4:01 analytics and data things in general so
4:04 do check it out
4:07 yeah but who knows what would have
4:08 happened if i knew about locally
4:10 optimistic but
4:11 maybe we wouldn't be talking now
4:13 yeah yeah but you know i think the fact
4:16 that there are multiple organic
4:19 emergences of these communities really
4:21 just speaks to like how much
4:24 data practitioners really like need that
4:26 community we're still figuring out so
4:28 much
4:29 uh and i you know i find
4:31 the community to be so helpful
4:35 yeah so welcome anyways
4:38 thank you
4:40 yeah before we go into our main topic of
4:43 conquering the last mile of data and we
4:44 will talk what it actually means let's
4:46 start with your background can you tell
4:48 us um in a few words about your career
4:51 journey so far
4:52 yeah um
4:54 so i started my career working for a
4:57 small very involved private equity firm
5:00 uh which was a really good six-year
5:03 mashup of kind of financial modeling
5:05 skills like you might get an investment
5:07 banking plus
5:09 bouncing from project to project in the
5:10 way you might in consulting so i did
5:12 everything from evaluated investments
5:14 designing incentive compensation plans
5:17 serving as like a temporary general
5:19 manager for one of our companies and so
5:21 i spent a lot of time in that role
5:23 making decisions based on data but i
5:25 actually did not even know where it came
5:27 from
5:28 i would just email someone describe what
5:30 i need and then get a csv back so it was
5:33 very black box
5:34 and i was instead really focused on how
5:37 to use that data to make decisions to
5:40 really spend a lot of my time analyzing
5:41 it
5:42 framing trends really understanding kind
5:44 of where we should go from here and also
5:46 the the typical spending hours creating
5:49 the perfect chart in powerpoints um
5:53 so ultimately for a lot of different
5:55 reasons i decided to leave private
5:57 equity and settle into a single company
5:59 and
6:00 um
6:02 to be honest i wasn't super thoughtful
6:03 about it but i was really lucky and i
6:06 ended up in an analyst role at a
6:09 self-publishing company
6:11 so as a data team of two i thought the
6:14 role was going to be more like fp a kind
6:16 of in my wheelhouse of using data but
6:18 but not creating it um and then very
6:21 quickly it became a lot more technical i
6:23 went from like
6:24 very nervously changing where clauses to
6:28 um you know trying to write php for a
6:31 home-coded daily sales email it was
6:33 it escalated quickly so
6:35 um
6:36 that was a really amazing experience we
6:38 it was around the same time redshift was
6:40 emerging but we were by no means on the
6:41 cutting edge everything was kind of home
6:43 baked and
6:45 you know we didn't have any the
6:47 user-friendly tools that we have today
6:48 so from there i moved to the bay area
6:50 started working for a company that was
6:52 in the middle of the transition to a
6:53 much more modern data stack
6:56 and that sort of began my love affair of
6:59 with modern data tooling enabling teams
7:02 um
7:03 and
7:04 so i ended up leading a team at that bay
7:07 area startup ultimately leaving to build
7:09 out
7:10 a team from scratch
7:11 which has been a really fun experience
7:13 and i still really love the technical
7:15 side um
7:17 it makes me really happy to just
7:18 disappear for a few days and go you know
7:20 write some code
7:21 but
7:23 especially as my role has grown i am
7:25 keenly aware of that moment where data
7:28 actually changes decisions and i'm super
7:31 focused on figuring out how we can get
7:32 more effective
7:34 at creating those moments making sure
7:36 the data is there making sure that the
7:38 right people are in the room
7:40 to some extent actually thinking about
7:42 whether it is the right answer one very
7:45 well crafted powerpoint slide sometimes
7:47 it is
7:48 um and kind of those challenges are
7:51 something i am spending a ton of my time
7:53 thinking about lately
7:54 do you still use have to use php
7:57 no
7:58 no
8:00 i don't
8:01 i i don't know whether i ever was
8:03 actually successful when i did need to
8:04 use it uh but i spent a lot of my time
8:07 beating my head against it
8:09 but you still you're you're saying
8:12 you're saying that you're still doing a
8:13 bit of hands-on work uh hands-on work
8:15 sometimes a bit of coding right
8:17 yeah um much less so very recently but i
8:21 do still like to kind of get in there
8:23 and
8:24 dig around and i wish that i had more
8:26 time for it now but you know that's the
8:28 reality of kind of
8:30 more strategic roles so
8:32 someday i think i think the kind of
8:34 cycle of my career is likely going to be
8:36 continuously going from
8:38 creating a team from scratch to growing
8:40 it and then
8:41 realizing that i'm too far away from it
8:43 and coming back so i haven't reached
8:45 that point yet but someday i'm going to
8:46 boomerang back
8:47 yeah so you said that
8:50 you're really interested in seeing or
8:52 understanding how data
8:55 is used can be used to change decisions
8:57 right and this is what you're focusing
8:59 on right now
9:01 and the topic today is conquering the
9:04 last mile and i think these two
9:06 things are related so i wanted to ask
9:08 you so actually maybe a bit of a story
9:10 so when i reached out to you and invited
9:13 to to this podcast to government to you
9:16 you wrote me that you've been thinking a
9:19 lot about the last mile of data in in
9:21 data right
9:23 and yeah i thought okay what is the last
9:25 mile and then i started to
9:28 to look it up i
9:29 googled it and then i checked so
9:32 what i want to ask you so what is the
9:34 last mile so and where does this analogy
9:37 come from and why do we
9:39 use this when we talk about data
9:42 yeah so
9:44 the last mile i think is sort of
9:46 colloquially used to refer to the last
9:49 stage of a process whatever that is
9:51 um and it originally comes from
9:54 delivering physical goods or um or
9:57 services to their final customers
9:59 so
10:01 you know getting a physical product into
10:03 a store or a warehouse is is a scale
10:06 problem so it's
10:08 relatively straightforward to kind of
10:10 design and implement solutions for
10:12 problems where you're dealing with a lot
10:13 of
10:14 things moving all at once
10:16 um but then getting that product from
10:19 the warehouse into your house
10:22 um getting that pint of ice cream from
10:24 the grocery store to me in under an hour
10:26 when i order from instacart like
10:28 that is the last mile and that's where
10:30 there's a massive amount of complexity
10:33 when you think about it kind of
10:35 operationally
10:36 and in these classic um
10:38 sort of last mile challenges it's
10:41 often half or more of the cost of
10:43 getting a product to you is really that
10:44 last mile so
10:47 it's
10:48 something where if you solve the big
10:51 problems you're it feels like you're
10:53 most of the way there but really there's
10:54 still a lot ahead of you but if you
10:56 don't solve that last mile you never get
10:58 the value out of the thing you're
10:59 building
11:00 um
11:02 so you know i started in data long
11:04 enough ago that
11:05 um
11:07 everything before the last mile used to
11:08 be really hard
11:10 um
11:11 that was you know it was really
11:13 challenging to actually implement tools
11:14 it was really challenging to make
11:15 changes to etl
11:17 all of this was really difficult
11:19 um
11:21 and so when you were in smaller
11:22 companies like you might not ever
11:24 actually implement a lot of this stuff i
11:26 was basically writing queries against a
11:29 copy
11:30 of the production database
11:33 and
11:34 we there was no data warehouse there
11:35 were no transformations just writing raw
11:37 sql queries every time like that was
11:39 kind of the way that you would operate
11:42 and
11:43 so
11:44 you know
11:45 if you think about this as the last mile
11:48 analogy
11:49 that the era of
11:51 that kind of data work was really like
11:53 when you had to build the railroads like
11:55 it used to be really hard to get from
11:57 the center of one city to the center of
11:59 another city
12:00 you build some railroads and suddenly
12:02 that is
12:03 much easier um
12:06 and so as our tooling has gotten easier
12:07 in data
12:09 it has become much much easier much and
12:11 much more simple to just set up a
12:13 pipeline that gets all your data into
12:15 one place you clean it up maybe with dbt
12:18 or
12:19 whatever transformation tool you use
12:21 you've got a beautiful warehouse that's
12:22 like pretty easy to use
12:24 um
12:26 and that really kind of opened up the
12:27 world of like what can a data team do
12:29 and it made the the challenges seem
12:32 um you know
12:34 much much more surmountable
12:37 um and really you know we kind of have
12:40 this general theme in a lot of analytics
12:43 communities that if you
12:45 can empower a great analyst you solve
12:48 value delivery you can get delivery to
12:50 it
12:52 and
12:53 yes it has changed like the amount that
12:55 one smart person can accomplish is crazy
12:58 compared to what you perceive this uh
13:00 modern data stack right so right yeah
13:03 the things that we didn't you you
13:04 mentioned that in your
13:06 funerals back you didn't have this so
13:09 you had to do a lot of
13:11 things
13:12 you know without really using these
13:14 tools
13:16 just trying things so now
13:18 you're saying the railroads so this
13:20 modern data stack is are the railroads
13:23 right um yeah or or you know the
13:26 interstates like whatever the primary
13:28 form of um
13:30 industrial transportation is in your
13:32 particular locality i'm in the us it's
13:34 all the interstates but in many places
13:36 it is railroads that are you know much
13:38 more efficient and less um less
13:40 environmentally damaging so
13:42 we'll we'll say that it's um a really
13:44 great you know rail system something
13:45 that gets it gets stuff to the warehouse
13:47 gets it to the middle of the city
13:49 um but you still see in like every
13:51 organization that people are really
13:52 frustrated that data isn't available or
13:54 that the data team's work doesn't feel
13:56 like it's impacting the business and
13:59 analysts are feeling like they're doing
14:01 all this work that
14:02 doesn't seem to be valued
14:05 and that's where we're seeing the pain
14:07 of the last mile so we're getting the
14:09 data most of the way there but we're
14:11 still not
14:12 really delivering
14:15 um
14:16 and so you know when you think about
14:19 when you think about data problems you
14:20 can kind of separate out these scale
14:21 problems how do you get it in the
14:23 warehouse how do you transform it how do
14:25 you
14:26 get to you know the most basic dashboard
14:29 get clearly defined metrics to a user
14:32 and then there's the last mile of that
14:34 which is how do you actually get a team
14:37 to change what they're doing based on
14:39 the data and
14:40 enable them to make better decisions
14:43 based on the data
14:45 and much like the last mile of delivery
14:47 is all about how many different houses
14:49 there are and navigating very different
14:52 terrain because this one is uphill and
14:53 this one is you know deep in the woods
14:56 it's it's very similar because you just
14:58 have so many different stakeholders so
14:59 many different ways of making decisions
15:02 the effort to
15:04 understand that landscape and actually
15:07 get plugged into it is really
15:09 substantial but if you aren't getting
15:11 the data to the decision
15:13 then your team just isn't having the
15:15 impact that you want them to have
15:19 does it have like this last mile does it
15:22 have anything to do with smartphones
15:24 because this is what i found so when i
15:26 was googling so as i said a few minutes
15:29 ago that i was looking up and trying to
15:31 understand what actually the last mile
15:33 is and i think sometimes
15:35 i found um something like a
15:40 analogy with marathons like the last
15:43 mile when you run a marathon is the most
15:45 difficult one because you're tired but
15:47 it's pretty close and you really have to
15:49 force yourself to
15:51 you know actually run this last mile so
15:53 have you heard anything like that
15:56 um so i've never thought of that
15:58 as
15:59 sort of the source of this analogy but
16:01 like i think it's very
16:03 it has a lot of parallels it is you get
16:06 to the point where you're like oh well
16:07 i've done most of the work you know i
16:09 ran
16:10 25 miles um it's pretty close right
16:14 um and
16:16 the
16:17 getting things across the line and
16:19 really finishing them can be really
16:21 challenging
16:22 it can feel much easier to start on the
16:25 first smile of the next problem than to
16:28 tie everything up with a neat little bow
16:30 and make sure that people understand how
16:32 to use your products and you know that
16:34 people are really understand the data
16:36 that they understand how to actually
16:38 bring these two things together at the
16:40 time of decision and
16:42 learn from
16:43 the data that you've provided
16:45 yeah there's this pareto principle or 80
16:47 20 principle say it's like you get from
16:51 20 percent of the work you get 80
16:53 percent of the results right and then
16:55 the other way around the remaining 20
16:58 takes 80 percent of the
17:00 of the effort right so would you say the
17:02 last mile is this uh the remaining
17:04 twenty percent
17:06 um
17:07 i think
17:10 yeah i mean i think potentially
17:13 it's similar but i i think it's really
17:16 challenging to get value out of data at
17:18 all if you don't
17:20 really understand how to connect it to
17:22 decisions and that might look really
17:24 different in a lot of organizations so
17:27 you know there are a lot of
17:28 organizations where users are really
17:29 savvy and they understand the data and
17:31 really all they need is access
17:33 and then
17:34 you know
17:36 you create you know a really solid data
17:38 set or a really useful dashboard and
17:40 people are good and they're going to use
17:41 the data and they're going to get it
17:43 into the meetings they're going to make
17:44 decisions based on it and that is really
17:46 kind of all you need to do and then
17:48 there are organizations where for
17:49 whatever reason
17:51 incentives to not look at the data or
17:53 just a lack of comfort with it or
17:56 not fully understanding it there are
17:58 lots of reasons where people don't make
18:01 that last sleep and so there
18:03 it's not really helpful at all to
18:06 put the data out there if people aren't
18:08 going to use it
18:09 so i tend to think of 80 20 more as
18:12 um you know sort of
18:15 the
18:17 figuring out how to tackle particular
18:19 problems so
18:21 for example if what you're trying to do
18:22 is optimize marketing spend
18:25 you're going to get 80 of the effort out
18:27 of twenty percent of the
18:29 or eighty percent of the value out of
18:30 twenty percent of the effort
18:32 in the sense that like you can answer
18:34 all kinds of questions you can dig
18:35 really really really deep and what you
18:37 need to do is find those high leverage
18:39 questions
18:40 where once you answer them you
18:42 get the value
18:44 um
18:45 but even within that you still have to
18:47 make sure that the stakeholders
18:49 the operators who are really
18:52 taking action
18:53 fully understand how to use it that you
18:55 understand how their decision making
18:56 process works that you
18:59 enable the data to be in the room at the
19:01 right time whether that's by a data
19:03 person being in the room or
19:06 making sure that the team really
19:07 understands the tools they have or you
19:09 know
19:10 there's sort of lots of different ways
19:11 that this might take shape but
19:14 that ultimately
19:15 if you're not at that point of
19:18 decision if you're not
19:20 really well plugged into how
19:22 operators are using the data
19:25 then you you can't even get that first
19:27 80 percent of the value
19:28 yeah okay so this is like binary like
19:31 you did all the work and then there is
19:33 last mile and like with smartphone if
19:35 you don't run the last mile you haven't
19:37 finished the marathon right yeah yeah
19:39 yeah so you need to make sure that
19:42 people the decision makers use your
19:44 dashboard to make decisions or use your
19:46 machine learning models to affect the
19:48 customer or whatever whatever data
19:50 product
19:51 you have right so you need to make sure
19:52 that
19:53 uh
19:54 decisions are made based on this or else
19:57 all the effort is uh
19:59 in maine right
20:00 yeah
20:02 oh yeah and
20:04 when preparing for this i read a few
20:06 articles and one of them was saying that
20:10 yeah what what we are talking about here
20:12 is more like
20:14 uh we have some data but where we are
20:16 failing to use it and this is the
20:18 problem the last mile problem that we
20:19 need to solve right so we make all this
20:22 work so how can we now use this
20:25 and the article was saying that
20:26 fundamentally failing to use data isn't
20:29 a technological problem but it's a
20:31 social problem
20:33 right and i think you mentioned that so
20:35 right now we have all these modern data
20:37 stack uh tools right that make it easier
20:40 for us like we have these railroads that
20:42 connect
20:43 to cities right so now technologically
20:46 it's easier
20:47 but so this article is saying that and
20:50 it's saying that it's a social problem
20:51 so why do you think it's the case
20:55 i mean i think
20:58 your data products are
21:00 are fundamentally products so if you
21:02 want someone to use a product
21:04 what they get out of it their benefit
21:06 has to be greater than what it costs
21:08 them
21:10 how hard it is to use whether that's
21:12 monetary cost time cost
21:14 any of that
21:15 and so
21:17 there can be a lot of different factors
21:19 that contribute to that equation being
21:22 off
21:23 so either the benefit is too low the
21:25 cost is too high
21:27 and most of those
21:28 sources of that those issues are really
21:31 social problems like it's about how
21:33 people think about this or
21:36 how they use it not whether the data is
21:38 available so
21:39 there's kind of two ways to make that
21:41 work out you either have to make the
21:42 benefit bigger or make the costs smaller
21:45 um
21:47 i think of the major driver of the
21:49 benefits of
21:51 good data-driven decision-making being
21:54 cultural like
21:56 you have to have a culture of measuring
21:58 people's results and rewarding them
22:01 um your better decision has to matter
22:04 and so if
22:07 you're in a situation where your budget
22:09 next year is going to be based on how
22:10 big your budget was this year and how
22:11 much of it you spent
22:13 then you should just spend your budget
22:16 period
22:17 um
22:19 what you're actually doing with it
22:20 doesn't matter that much if your
22:22 manager just gives you a list of things
22:24 to do
22:25 and you're rewarded by just doing them
22:27 then just do them
22:29 shift the feature
22:30 run the campaign check the box
22:33 and
22:34 on the other hand if you have a really
22:37 clear target that's driven by metrics so
22:40 you're really focused on improving
22:42 conversion acquiring new users um
22:46 you start to actually care about which
22:47 activities are highest leverage
22:49 and
22:50 how to
22:52 get back to the pareto principle like
22:54 how to drive 80 percent of the results
22:56 of 20 of the effort
22:58 and the only way you can understand that
22:59 is if you start to really dig into the
23:01 data and understand
23:02 how you are how your various campaigns
23:05 are performing how
23:06 various parts of the conversion funnel
23:08 are behaving
23:10 and so
23:11 if you don't have those incentives then
23:13 you just there's not a lot of benefit
23:14 from using data and why bother
23:17 and then on the other side of the
23:18 equation you have to keep the costs low
23:20 so
23:21 you have to know how to find the data
23:22 you have to know how to use the tools
23:24 you have to know how to interpret the
23:25 data
23:26 you
23:28 have to have trust in it and not
23:30 constantly be concerned about data
23:32 discrepancies is this real is this true
23:34 hopefully you don't rely on an analyst
23:36 for every question you ask and because
23:38 all of those just add cost to the
23:40 process and so you
23:41 by sort of getting the balance right
23:44 then you get to a place where people use
23:46 the data they bring it into the decision
23:48 making process
23:49 they're you know really using it for
23:51 prioritization they're
23:53 maybe this is sort of one path to
23:55 building a culture of experimentation
23:56 where people really want to test things
23:58 and understand how they performed and
24:00 make better
24:01 choices next time and all of that kind
24:03 of healthy data culture comes from
24:05 the incentives and the skills and and
24:08 training both of which are really kind
24:10 of people problems
24:13 okay so we need to have a healthy data
24:15 culture and i think you mentioned that
24:17 for that data should be must be
24:19 discoverable so people know how to find
24:21 it
24:22 then it must be interpretable people
24:25 need to know how to interpret the data
24:27 and then finally it has to be trust
24:29 trust
24:30 like people need to to be able to trust
24:32 the data because if they
24:35 uh see that something is off they okay i
24:37 don't want to base my decision on this
24:38 dashboard better you know just do some
24:42 base my decision on my gut feeling
24:44 because i don't trust this word right
24:46 and then so you have to have all these
24:49 right and then you also said that
24:52 everything should be measurable and
24:54 people should be able to see the impact
24:56 of their work
24:58 as a number right
25:00 and then when we have that
25:02 then it's possible to like
25:05 then uh our data products make sense
25:07 then we can use them then we can
25:10 show people that it's actually better to
25:13 use our
25:14 data product to make decisions because
25:16 look if you do this your number improves
25:19 right
25:20 and this is not a technological problem
25:22 is it right
25:24 yeah absolutely sort of you have to have
25:26 the
25:26 baseline obviously the the technical
25:29 side of it is kind of table stakes but
25:31 this is where you get into sort of
25:33 the last mile the last mile is all kind
25:35 of the people part it's making sure
25:38 that people know how that
25:40 the incentives are aligned correctly for
25:42 it and that
25:43 you know to the extent necessary you
25:45 might have to actually sit in the room
25:46 with them and sort of help them
25:48 understand how
25:50 how do i understand this campaign data
25:52 how do i tell which ones are performing
25:54 how do i tell what happens if i put more
25:56 money behind the same campaign does it
25:59 still
26:00 you know am i getting the same impact
26:02 from the next dollar as i did from the
26:04 last dollar like
26:06 there are real questions people don't
26:07 understand and the barriers are not
26:10 because are often not because the data
26:13 is not available um
26:15 it's often you know kind of hand-holding
26:18 and training and understanding
26:20 so
26:21 is there any other way so let's say we
26:23 have everything measurable then we have
26:25 an analyst who can
26:27 sit with the decision maker and explain
26:29 everything is it enough to actually make
26:32 sure that the data is
26:33 used
26:35 all right
26:36 i think it depends i mean you have to
26:38 really
26:39 it takes a lot of work to understand why
26:42 the data isn't being used if the data
26:45 doesn't exist then it's not a lot of
26:46 work to understand kind of like
26:48 what do we need what's the path to fix
26:50 this like you know i have to bring it in
26:52 the warehouse i have to make it i have
26:54 to clean it up make it useful
26:56 gotta you know create some reporting on
26:57 top of it and voila
27:00 once you've got that if it's not getting
27:02 used there are lots of different things
27:05 that might be going wrong and you really
27:08 have to spend some time
27:11 understanding um
27:14 what are those barriers and and what
27:16 does that look like and so
27:19 in a lot of ways it looks more like user
27:21 research
27:22 like you built a product you put it out
27:24 there and people aren't using it the way
27:26 that you thought it would thought they
27:27 would so
27:29 what's the what's the barrier do they
27:31 know that it exists
27:33 do they know how to use it does it solve
27:36 the problem they actually have
27:38 um
27:41 really kind of interrogating and
27:42 understanding
27:44 where the gaps are is the key to being
27:46 able to fix those gaps because obviously
27:48 if people just don't know about it
27:49 that's relatively easy to solve
27:52 if it fundamentally doesn't answer the
27:54 question they're asking then that
27:56 can be a little more challenging to
27:58 solve and it really requires you know
28:00 another round of work and understanding
28:01 what are you really trying to do with
28:02 this data
28:04 and if it's difficult to use
28:06 you either solve it by educating or just
28:09 simplifying
28:10 yeah i mean hopefully a little bit of
28:12 both if it's if it's
28:16 it's all about kind of you know creating
28:17 the right balance of what's possible in
28:19 your tools what's
28:21 how much you think people are really
28:23 gonna kind of learn and and work around
28:26 so if it's just not knowing how it works
28:28 then it's just teaching them if it's
28:29 that it's really hard then you might
28:31 have to really think about how much can
28:33 i simplify this do i need to take a
28:35 different approach is there kind of a
28:36 totally different way to get to the same
28:39 result that would be more user-friendly
28:42 yeah so uh i'm thinking about an example
28:45 we have a two licks um so we
28:48 do a lot of experiments and usual a b
28:50 tests right and uh let's say we have
28:53 some traffic of users coming in so for
28:55 some users we show one variant for some
28:57 of our other users we show a different
28:59 variant and then we compare different
29:01 metrics and
29:03 seeing if
29:04 let's say the the new feature gets
29:07 uplift in some metric right
29:09 yeah the usual eb tests
29:12 then i think the problem we had at some
29:14 point was that uh the people who
29:16 are looking at this
29:18 uh we were showing them too many
29:21 statistical stuff okay you know p values
29:25 you know power
29:26 test power like all this statistical
29:28 thing and then they were just
29:29 overwhelmed like what does this all mean
29:31 like all this confidence interval and
29:33 all this value so
29:35 and
29:35 yeah it requires a lot of iterations
29:37 actually to first teach them and then
29:41 think what can we
29:43 not show them like do we really need to
29:44 show all that to them or
29:47 like do they really need to know about
29:48 the p values right what and what this by
29:51 p value mean
29:52 or maybe we just show you know that okay
29:55 this is significant and that's uh that's
29:57 it
29:57 right so
29:59 i guess this is
30:01 quite in line to what you're saying
30:03 right
30:03 yeah yeah it's it's very similar and
30:06 even
30:07 so i've been in similar
30:09 a b testing
30:10 situations before and even within your
30:12 users i bet there's a variation of sort
30:14 of how much people are willing to learn
30:16 or interested to learn
30:18 and so it's all about figuring out you
30:20 know who are you optimizing for how do
30:22 you trade off between simplicity and
30:25 sort of you know
30:28 functionality and so we did the same
30:30 thing and so
30:31 for us that looked like
30:32 setting a default
30:35 p-value for significance but also
30:37 allowing people to change it if that was
30:39 something they were comfortable with
30:41 understood
30:42 all that and so the experiment was
30:44 communicated in terms of like
30:46 significant not significant
30:48 here's kind of
30:50 how you interpret it but we had enough
30:53 toggles for people who we had a couple
30:55 of power users to go in and say
30:57 actually in this situation i'm good with
31:00 this level versus this level and you
31:04 know this is how we want to to measure
31:07 this test in particular um but it's it's
31:10 really hard to get the right
31:12 balance and to make it feel really
31:14 useful to people when you have such a
31:16 deep
31:17 knowledge
31:18 of
31:19 the data and the space you can often
31:21 think oh well i can look at this and
31:24 the depth of understanding i get is
31:26 really rich but often business users
31:29 just really want
31:31 a simple
31:32 super easy to interpret results
31:35 yeah that's uh
31:36 if you let data scientists build a tool
31:39 then they will show like all this you
31:41 know this is my whitney test this is the
31:43 power of the test this is the p-value
31:45 this is the confidence interval all
31:47 these technical things
31:48 or like if you talk about machine
31:50 learning then you know this always
31:52 accuracy and precision recall every
31:55 seeker of ac and things like this and
31:58 if you throw this to business and then
32:00 okay what is that like how much money is
32:02 this thing actually
32:03 like why are you showing me all that
32:06 and
32:07 i guess this is what uh
32:09 ultimately matters for them
32:12 yeah yeah okay interesting so and then
32:15 like bridging this gap is more like uh
32:18 uh it's not a technological problem it's
32:21 more like uh a social problem like how
32:23 do you actually communicate this yeah
32:26 it's
32:27 i mean it's a lot like
32:28 building
32:30 any technical product right like
32:33 i think a lot about
32:34 zapier as a really good example of this
32:36 like
32:37 zapier exists
32:39 to take something that is technical and
32:41 make it non-technical
32:43 and to allow people to you know leverage
32:46 apis
32:48 to accomplish
32:49 automation without knowing what an api
32:51 is
32:52 and
32:54 it's really hard to get the right level
32:55 of abstraction when you start to talk
32:57 about something like that like
32:59 it's
33:00 it's really difficult to build the thing
33:02 that's gonna let someone who's
33:04 comfortable
33:05 you know parsing out text and doing like
33:08 a bunch of sort of interim steps versus
33:10 the person who just like
33:12 when i get an email can you just send me
33:14 a slack also like i just
33:17 want it to be exactly the same
33:19 and
33:20 it's
33:21 it's i mean it really is it's a lot like
33:22 product design and there's the challenge
33:25 is that your data team is really small
33:27 compared to your product team most
33:29 likely and so you have to learn to
33:32 you know
33:33 find the right points of leverage and
33:36 find enablers and you know train power
33:39 users and have them train their teams
33:41 and you know find all of these ways to
33:43 scale
33:44 um
33:45 the work that you have to do because you
33:47 don't have a whole team dedicated to you
33:49 know doing a lot of research and
33:51 spending a ton of time on design it's
33:53 sort of a scaled down version but
33:57 ultimately kind of the same problem
34:00 yeah and you wouldn't believe but i
34:01 actually read another article so i think
34:04 i read the screen total so i think i
34:05 told you the first one was about you
34:07 know complaining with marathon then the
34:09 second one was talking about uh
34:11 social versus technological problem
34:14 and then the third one was saying that
34:18 you should prioritize the last mail of
34:20 the analytical
34:21 journey and work backwards
34:24 this is like
34:26 probably
34:27 quite a long sentence there are many
34:29 things to unpack here
34:31 um so they were saying they prioritized
34:33 last mile and work backwards do you
34:36 know what they mean here so why do we
34:38 prioritize it and how do we work
34:40 backwards from from that
34:42 yeah i mean
34:43 i interpret that as
34:45 really
34:46 focusing first on what success looks
34:49 like for the thing that you're building
34:51 so
34:53 you know
34:54 for
34:55 different types of projects that's going
34:57 to be very different for
34:59 your a b testing
35:01 um results
35:03 dashboard or tool
35:05 what you want is for a product manager
35:07 to be able to make the right decision on
35:09 whether to roll out a feature or not
35:12 um
35:13 and there are sort of other goals around
35:15 that you want to make sure that they
35:17 wait long enough to get meaningful
35:18 results so you want to make sure that
35:20 you know they are running
35:23 an experiment in a responsible way
35:24 there's kind of some
35:26 subsidiary goals there but what you're
35:28 really focusing on is
35:30 i want a project manager product manager
35:33 to be able to come here
35:34 look at their experiment and understand
35:38 whether it worked
35:39 um or potentially what the business
35:41 impact was like maybe you actually need
35:44 to convert it into dollars
35:45 um
35:48 focusing there helps you really
35:50 understand
35:51 what you're going to need to build if
35:53 you need to communicate it in dollars
35:54 then you need to start thinking
35:56 from the beginning of sort of how you're
35:58 going to build toward that
36:00 so instead of immediately thinking
36:03 i need an a b testing
36:06 dashboard i need to start thinking about
36:08 what
36:09 our event data looks like
36:10 you're thinking about the user first and
36:13 that's going to change how you think
36:14 about the data sources the
36:15 transformation jobs what else you need
36:17 to join in
36:19 um
36:21 how you want to build the dashboard
36:22 you're really starting with kind of that
36:23 decision you're trying to drive
36:26 so this is uh i think at amazon they
36:29 have this principle like working
36:31 backwards
36:33 and they have the i'm not sure if it's
36:35 amazon or something else
36:36 but i hear this concept of um so let's
36:39 say you're working on some product it
36:41 can be a feature it doesn't have to be a
36:43 data product and then
36:45 the first thing you do is you write an
36:46 announcement like you write a blog post
36:49 that you would publish when the feature
36:51 is done and you write like this blog
36:53 post like a couple of pages
36:55 and you have that and then you work
36:57 backwards from that from this
36:58 announcement like as if this feature
37:00 already existed as if you already did it
37:02 and now you think okay what i need to do
37:05 to actually build this feature
37:06 and what are you saying as i understood
37:09 so
37:10 think of the end user so in case of
37:13 a data product let's say of a b testing
37:15 system it could be a product manager
37:17 who's
37:19 who will need to make a decision based
37:21 on what you show right so they will need
37:23 to understand
37:24 if the feature we're testing is making
37:27 impact and what is this impact like how
37:29 much better this thing is than the
37:31 current
37:32 thing right so we think about this
37:35 and
37:36 we i guess we can start involving the
37:38 user the product manager
37:40 immediately before even building this
37:42 thing right okay what kind of things you
37:44 need like what are the problems you have
37:46 how can we solve your problem right and
37:48 make sure free involve them then
37:51 you know it makes it easier for us to
37:53 to build this thing because um
37:56 yeah because we're already thinking
37:58 about the end user and uh so the last
38:01 mile here as you said is making sure
38:03 that the data is used right where the
38:05 product is used
38:06 and if we involve the end user if we
38:08 think about the end user from the very
38:09 beginning then it makes it easier right
38:13 do they execute this correctly yeah i
38:15 mean i think that's exactly right and
38:17 it's it's talking to the users it's
38:19 you know we're talking
38:21 a lot about sort of how data drives a
38:23 decision
38:24 it's literally sitting in the meetings
38:26 where those decisions are made right now
38:27 and understanding what that process
38:28 looks like and maybe
38:30 you know for this a b testing example
38:32 maybe the decision consistently looks
38:34 like i
38:35 have to
38:36 share the results of my test against my
38:39 ideal my my intended impact metric and
38:42 then i also have to share these other
38:43 two to make sure that we're not
38:44 adversely affecting something else too
38:47 much
38:48 and that's the kind of insight where if
38:50 you understand this is what the product
38:52 team really needs to make the right
38:53 decision you build from that as well and
38:55 and sitting in the meetings where these
38:58 decisions are being made talking to the
38:59 people who are making them um
39:02 i love pen and paper i spend a lot of
39:04 time sketching things out and saying
39:06 like okay if it looks like this
39:08 what does that tell you what's missing
39:10 how
39:10 like what is the deck you're building
39:12 look like what is the deliverable you're
39:15 creating look like does this dashboard
39:17 get you there does this tool get you
39:19 there like
39:21 really getting hands-on and um i love
39:24 the writing the press release first it's
39:26 very similar it's like what what do
39:28 people say about the thing that you
39:30 build
39:32 and this sketching is like prototyping
39:34 basically how would
39:36 this look like at the end and then you
39:38 show it to the
39:40 decision maker whoever the end user is
39:42 like
39:43 just using like a piece of paper and is
39:45 this does this look like what you have
39:48 in your head and then they say okay you
39:50 know what actually it looks completely
39:51 different
39:53 and you start talking right and then
39:54 they say okay like i want this thing
39:56 here right and then this thing is not
39:59 what you understood initially but it's a
40:01 different one and then you start talking
40:03 about this right
40:05 yeah yeah and that conversation's easier
40:07 to have if they don't think that you put
40:09 a lot of time and effort into it not
40:10 that you didn't put enough thought into
40:12 it but like i just sketched this out
40:14 really quick so like feel free to speak
40:17 up if it doesn't speak to you versus i
40:20 created this really robust prototype
40:21 enigma and it's really beautiful and
40:23 you're going to be like calling my baby
40:25 ugly if you tell me that this thing
40:28 doesn't work for you
40:30 it's a lot easier to get real feedback
40:32 if the bar is relatively low
40:35 so or i don't know maybe these days it's
40:38 not so easy uh but using a whiteboard
40:41 right just
40:42 getting around the whiteboard and then
40:44 starting okay let's go to this
40:46 whiteboard let's start you know drawing
40:48 there and
40:50 then you get a lot of feedback right
40:52 yeah yeah
40:53 okay
40:54 yeah we have a few questions so
40:57 um maybe we can start uh covering these
41:00 questions a question yeah okay aiden i
41:02 hope i pronounce your name correctly
41:05 so when data challenge the traditional
41:07 decision system how can we show the
41:09 benefits of the power data
41:12 do you have some experience on this
41:15 issue
41:17 yeah i mean i think
41:19 ideally again this comes back to like
41:22 making sure the right incentives exist
41:24 in the organization but ideally you want
41:28 people to be really incented by good
41:30 results and so then what you need to do
41:32 is show better results so
41:35 you know you're thinking about the
41:36 marketing team and
41:37 helping them make better choices well
41:39 the results of that are we spent the
41:41 same amount
41:42 this month as last month but we acquired
41:46 30 more users
41:49 usually the results are not that clear
41:51 um
41:52 obviously if you have a culture of a b
41:54 testing and the tools for a b testing
41:56 that's an amazing place to start to be
41:59 really confident in your results but
42:02 a lot of times this
42:03 is really more anecdotal and you might
42:05 not have the way
42:07 sort of super robust ways to report on
42:10 results but creating those moments of
42:13 um comparison are still really helpful
42:18 would you say it's a must that we have
42:20 everything measurable or we can already
42:22 start
42:24 convincing people to use our data
42:26 product when not everything is
42:28 measurable yet
42:29 i mean and nothing is
42:30 you're never at the point where
42:31 everything is measurable so the
42:33 as close as you can get is what you need
42:36 and so
42:37 um
42:38 you know
42:39 i for example i work in an environment
42:41 where we've got a warehouse
42:44 through which some activities are like
42:47 basically invisible in our data they're
42:49 fully manual
42:51 it's
42:52 you can't effectively measure how long
42:54 this process took or anything else but a
42:56 lot of times when we
42:58 make changes to process or make changes
43:00 to um to our tools like we literally
43:03 just spend a couple hours doing a time
43:05 study and someone sits with a stopwatch
43:07 and times people and says did this take
43:09 more or less time
43:10 is that precise no is that a sort of a
43:13 real
43:14 experiment no but it's better than
43:15 nothing and if you
43:17 are talking about
43:19 sufficiently large changes then it's
43:22 it's compelling you can tell i cut this
43:24 time in half um that must be real
43:27 versus you know it's it's
43:30 indistinguishable and so we should go
43:32 with the
43:33 process that's more scalable or the
43:34 process that's better for some other
43:36 reason
43:37 and start to make decisions from there
43:39 yeah that's interesting i was also
43:40 thinking about that you know if you work
43:43 in e-commerce uh at least uh if we're
43:46 talking about a website only then all
43:48 these clicks they're relatively easy to
43:50 measure right i mean capture track them
43:52 and then
43:53 put them somewhere in your database
43:55 house and then have all these dashboards
43:57 but if we talk about
43:59 some
44:01 i don't know like a
44:03 manufacturing line somewhere
44:06 or a warehouse where the actual people
44:08 move things like not robots but actual
44:11 people
44:12 then it becomes tricky
44:14 right and then it's uh
44:16 you cannot put
44:19 you know trackers to people and watch
44:21 how they move because a they will not
44:24 like it and then b
44:25 it's probably not
44:27 cheap right
44:29 yeah
44:30 i mean it can be it can be really hard
44:32 but it's
44:33 i really love a good proxy metric like
44:35 what's the closest we can get
44:37 to measuring this thing
44:40 and you know it's
44:42 i always think anything related to
44:44 employees is a great example of this
44:46 like we're never gonna have a large
44:47 enough sample of employees that we're
44:49 running a b tests on like employee
44:51 engagement but like we're gonna throw a
44:54 survey out there and see how people feel
44:55 and
44:56 you know
44:57 ultimately you kind of make your best
44:59 decision but there are in most
45:02 businesses a ton of parts of the
45:04 business that are
45:06 really easy to measure and i think
45:08 that's a really good place to start for
45:09 kind of these cultural changes where
45:12 it's
45:13 you want people to use data start with
45:15 the data you have and then you'll get to
45:16 a point where you're starting to
45:19 talk about how do we optimize these less
45:20 visible parts of the process and then
45:22 hopefully you've got sort of the trust
45:24 from everybody and you've got enough
45:26 sort of culture of
45:28 data oriented thinking that you can
45:29 start to
45:31 find ways to feel good about those areas
45:34 as well
45:35 so probably marketing is a good
45:38 good start
45:40 in many cases because you quickly have
45:42 some sort of web page and then you can
45:44 play with i don't know
45:46 different
45:47 wording there or different position of
45:49 things even that you can already start
45:51 measuring this and then showing that
45:54 okay you can measure this and then
45:56 people see that this is useful to have
45:58 things like this and then you start uh
46:02 you use this as convincing uh argument
46:05 yeah people start believing you and then
46:07 you
46:09 take care of more complex things right
46:11 yeah yeah
46:12 i i think
46:14 emily sherio
46:16 oh it does a really good job talking
46:18 about how to when you're trying to drive
46:20 change with data how to focus on as
46:23 narrow a slice as possible and i'll have
46:25 to dig up i actually am not sure where
46:28 she wrote this but
46:30 um i'm going to credit it to her
46:32 but as you
46:33 think about how
46:35 much you can scope down your work like
46:38 you want to really focus on i want to
46:40 enable this sales person to make this
46:42 better decision
46:44 based on the data and so
46:46 i'm going to focus entirely on that
46:48 until that end is accomplished and that
46:50 means all the infrastructure work
46:52 necessary all of the transformation work
46:54 like whatever it takes to get there but
46:57 when you narrow in then you've got a
46:59 really clear success story you've got an
47:02 advocate in that stakeholder you've got
47:06 everything that you need to
47:07 start to build the case for a bigger
47:11 role for data in general and you move on
47:13 to the next team and you say okay how
47:15 can i help marketing
47:17 address this decision
47:19 and how can i help product address this
47:21 decision
47:22 and uh i didn't hear the last name emily
47:25 uh what's your last name emily sharia
47:28 um i'll i'll
47:30 yeah
47:31 link
47:31 yeah okay i'll put this in the in this
47:34 description then we have a question from
47:37 court
47:38 so court is asking you have emphasized
47:40 asking high leverage questions do you
47:42 have any tips on finding these points
47:46 as both an analyst and executive
47:52 so
47:54 part of this is
47:56 having enough bandwidth
47:59 for analysts to kind of do
48:03 a little digging and and understand this
48:05 themselves if the business isn't looking
48:07 at data
48:08 um then you probably don't off the top
48:10 of your head know the highest points of
48:12 leverage but if you spend a little time
48:14 in the data i think you'll you'll start
48:16 to understand that and often
48:19 the best place to start is actually not
48:20 with the data in your data warehouse but
48:22 with your financials
48:24 and sitting with someone from your
48:27 accounting and finance team to
48:28 understand like what does our
48:30 performance look like what what's our
48:31 biggest cost center where are we
48:33 spending money
48:34 wherever you're spending money is a
48:36 really good place to bring the data to
48:37 understand
48:39 how you can do that more effectively
48:41 more efficiently either spend less or
48:43 get more for what you're spending and
48:46 um
48:48 that has consistently been kind of a
48:49 good approach for me to to identify
48:51 those points of leverage and once you
48:54 you don't have to solve the biggest
48:55 problem first
48:57 sometimes the biggest problem is really
48:58 hard but you have solve a big enough
49:00 problem that people care about it and so
49:02 you kind of find that sweet spot between
49:05 you know we spend x dollars
49:08 a month on marketing and so i want to
49:11 focus on improving the efficiency of our
49:14 marketing spend even though we spend 10x
49:17 on
49:19 our warehouse employees but we don't
49:20 know what they're doing so i'm not ready
49:22 to tackle that problem yet
49:24 i also imagine that you can get some
49:26 resistance so let's say you're starting
49:28 with financials you're identifying the
49:30 biggest cost center this is free house
49:32 and then
49:33 you go to the
49:35 way house manager and they are
49:38 like and you're saying hey how about
49:40 using data and they are like how about
49:44 like
49:45 i imagine that this can happen right
49:49 so what would you do in this case if
49:51 there is some resistance from people
49:53 they are not really
49:56 eager to use the data
49:58 so i think there's two separate answers
50:00 if i am independently trying to to push
50:03 this project and push this cultural
50:05 change i would not start with that
50:06 warehouse manager like i i want my first
50:10 project to be with someone who has
50:11 bought in with me and i'd rather work on
50:12 something smaller
50:14 or
50:16 something harder
50:17 and have someone who's in the boat
50:19 rowing in the same direction with me
50:22 then try to convince
50:24 the primary stakeholder that this is a
50:25 good idea um you want to find someone
50:28 who will be your advocate someone who
50:29 really wants this and in most
50:32 organizations you're going to be able to
50:33 find one person who
50:36 really wants more data and wants to make
50:37 decisions with that data
50:39 so
50:40 if it's totally up to me i just
50:43 say okay cool
50:45 thank you
50:46 i am excited to talk more about this in
50:48 the future and we come back when there's
50:51 you know much more of a snowball of a
50:53 more healthy data culture coming
50:56 and
50:57 they're more likely to buy in
51:00 if this has to be the first area then i
51:02 assume that is coming from someone else
51:05 you know if
51:06 a co hands this off to you and says work
51:09 with this person in the warehouse
51:12 and figure it out then
51:14 you
51:15 have to be a lot more delicate around
51:16 kind of how to get that person on board
51:19 and how to convince them of the value of
51:20 it and so i would say generally always
51:23 focus on upside not savings like don't
51:27 talk about how we could you know
51:30 ultimately need half as many people in
51:32 the warehouse
51:33 um
51:34 and that's why we should do this talk
51:36 about
51:37 you know talk to them enough to
51:38 understand what they're not doing that
51:40 they wish they could do
51:41 and then you can start to talk about it
51:42 say like well if we were more efficient
51:45 in this part of the process then you
51:47 would have enough people to do this
51:48 other thing that you want to do or
51:50 um you know find ways that
51:55 driving better
51:56 performance really benefits that person
51:59 instead of potentially feeling like it's
52:02 you know coming um something's being
52:04 taken away from them i think that's
52:06 usually sort of the most impactful part
52:08 of getting somebody in the boat but yeah
52:10 just really sell on the benefits and
52:12 find something
52:14 find something that bothers them that
52:16 you can help with and start to kind of
52:18 build that rapport and that trust and
52:21 honestly in data roles this is almost
52:23 always manual excel processes find what
52:25 they do in excel
52:27 and find a way to make that better even
52:29 if it has nothing to do with the project
52:30 you're working on
52:33 and
52:34 start to make them your advocates start
52:35 to make them appreciate you
52:42 oh you're muted alexi
52:44 oh yeah i am so and i was saying that uh
52:47 i guess if you are looking for a
52:49 long-term improved
52:50 this could be a marketing department i
52:52 think marketers have um
52:54 by now realize the importance of uh
52:57 using data for making decisions like
52:59 what which channel is more effective
53:01 right so
53:02 like where should they put more money
53:04 and they are probably
53:06 um they will be more welcoming uh
53:09 like
53:10 you your work
53:12 and using data but probably they are
53:14 using some data already but they will be
53:17 happy to use more data
53:18 yeah
53:20 and growth marketing especially
53:22 i think i took a course in growth
53:24 marketing and i was surprised by how
53:27 much stats was there
53:29 like uh it's about all about a b testing
53:32 and then
53:33 tracking data and uh like it's basically
53:36 like i don't know some
53:38 data analyst plus uh marketing
53:40 sort of position so yeah i was surprised
53:43 by that
53:44 yeah
53:47 okay
53:48 another question what kind of questions
53:50 do you ask domain experts to understand
53:52 their domain and can you recommend some
53:54 good literature on that
53:56 oh that's a really big question i think
53:59 it depends a lot on
54:02 the sort of domain and the the person
54:04 that you're talking to but i mean i
54:05 think coming to any conversation with
54:07 just a really genuine
54:10 curiosity can get you a really long way
54:13 and so
54:15 framing questions
54:16 from genuine curiosity can make all the
54:19 difference
54:20 when you say something as open-ended as
54:22 like so what do you do here like that
54:24 can be a really curious question or that
54:26 could be a really judgmental question
54:28 and
54:29 you have to make sure that you're
54:31 genuinely coming from a place of wanting
54:33 to understand and wanting to um
54:35 you know really get a grasp on what they
54:38 do and what's hard for them and what are
54:40 the challenges that the team overall is
54:42 facing and
54:43 um
54:45 just building rapport and so
54:47 you know that will depend a lot on how
54:49 much you know about the person and the
54:50 organization and how embedded you are
54:52 and sometimes it really just starts with
54:53 like
54:55 not talking about what they do at all
54:56 and getting to know them as a person and
54:58 you know building the relationship
55:00 before you start to build the work
55:02 relationship but
55:04 um
55:05 just
55:06 coming from a real
55:08 open-minded place of wanting to
55:11 understand what they do is the biggest
55:12 part and just you know
55:14 ideally after you understand what they
55:17 do you could document their job for them
55:19 and just ask all the questions you would
55:20 want to know to write like
55:22 the handbook to this person's job but
55:24 certainly don't frame it that way to
55:26 them because that sounds a little bit
55:27 a little bit scary and a little bit like
55:29 maybe you're trying to
55:31 onboard the next person um
55:34 okay yeah
55:36 so you you have to be quite good at uh
55:39 understanding people right
55:42 how you approach them and what
55:44 even in what tone you ask a question
55:46 because if you ask a question like what
55:48 do you do here it can sound
55:50 curious or it can sound
55:53 judgmental right so then you really have
55:55 to you know be careful
55:58 yeah and i mean i wouldn't i certainly
56:00 don't want to bring that in in a way
56:01 that intimidates anyone i've worked with
56:04 a lot of data teams and um
56:06 we definitely over index on introverts i
56:08 am a strong introvert and i wouldn't
56:10 necessarily say that early in my career
56:12 i felt super confident about my social
56:14 skills and my um i was not a person that
56:17 someone would say oh yeah gr great um eq
56:21 like very high emotional awareness
56:23 um
56:24 that's something that i've had to build
56:26 over time but i think if you just
56:28 genuinely really focus on like the
56:30 curiosity side of it it'll work out
56:33 maybe
56:34 i wouldn't worry too much about the
56:36 missteps but um
56:38 but frame even for yourself your
56:40 questions as wanting to understand and
56:42 not immediately as wanting to make
56:44 better and i think that makes all the
56:47 difference in kind of the way that you
56:48 approach the situation okay so
56:50 recommended literature would be some
56:53 some books about the emotional
56:55 intelligence
56:56 yeah maybe or you know i think it also
56:58 depends on kind of the area if you're
57:01 working on something super specific and
57:03 you know if if you're working with
57:04 digital marketers then like read a book
57:07 on digital marketing um
57:09 if
57:10 what you're trying to understand more
57:12 generally is like how do i
57:15 influence without authority then um
57:18 there are some really good books around
57:21 how
57:22 to
57:23 you know there's like dale carnegie's
57:24 how to win friends and influence people
57:26 it's the classic but there are others if
57:28 that's
57:29 not your particular cup of tea but
57:32 finding books that are more focused on
57:34 just how do you
57:36 build rapport with people how do you
57:38 you know really kind of
57:40 build those soft skills might make you
57:42 feel more comfortable as well
57:44 yeah i actually tried reading this at
57:46 some point
57:47 it was difficult for me like this
57:50 yeah
57:51 i've actually never finished it either i
57:53 don't i don't love it but it is it is
57:55 the classic and people
57:57 who have read it generally speak very
57:58 highly of it so
58:00 um
58:01 i don't know
58:02 it's like this
58:04 book getting things done some people
58:06 love it some people hate it
58:08 yeah yeah
58:10 it's like
58:11 it's always binaries never in the gym
58:14 okay maybe so we have a last question
58:16 and we still have a couple of minutes so
58:19 a question from eileen
58:20 is sometimes data projects can't give
58:23 expected results and this is normal
58:26 but this is creating trust problems in
58:28 data projects so what do you think about
58:29 this and how do you approach it would
58:31 you approach it in such a way that you
58:33 know it doesn't create task problems
58:36 yeah so i actually wrote two blog posts
58:39 with um alexis johnson gresham
58:42 and i can share links to those about
58:44 this exact problem because i think it's
58:46 it's really difficult and so
58:48 what
58:50 alexis at first shared this phrase with
58:51 me that was like really a light bulb
58:53 moment
58:54 around linear projects versus circular
58:57 projects and so even within data you
58:58 have both of them there are linear
59:00 projects where you can chart out the
59:02 next step
59:03 you have a high level of certainty that
59:05 if you do step one you can do step two
59:07 after you do step two you can do step
59:09 three
59:10 and then there are circular projects
59:12 where you don't know what you don't know
59:14 and a lot of data projects fall into
59:16 this category so something like
59:19 you know
59:20 building a
59:21 data pipeline to bring something in to
59:24 the warehouse like that's probably
59:25 pretty linear you know there is an api
59:29 might not have all the data i want in it
59:30 but you can look at the docs pretty
59:31 quickly and understand that
59:34 a circular project is one where until
59:35 you know what's in there you don't know
59:37 if you're gonna be able to do it and so
59:39 a lot of data science projects fall into
59:41 this until i test it i don't know how
59:43 good my results are gonna be and a lot
59:45 of analysis projects are like this
59:46 because it's like
59:48 i want to answer the question why was
59:50 conversion up last month i have no idea
59:53 if i actually have the data to answer
59:54 that and i won't know until i dig into
59:56 it really quickly really
59:58 substantially and so
1:00:00 um i'll share more about this but so
1:00:03 that very high level overview is first
1:00:05 just to set expectations like
1:00:06 acknowledge ahead of time that it is a
1:00:08 circular project you don't know if it's
1:00:09 going to be successful
1:00:11 you lose a lot of trust by saying you
1:00:13 can definitely do something and then not
1:00:15 delivering
1:00:16 but people understand if you say
1:00:19 i'm not sure if this is possible i need
1:00:20 to dig into it
1:00:21 break it down into as small parts as as
1:00:24 small pieces as possible so that you can
1:00:26 quickly make progress and report back
1:00:28 and say i've gotten this far looks good
1:00:30 so far next stumbling block is this
1:00:33 or
1:00:35 i spent two days on it it's actually i
1:00:37 think it's gonna be really hard for this
1:00:38 this and this reason here's kind of
1:00:40 alternatives things we could do to make
1:00:42 it less difficult or more possible
1:00:45 um
1:00:46 and
1:00:47 really lean into
1:00:51 just that communication and alternatives
1:00:53 and here's what we can do rather than
1:00:55 here's what we can't do
1:00:57 or here's what we learned and
1:00:59 hopefully you then start to also build a
1:01:01 culture of like failure is learning and
1:01:03 let's let's talk about it let's be
1:01:05 really excited that now we know this
1:01:06 thing is um
1:01:08 not possible with the data we have and
1:01:11 at the minimum like we don't ever have
1:01:13 to think about that problem until
1:01:14 something dramatically changes we don't
1:01:16 have to
1:01:17 put resources against this again it's
1:01:18 not it's not on the backlog anymore
1:01:20 we've checked
1:01:22 whether it's gonna work it's not um so
1:01:24 finding finding some ways to kind of
1:01:26 celebrate the learning
1:01:29 do you have a couple of more minutes
1:01:30 there is one more question that popped
1:01:32 up and it's
1:01:34 it's really interesting
1:01:35 so
1:01:37 court is asking so he's saying i'm
1:01:39 currently a data only student do you
1:01:41 have any recommendations or resources or
1:01:44 habits that helped you achieve success
1:01:46 in your career
1:01:49 um yeah getting getting started in data
1:01:51 is really interesting because um
1:01:54 it's it's just it's really hard
1:01:56 outside of a data role to even
1:02:01 remotely
1:02:03 approximate what it's going to be like
1:02:05 like public data sources are completely
1:02:06 different than the data you're going to
1:02:07 run into in a company and so
1:02:10 you know my
1:02:12 biggest advice is just to to be really
1:02:14 curious and to think a lot about
1:02:17 why things matter and so like the
1:02:19 logistics of being a data analyst you
1:02:21 can learn on the job no problem like
1:02:23 you'll learn how to write queries you'll
1:02:24 learn how to kind of approach problems
1:02:26 but building that
1:02:28 curiosity and that sense of
1:02:31 impact and
1:02:33 tying results back to the business is
1:02:35 often the hardest part and so sometimes
1:02:38 that looks like you know taking
1:02:40 more business classes or taking
1:02:43 the time to kind of
1:02:46 understand
1:02:48 the scale of impact that you can get
1:02:49 from a data source in whatever project
1:02:52 you're working on if you're
1:02:54 in an econometrics class like
1:02:56 do all of your analysis and then
1:02:57 understand like what would this mean if
1:03:01 this were true holistically if this
1:03:03 policy went out like what what would the
1:03:05 the real impact of that be and how do
1:03:07 you kind of think about that versus
1:03:08 other options and that skill is
1:03:11 hands down the most useful skill for a
1:03:14 data analyst to have
1:03:16 so more like business skills business
1:03:19 acumen and this like understanding how
1:03:21 business works versus just you know
1:03:23 being really good at sql being really
1:03:25 good at uh
1:03:27 other things that analysts do
1:03:29 technically yeah yeah i mean the
1:03:30 technical skills are useful but also are
1:03:34 not nearly as sort of
1:03:36 hard to teach and so most good analytics
1:03:40 leaders know if i find someone who
1:03:43 is really smart and understands the
1:03:45 importance of the data i'm going to be
1:03:47 able to teach them sql
1:03:48 whereas the reverse might not always be
1:03:50 true
1:03:51 okay
1:03:52 so where can people find you locally
1:03:54 optimistic
1:03:56 locally optimistic i should really join
1:03:58 um your slack as well i was thinking
1:04:00 about that earlier this week um so i can
1:04:02 i will join today and i'll be there if
1:04:04 there's a particular channel that you
1:04:06 tend to chat with people in but i'm also
1:04:07 always and locally optimistic um
1:04:10 happy to chat there in your blog post
1:04:12 did you draw your pictures yourself
1:04:15 no we we have an amazing illustrator um
1:04:20 who does all of our blog posts it's like
1:04:22 my favorite thing about the vlog
1:04:24 yeah the the illustrations are amazing
1:04:26 because you were saying that uh at some
1:04:28 point you would just uh take a piece of
1:04:30 paper and start sketching and then i
1:04:32 thought maybe
1:04:33 it is actually you who create all these
1:04:35 illustrations no sadly i am not quite
1:04:37 that artistically talented my my
1:04:39 sketches really encourage people to say
1:04:42 that's not quite right
1:04:45 see okay thanks a lot thanks for joining
1:04:47 us today thanks for sharing your
1:04:49 experience uh thanks for answering
1:04:51 questions and also thank you everyone
1:04:52 for joining us and for asking questions
1:04:54 and uh
1:04:55 yeah
1:04:56 thank you caitlin yeah thank you so much
1:04:59 this has been awesome
1:05:00 yes thanks have have a great rest of
1:05:02 your day and have a great weekend
1:05:04 you two awesome