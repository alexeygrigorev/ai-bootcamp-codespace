0:00 hi everyone welcome to our event this
0:02 event is brought to you by data talks
0:03 club which is a community of people who
0:05 love data
0:06 this is our weekly event and if you want
0:08 to find out more about the events we
0:10 have there is a link in the description
0:12 so just go there click on this link and
0:14 you'll see
0:15 all the events we have in our schedule
0:18 if this button is still read for you
0:21 click on this you will subscribe to our
0:23 youtube channel and you will see all the
0:25 content we publish so this is very
0:26 important this is
0:28 below the video so go there and click on
0:31 this button
0:32 and last but not least we have an
0:34 amazing slack community where you can
0:36 hang out with other dating enthusiasts
0:38 so the link is also there
0:41 click on this and register and see you
0:42 there
0:43 so during today's interview you can ask
0:45 any question you want there is a link in
0:47 the live chat it's pinned so click on
0:49 this link and ask your question
0:52 and i will be covering these questions
0:53 during it through
0:55 so
0:56 this is the introduction now i need to
0:59 open
1:02 um the questions i prepared
1:04 wow there are actually four questions
1:06 already to you
1:07 that's uh that's amazing but first we'll
1:09 start with questions i prepared
1:15 because i also spent a bit of time
1:17 preparing them it would be pt if i don't
1:19 ask them
1:20 okay are you ready yeah yeah
1:24 and this week we'll talk about uh
1:26 developer advocacy engineering for open
1:28 source projects and we have a special
1:30 guest today marvel marvel works as a
1:33 developer advocates engineer at
1:34 hugginface and it's actually not the
1:36 first time marvel appears as our guest
1:39 previously she gave a talk about
1:41 building a child board i think it was
1:42 one year ago the work is the the talk is
1:45 really good so check it out
1:46 so yeah and welcome back
1:50 hello uh i'm really happy to talk to you
1:52 every time you have like a really nice
1:54 energy i really love that it's like
1:56 usually like a chat rather than just
1:59 podcasting to be honest
2:02 thanks
2:03 okay but since we're on a podcast let's
2:05 start with your background
2:08 yeah can you tell us about your career
2:09 journey so far
2:11 so
2:12 um i studied industrial engineering and
2:15 in industrial engineering you have most
2:17 of the you know like operations research
2:20 type of
2:21 stuff it's like a mix of mathematics
2:24 statistics and coding to be honest to
2:26 optimize uh workflows and everything
2:30 and over there i have taken a data
2:32 science class i was previously doing
2:34 forecasting already but i have taken a
2:36 data science class and i was like
2:38 i'm going to do this as a job and then i
2:41 started uh going to boot camps
2:45 doing open source projects you know i
2:47 sometimes did kaggling i took online
2:49 courses
2:51 i kind of improved myself and
2:54 then i found my first job as a
2:57 you know like nlp engineer
2:59 i was doing chatbots and question
3:01 answering
3:02 models like in both of my um
3:06 previous jobs i was actually doing
3:09 information retrieval and chatbots
3:11 mainly
3:14 yeah like i was using hugging face back
3:16 then and i was already contributing to
3:18 hiking face
3:20 as an open source contributor so
3:22 it's i was already a fan of the company
3:25 and then
3:26 people like reached out to me saying hey
3:29 would you like to work with us and i was
3:31 like super happy
3:32 when that happened so yeah
3:35 and i did the masters
3:38 and
3:39 i also took part in you know the goggles
3:42 and aws community giving workshops on
3:46 you know predictive analytics nlp and
3:48 other things you know tensorflow
3:51 sagemaker
3:52 so yeah
3:54 so far this is what i did i would say
3:58 how did it uh how did you end up working
4:01 on llp stuff so you were doing boot
4:03 camps uh
4:05 i know kaggle but then eventually you
4:07 started working on chat bots yeah yeah
4:09 like
4:10 was it accidental
4:12 so in my uh in my boot camp
4:16 so basically i was going to um boot camp
4:19 sponsored by microsoft
4:21 and over there i was actually mentoring
4:23 because like it was a
4:25 health uh theoretical and health
4:27 practical and i was like doing good in
4:30 both
4:31 but i did a project about uh some text
4:34 classification and then i stick to it
4:36 with even more you know nlp projects
4:40 my first ever nlp project was actually
4:42 at school it was uh i was learning uh
4:45 the data science with r like it's quite
4:47 surprising to be honest
4:50 and can you do an lp with r
4:52 yeah
4:53 okay
4:55 it's quite unexpected but there is even
4:57 like a tensorflow for r you know if you
4:59 want to use that and there is keras
5:01 right keras for r yeah i did um so
5:04 basically we have scraped the
5:07 data sets from twitter on climate change
5:12 you know people's opinions and
5:13 everything and we did sentiment analysis
5:16 we started like a topic modeling at the
5:19 first place looked at the embeddings and
5:21 other stuff
5:23 that's how i got into it i would say
5:25 like it was really um
5:28 like i was like
5:29 this is super cool that you can
5:31 analyze a lot of people at one place
5:35 and that's how i started doing nlp
5:39 and
5:40 one thing after another like i
5:42 started doing it for living
5:45 and yeah
5:47 but i also did like because i was always
5:49 working on
5:51 working in startups i was doing
5:52 everything
5:54 um like i was taking taking the data or
5:56 you know like getting it from apis or
5:58 scraping it you know i started from that
6:01 to eda and then
6:04 building models and even deploying them
6:06 which is like very end-to-end because
6:08 that's what you do if you're a machine
6:10 learning engineer working in a startup
6:13 because you do most of the things i was
6:15 also doing predictive analytics like
6:17 churn or sales prediction so yeah like i
6:21 was basically doing everything but
6:24 i did learn a lot of stuff so i am not
6:27 regretting that
6:30 and you said that while working on nlp
6:33 with chatbots you contributed to open
6:35 source
6:37 i guess also to
6:39 other libraries how did it happen to you
6:42 like what was your first contribution do
6:43 you remember
6:44 yeah yeah so basically um how i met
6:47 hugging face was different uh i saw you
6:50 know like we have a
6:52 chief scientist uh thomas wolfe
6:55 and he has a video called the future of
6:58 nlp which like for that for two hours or
7:01 something he goes from the start of the
7:03 nlp and through so many papers he just
7:07 analyzes uh the you know state of the
7:10 nlp and like
7:13 explains the papers themselves and i was
7:15 like
7:17 i mean this is so much work like what
7:19 are they doing and then i learned
7:20 hugging place
7:22 and at my job i was i started using
7:25 hugging face as well especially you know
7:27 the birth model and everything
7:29 and
7:30 then i saw one day that thomas wolfe
7:32 tweeted that they are going to have a
7:35 contribution sprint about data sets
7:37 library and
7:38 in data sets library we will we actually
7:41 have something called canonical data
7:43 sets which is like i don't know if
7:45 you've heard about it but it's like glue
7:47 or image nets you know you have to
7:50 make them easy to use and to do this you
7:53 need to write scripts on these data sets
7:56 so that it's easily loadable and fed to
7:59 models
8:00 in a very native manner rather than you
8:02 know just taking a
8:04 i don't know csv data set and just
8:07 dealing with it in hacking
8:10 a lot track them yeah
8:13 yeah yeah but these data sets are very
8:15 complicated like for instance there's
8:16 something called the
8:18 you know like we have attention masks in
8:20 the data set we have
8:22 i don't know like in
8:24 in um
8:26 for instance segmentation data sets are
8:28 like very complicated so they need to be
8:32 made
8:33 easy to use like for instance named
8:35 entity recognition or question answering
8:36 data sets they have like
8:39 spam indexes and other stuff so
8:42 we were writing scripts to do that and i
8:44 contributed a couple of them
8:47 and that's how i met my colleagues as
8:49 well you know like the other day i i was
8:51 talking to the to quantum who is like
8:55 the lead
8:56 of the datasets library and i was like i
8:59 didn't even know like i would be working
9:01 with you because
9:02 you know i was bugging him back then a
9:04 lot because i had zero idea about cicd
9:08 or
9:10 i mean
9:11 they were used using circle ci for
9:13 instance i have never used ci cd because
9:16 i was already working in a very small
9:18 company
9:19 we did not have
9:21 any um
9:24 development processes
9:26 that
9:27 you know like help you
9:29 maintain the
9:30 big code basis it wasn't that big
9:33 so i learned you know formatting
9:36 everything from hugging face so it was
9:39 really nice actually
9:40 and then
9:41 i attended their speech sprint where you
9:44 fine-tune
9:46 speech models with the
9:48 language specific data sets it was also
9:51 fun
9:53 and yeah they were asking me to join and
9:55 then
9:56 yeah i joined
9:58 it was also
9:59 that
10:00 in google i o last year i was talking
10:02 about transfer learning and i included a
10:05 hugging face in my slides and you know i
10:08 looked back and
10:10 i said you know i was sort of like
10:12 destined to work there or something
10:15 so yeah
10:17 was it your first open source
10:18 contribution i guess no you probably
10:20 contributed to other libraries before
10:22 did you you know like issues and other
10:25 stuff not much not like
10:28 not contributions okay because the
10:31 libraries like you know hugging face or
10:33 scikit-learn have sprints
10:35 in which the maintainers are spending
10:38 time to for to help you out in your
10:41 contribution
10:43 because like we observed that first once
10:45 you onboard the contributor for the
10:47 first time it's easier for them to
10:49 contribute later on
10:51 so um it's actually a good thing to you
10:54 know like have more open source
10:56 contributors and help people out so that
10:58 there is no scare
11:00 from contributing
11:03 yeah imagine that it can be quite
11:06 scary quite daunting when you see like
11:09 all these issues of all this code base
11:10 and you mentioned things like say cd
11:12 test code formatting then yeah maybe
11:15 this is too much for me like i don't
11:17 know how to start right yeah yeah
11:19 exactly and now i'm doing pr reviews and
11:21 stuff and it's like actually you know
11:23 like a weird uh like a journey you know
11:26 you eventually become that person that
11:29 is
11:30 giving the review
11:33 do you remember when you actually were
11:35 making these contributions you already
11:37 worked at a startup right
11:39 um like were you doing this as a part of
11:42 your job or this is something like more
11:44 like a side activity no it was more like
11:46 a side activity i don't think the
11:48 companies would actually
11:50 do that unless they are a very big fan
11:52 of uh hugging face or something
11:55 but let's say you work on something and
11:57 you use uh hugging face as a library
11:59 right and then there is a feature
12:01 a missing feature right then it makes
12:03 sense to kind of contribute to the
12:05 library yeah exactly like for instance
12:07 we have our tensorflow developers and
12:09 i see them that sometimes they develop
12:12 to
12:12 contribute to keras or tensorflow in
12:15 order to
12:16 you know ease the process and just
12:18 optimize the some of the
12:21 functions and workflows that aren't
12:23 really optimized over there
12:27 okay so you started you contributed to
12:29 this data sets
12:31 functionality of hacking phase before
12:34 joining and then they saw
12:36 they saw you and they offered you a job
12:38 right and
12:40 what do you work on now is it something
12:42 also you keep working on these data sets
12:45 part
12:45 i have a couple of uh couple of projects
12:48 um so basically uh the reason why
12:51 developer like developer advocacy
12:53 engineering is called engineering is
12:55 because like
12:57 it depends on the company how the um
13:00 the
13:01 job scopes
13:03 and their technicality change uh and in
13:06 hugging phase it's a very technical job
13:08 to actually become a
13:10 developer develop developer advocates
13:13 so
13:14 uh we do not have like a community
13:16 builder type of people but more like
13:19 you know a horizontal engineer that is
13:22 like supporting teams and everything and
13:25 um you know helping people out in
13:27 general so
13:28 and
13:30 i wanted to become that person sort of
13:33 earlier i was being
13:35 interviewed for machine learning
13:37 engineering but i was just on board for
13:39 my previous job
13:41 so i couldn't do that and then i was
13:43 like
13:45 approached for this so i was approached
13:48 twice
13:49 and i was like super happy because like
13:52 i wanted to actually like
13:55 have such position sort of
13:57 you know technical but also
14:00 developing things for people to
14:03 have an easier journey in machine
14:05 learning
14:06 so basically i have a couple of projects
14:08 um one is um there is something called
14:12 hugging face tasks i'm going to
14:15 write in the youtube
14:18 so
14:20 as a as a previous machine learning
14:22 engineer i have observed that
14:25 i've seen so many software engineers who
14:28 want to build machine learning products
14:30 but didn't know where to start
14:33 and these tasks are actually like giving
14:36 the baseline information for a given
14:38 tasks like
14:39 image segmentation or question answering
14:42 it's like sort of like i have gained so
14:44 many know hows in my previous jobs that
14:47 i wanted to channel them
14:49 so that people would have easier
14:52 and you know like a lower entrance level
14:56 um in starting doing machine learning
14:58 products because basically
15:00 in hugging phase we have hugging face
15:03 hub where there are so many models that
15:05 you can actually use
15:07 directly without training a model
15:09 yourself
15:10 so it's a bit
15:12 um developed in that manner um
15:16 this was my first project
15:19 and um
15:20 i have maintained uh also the
15:24 transformers uh but the tensorflow side
15:27 because there is so many people using
15:30 pytorch in here they do not have much
15:33 you know tensorflow people so that's
15:35 what the hacking face uses exclusively
15:37 by torch so they don't like tensorflow
15:39 at all this is not true right so this is
15:43 not true i would say but yeah there is
15:46 like number of people who like fly torch
15:48 and fastai i are more than you know
15:51 people who use tensorflow when and
15:53 scikit-learn i would say
15:55 so they didn't have like they only had
15:57 one tensorflow maintainer
15:59 maths so like before we had more
16:02 tensorflow maintainers i was helping
16:04 matt out develop stuff and you know like
16:07 debug things
16:08 i
16:10 we now have more tensorflow people
16:12 and currently so i also integrated keras
16:17 into hugging phase in which
16:19 um
16:20 when you host the keras model on the
16:23 hugging face hub you can just
16:26 push your model with one line of code
16:29 it generates a model card for you about
16:32 it has like insights regarding to your
16:34 model your models architecture hyper
16:36 parameters anything for reproducibility
16:39 basically so it's
16:41 hacking facehub is sort of all about
16:43 versioning your models and data sets
16:46 yeah yeah like a model registry and like
16:48 mostly most of my job is actually
16:50 working on hub
16:52 so i developed stuff for keras
16:56 that would improve the reproducibility
16:58 of the experiments version the models
17:01 you can host your tester board inside
17:03 the model repository you can have model
17:06 architecture metrics anything history in
17:10 the
17:11 repository so that's yeah yeah it's good
17:13 for collaboration with the teams because
17:16 if you have your model on your local
17:18 it's not much of a
17:20 it's hard to collaborate with people so
17:23 it's a bit like
17:25 you know like a
17:26 github or gitlab but for machine
17:29 learning i would say yeah that's why
17:32 it's called hub as well right yeah
17:35 like github hacking face hub yeah yeah
17:38 and we also have something called spaces
17:41 which is like uh you can just build your
17:43 demos uh with streamlined gradual or
17:46 just static
17:48 and just share with people and recently
17:51 we opened a feature called the community
17:53 tab
17:54 which has
17:56 pull requests and discussions like you
17:58 do on github but for model repositories
18:01 or dataset or space repositories
18:04 and
18:06 yeah
18:07 yeah i just i just wonder so you
18:09 probably covered the engineering parts
18:12 right so all you described all these
18:14 features
18:15 they are quite heavy on engineering so
18:16 you actually need to
18:18 write code there i don't know
18:20 maybe yeah that's
18:22 you know make sure like the acid is
18:24 working and all these things but what
18:26 about the first part the developer
18:28 advocacy part do you also do something
18:30 like that yeah we also do that so
18:32 basically
18:33 uh like the last thing i'm working on
18:35 currently is
18:37 putting the tabular data modality on the
18:40 hub which is like you know improving
18:43 reproducibility and collaboration for
18:45 the tabular data related workflows
18:48 having a better integration of psychic
18:50 learning stuff
18:51 but it's also so basically everyone in
18:54 hugging phase is sort of like a
18:56 developer advocates you will see hugging
18:58 face course for instance
19:01 every engineer is in the course build
19:04 you know like uh producing content over
19:06 there and
19:08 i don't know um
19:10 shooting videos or like doing um
19:14 doing community sprints like community
19:17 events so everyone is a bit of a
19:19 developer advocate
19:21 uh in that sense and like my par part of
19:24 my job is to um you know like i help
19:28 people out in the forum you know
19:29 reproduce their errors and fix them
19:32 if you know like if there's an issue to
19:34 be opened i better test everything to
19:37 make sure you know it's good for
19:39 developers and i usually try to
19:41 understand the user journey in
19:43 everything and
19:44 i stress test everything
19:47 or develop something that would ease the
19:49 developer's pain so it's usually about
19:51 developer experience i would say
19:54 and also i um
19:56 i do for instance prints it's called
19:59 keras print where we serialize the
20:02 examples on the keras
20:04 official websites and we built demos
20:07 over them and we we later contribute
20:10 them to keras
20:12 because like those examples are very
20:13 minimal or for a good reason because
20:17 i have talked to francois chole and like
20:20 he doesn't want it to be like
20:22 overwhelming
20:23 there are like rules to contribute
20:25 examples and stuff
20:27 so
20:28 we put
20:30 models and the demos over there later on
20:34 uh to improve reproducibility over that
20:36 because like it's it's not good to
20:39 you know like you go to keras website
20:42 and
20:43 you
20:44 you have to run a whole collab in or in
20:46 order to see what the model actually
20:48 does
20:49 so we actually do this for people and
20:51 host those examples we did the same for
20:53 pytorch
20:55 as well so we have community events like
20:57 this
20:58 where we onboard people
21:00 to
21:01 you know like contribute to open source
21:03 as well
21:04 and i also do
21:06 i also do workshops on transformers
21:10 or uh you know building spaces it's more
21:12 like a beginner level workshop that's
21:15 also i would say an advocacy part of my
21:18 job
21:19 yeah
21:21 is it like would you say it's divided 50
21:23 50 like 50 on the advocacy part and 50
21:26 on the engineering part
21:28 or it's something else so it depends so
21:29 basically currently we do not have uh
21:32 much people working on the tabular data
21:34 modality
21:36 we only have adrien who is uh who is one
21:39 of the core contributors of scikit-learn
21:41 and we
21:42 hired one more person who is uh
21:45 i don't know
21:46 like they have a famous package again on
21:49 scikit-learn
21:51 um
21:53 because like it is lacking i am
21:55 currently coding stuff i would say
21:59 but
22:00 it also depends on you know developer
22:02 conferences and everything like you know
22:05 i think around this year of the
22:08 you know if this season of the year
22:10 there's more developer conferences so i
22:12 go and yeah
22:13 yeah yeah and like next next month i'm
22:16 going to epf
22:18 for instance uh to
22:20 present uh today i'm going to pi data
22:22 paris
22:24 i have a couple scheduled um so yeah
22:27 like it's it depends uh but
22:30 i would say i'm mostly
22:33 like i am like 60 70
22:36 coding
22:37 stuff and you know like 30 percent 40
22:40 percent presenting things
22:43 or uh
22:44 i don't know like doing community
22:46 sprints in order to get more
22:48 contributors
22:49 i'm spending time on the forum to you
22:52 know like all like github issues to help
22:55 people out as well which is a part of
22:58 advocacy i would say
23:02 yeah and i guess the main difference
23:03 between a deaf advocate and the default
23:05 gate engineer is the engineering part
23:08 right so maybe
23:09 in the traditional sense of this role
23:12 they maybe spent less time on
23:15 the actual features of
23:18 two of the product and they spent more
23:20 time educating or
23:22 helping community but here yeah
23:25 you're doing both right yeah yeah i'm
23:27 i'm so basically uh
23:29 in some of the companies like it depends
23:31 heavily on the company
23:33 in some of the companies these developer
23:34 advocates like some of them are focused
23:37 mainly on doing community events or like
23:42 i don't know
23:44 doing podcasts or
23:48 you know educational material i would
23:50 say
23:51 uh but
23:52 in some of the companies they
23:54 this like in hugging face or i think in
23:58 google as well
23:59 we develop in um stuff inside and we
24:04 test things we do you know like i
24:08 and i i develop
24:10 lately more but like it's it just
24:12 depends but i would say
24:15 the reason why we call it engineering
24:17 was that previously it was actually
24:18 called developer advocates
24:20 but we received
24:22 applications from
24:24 people with
24:25 lesser technical backgrounds so not to
24:28 steal their time we have turned its
24:30 turned the title into
24:32 engineering because uh like we want to
24:36 have like former ml engineers or
24:39 yeah mostly former ml engineers that has
24:42 been doing open source and like the most
24:44 important thing um that we are looking
24:47 for is already existing open source
24:49 experience
24:50 because um like that's the fundamental
24:54 thing we do we
24:56 for instance i do hiring sometimes for
24:58 the team and the first thing i do is
25:00 looking at the github profile of a
25:02 person
25:04 what's the best way to get this open
25:06 source existing open source experience
25:09 and you can join uh the sprints of
25:14 psychic learn or how this is how you you
25:17 got this experience right yeah yeah
25:19 exactly
25:20 you can pick like you you can pick a
25:22 library and just go and
25:25 pick one of the good first issues and
25:27 assign it to yourself
25:29 and open up er and you know it's the
25:32 it's your first
25:33 experience and what else let me think
25:37 yeah
25:38 i would say good first issues are a good
25:41 one sprints are good
25:43 and
25:46 yeah like if you want to do code
25:48 contributions because we first look at
25:49 the code contributions to make sure that
25:51 person is actually technical
25:53 but
25:54 a couple of other things that you can do
25:56 to
25:57 you know actually contribute to open
25:59 source
26:00 is not code but you know like
26:02 documentation
26:04 you know helping
26:06 helping people out in stack overflow of
26:08 or forums
26:13 writing blog posts or something like
26:15 that you know or submitting bugs or
26:17 issues it's also a very valuable thing
26:21 and what else you know like even
26:24 developing your own library that solves
26:27 a problem is a thing you know i get
26:29 these uh
26:31 ideas of libraries all the time but i do
26:33 not really have any time like i really
26:36 like building tools in general like i
26:38 was previously a person
26:41 building models but like after i started
26:43 you know developing more open source
26:46 it's like uh
26:47 it's like a thing i just want to build
26:49 tools i became sort of more like a
26:51 software engineer i would say rather
26:54 than a data scientist now
26:56 but yeah
26:58 it's it's fun
27:00 yeah there are also things like google
27:03 summer of code yeah yeah
27:06 this is similar to sprints right but
27:08 yeah
27:09 it's it i guess it takes longer and
27:11 usually in google summer of code i tried
27:13 to take part i wasn't accepted but they
27:16 in general know a bit about the project
27:18 weren't accepted like when did you even
27:21 apply
27:23 it was long ago and i think they just
27:25 didn't have a lot of
27:27 places for google summer of code
27:31 like their project they choose is the
27:32 apache link
27:34 and i think they only they it was before
27:37 they became an apache project and i
27:39 think they had just one or two okay
27:41 apparently they have a lot
27:43 yeah right now they have a lot of course
27:45 but it was before they became an apache
27:47 project
27:48 and
27:49 yeah i just got unlucky but i remember
27:52 the process is like you need to write
27:53 some sort of proposal like what exactly
27:55 you want to work on and then if this
27:57 proposal is selected
27:59 um then you get a mentor and you
28:02 actually work with this mentor and
28:04 you end up contributing like a
28:06 relatively large feature
28:08 i was moving meanwhile like meanwhile
28:11 the applications were open so i couldn't
28:13 really apply that time and i regret that
28:16 maybe next year
28:19 yeah and right now it's open to everyone
28:20 not just students which is even cooler
28:22 right so back then i was a student
28:24 so i thought okay this is my last
28:26 opportunity i was graduating that year
28:28 it was my last opportunity to
28:30 contribute but now yeah you don't have
28:32 to be a student to do that that's
28:34 that's pretty cool and you get some
28:36 money for that as well it's not like uh
28:38 insane amount of money maybe you can get
28:40 i don't know a beer on that money but
28:43 not much but still especially if you're
28:45 a student that's uh
28:46 i i do it for the glory you know like i
28:49 do
28:50 at this point so yeah
28:52 it's a good bonus right yeah yeah
28:54 exactly
28:56 and also uh there is a thing called
28:58 oktoberfest i think maybe the first one
29:01 was last year you heard about this yeah
29:03 yeah but i didn't really contribute uh
29:06 on oktoberfest
29:08 yeah but it was uh
29:10 more uh how to say
29:13 global than uh google summer of code it
29:16 just i think
29:17 a lot of like a big amount of uh
29:19 libraries stills took part there so
29:22 maybe this october which watch out if
29:24 you want to make open source
29:25 contribution i'm planning to contribute
29:27 more to scikit-learn because like i met
29:29 their core developers uh there in here
29:32 like living in paris
29:34 and they are doing sprints but aside
29:36 from sprints i'm just planning to you
29:38 know go pick some good first issues
29:40 because like i looked at the code base
29:41 and it was really nice and uh to
29:44 contribute
29:45 and you can also learn a lot from the pr
29:48 reviews that you get by means of the
29:50 clean code
29:52 you know sustainability of the code and
29:54 everything so it's a
29:56 it's a it's a big journey you know like
29:58 i really like uh working in open source
30:01 and by the way coming back to you
30:04 you said you take part in hiring in the
30:06 hiring process and when hiring you look
30:09 at the contributions of this person at
30:11 the code contributions yeah yeah do you
30:13 look at contributions to some projects
30:16 or contributions to
30:20 yeah
30:22 so
30:23 it can be a person
30:25 a person's own project it doesn't
30:27 necessarily have to be
30:29 another code base but like
30:32 if it solves a problem or something it's
30:35 um yeah like it's a good thing but like
30:37 the thing is
30:38 in here we
30:40 like we have standardized um
30:43 how can i say development processes
30:46 like you develop something or like you
30:48 contribute to something you fix a bug
30:50 and then you
30:52 you go through this whole pr review
30:55 and you know merger and everything
30:57 so we expect sort of like a familiarity
31:00 with uh
31:01 developing something for a bigger code
31:03 base i would say
31:05 yeah it's not easy like and sometimes
31:07 the author
31:09 authors have their own vision
31:11 yeah yeah exactly and like that are done
31:13 in the in some
31:15 perspective yeah
31:16 i can say i can definitely say that you
31:18 know like in open source you do not
31:20 there's no ground truth there is like
31:22 uh
31:24 like you will come across a lot of
31:25 opinionated people about the code bases
31:28 or
31:29 i don't know like
31:30 even so many needs picking of your pr to
31:34 a point of you know a lot of
31:36 comments
31:38 um but you learn a lot and at some point
31:41 you get used to it and you understand
31:43 the way they develop things especially
31:46 if they start as a like a really good
31:48 small group of people like you will
31:51 definitely um you might struggle at the
31:53 first
31:54 but like it's i don't think there is
31:56 like a standard way of developing things
31:59 that everyone would agree on so it's
32:01 quite normal
32:04 yeah i remember contributing to xd boost
32:06 to the java library
32:09 yeah well
32:11 wait till the end so they actually
32:13 didn't accept my uh pr yeah okay
32:16 so yeah maybe not so cool at the end
32:19 and this is quite frustrating right so
32:20 because they have their own way
32:23 or expectations of the code right and if
32:25 this code doesn't follow i'm not talking
32:28 about extreme boost maintainers in
32:29 particular but in general about open
32:32 source libraries so if the
32:34 way maintainers imagine the features
32:36 written maybe they might just not accept
32:39 the request right and this is
32:40 frustrating so uh
32:42 like it was actually my second
32:44 contribution to exuberance my first was
32:46 accepted and i was very enthusiastic
32:48 like yeah
32:49 now i do another one and then my other
32:51 one wasn't accepted and i okay why i'm
32:54 doing this no i don't want to contribute
32:56 to you anymore so how how to deal with
32:58 this kind of
33:00 [Music]
33:01 you know rejections because they suck
33:03 right yeah yeah exactly containers have
33:05 the best
33:07 motivation because this is where their
33:09 projects and in the end they will have
33:11 to maintain it not me because i will
33:13 commit something and then disappear
33:14 right and then they will have to deal
33:15 with this code right but for me as a
33:18 contributor that was a bit demotivating
33:21 maybe you have some suggestions yeah of
33:23 course so basically
33:24 two days ago i had to reject someone's
33:26 pr because so basically
33:30 we
33:31 save tensorflow models in a
33:34 format called
33:35 saved model which basically has
33:38 everything like it has the graph the
33:39 variables so you it's uh it's sort of
33:42 like
33:43 the agreed way of serializing models in
33:46 scikit-learn
33:47 sorry tensorflow venkeros
33:49 and with this you can use the production
33:52 tools on tensorflow extended ecosystem
33:55 as well
33:56 so and some of the models
33:59 that are very like in in how can i say
34:03 in the early days of keras there are
34:06 some serialization techniques in which
34:08 for instance
34:09 yeah hdf5 yeah exactly some of the
34:12 models cannot be saved in this format
34:15 because like of old ways of keras like
34:18 for instance there is like models that
34:21 are like cnn encoder and are an end
34:23 decoder and then you have like a
34:25 sequential model and you serialize them
34:28 together or you have like a gun model
34:32 like you do sequential and then inside
34:34 the list you put your generator and
34:36 discriminator for instance you cannot
34:39 save this with the saved model
34:41 so someone opened the pr to enable hdf5
34:44 saving and like i had to reject that
34:47 because this is the agreed
34:49 like this is like a design decision
34:50 being made to make these models easier
34:54 for production and
34:56 it is like a witchcraft to actually save
34:59 those models like that it's not really
35:01 encouraged
35:03 so um
35:05 the one thing i can say firstly open a
35:08 discussion in the
35:11 repository or like organization to see
35:15 the design decisions made and like why
35:18 the developers couldn't fall uh fix that
35:20 so far
35:22 or you know like any experience or
35:25 insight they have so you know actually
35:27 there is a problem
35:29 and
35:30 like communicating with the core
35:32 developers is actually helping
35:35 and i honestly uh
35:37 do not have
35:39 much
35:41 advice to that also writing good unit
35:44 tests
35:45 to confirm that it works is a very big
35:48 part of the work to be honest like i
35:50 test every single thing that i write
35:53 to make sure it's
35:55 it's working and it is compatible with
35:58 the rest of the ecosystem because like
36:01 those tests make sure that
36:04 your code is like those tests are there
36:06 to make sure that any new contribution
36:08 will not break other functionalities as
36:11 well so i would say the unit tests are
36:14 very good way of convincing the other
36:17 person to
36:18 you know have your code there
36:21 and i think these are like
36:23 two big advices i don't know if i have
36:25 like any other one
36:27 i think the the point you made about the
36:29 discussion is a pretty good one and
36:31 maybe you should this is something that
36:33 you should do even before contributing
36:36 code like okay this is my idea i want to
36:38 implement it yeah exactly what do you
36:40 think about this and then if you get a
36:42 green light
36:43 then you spend time implementing and
36:45 then writing tests because i guess it
36:47 can be pretty demotivating
36:50 if you are rejected after the fact after
36:52 you wrote yeah yeah yeah exactly much
36:54 less well not say the motivating if this
36:58 idea is rejected before your article
37:01 yeah yeah exactly exactly like if that
37:03 person actually
37:04 discussed with us beforehand on you know
37:06 why we do not save models this way they
37:09 wouldn't have to spend any time
37:12 so yeah
37:12 [Music]
37:15 we really like people contributing so we
37:17 do not we try to reject people in the
37:20 least discouraging way so
37:22 that's a good sign that you know
37:24 developers actually care about the time
37:26 you spend
37:29 and yeah i think most of many tools they
37:32 have their own slack or discord
37:33 communities or
37:35 also these discussions in github like a
37:38 relatively new feature or it can be even
37:40 an issue in github repo
37:43 in issues and discussions yeah
37:45 yeah
37:46 well yeah i think
37:48 so talking to open source authors they
37:50 usually recommend
37:52 um like first going to their discord and
37:55 then chat there a little bit
37:57 before starting to implement the future
38:00 yeah yeah exactly
38:02 and then
38:03 at the beginning remember i told you
38:04 that there are a lot of questions for
38:06 you i think now it's time to come back
38:08 to these questions uh sorry for keeping
38:11 you waiting everyone uh so yeah the
38:14 first question is outside of hacking
38:16 phase what's the best resource to learn
38:18 about nlp not just theory but also
38:20 applications
38:22 question so
38:25 so most of the nlp is about solving
38:28 tasks
38:30 which is shaped according to your data
38:32 and this can be question answering like
38:35 what you want to do you need to
38:36 determine that first and then
38:39 pick the task that is uh suitable for
38:41 your use case
38:42 and like it can be question answering
38:45 name density recognition or
38:48 part of speech tagging or anything
38:51 so
38:51 um
38:53 most of these are nowadays solved with
38:55 fine-tuning models actually uh like
38:59 through transfer learning which is
39:00 transformers what is transformers are
39:03 used for
39:04 so we for instance we have a
39:07 course i'm going to write it down in the
39:09 chat we have a please write not in chat
39:12 because i think youtube blocks all the
39:15 things so okay so
39:18 and then i will send it to you like it
39:20 shouldn't block my links
39:22 yeah okay i just i just uh sent the link
39:25 to you and then i will now post to you
39:28 live chat it's it's a good good one to
39:31 get started with nlp i would say
39:34 um
39:36 yeah
39:37 you can also i don't know like uh check
39:39 out the keras examples i really like the
39:41 more pie torch python examples and stuff
39:44 if you want to learn about the practical
39:46 site
39:47 for the theoretical side i don't think
39:49 there is much to learn like at the end
39:51 of the day it seriously is just a
39:54 different form of data representation
39:56 and solving your problem according to
39:59 that
40:00 so
40:01 you just
40:03 learn how to represent and process your
40:05 data and it's not even like the tabular
40:08 site to be honest like in nlp what we do
40:11 is we
40:12 tokenize the text which means like
40:15 you have a big paragraph and you or like
40:17 a sentence and
40:19 you
40:20 put them into pieces and just
40:23 um match those pieces to some numbers so
40:26 that your computer can understand that
40:28 it's just like pixel values uh how they
40:30 are like
40:31 uh labeled between 0 and
40:34 255 in nlp we have like pieces of text
40:38 and they are numbers
40:40 so after that point it's it's more about
40:42 you know how you represent your data and
40:45 um that's pretty much it's like most of
40:48 the problems are solved very in a very
40:50 similar way so i would say you can take
40:52 the hugging phase course and
40:55 look at the for instance in our github
40:57 we have many code examples i'm going to
40:59 send
41:01 in transformers i think the question was
41:04 also about outside of hacking face maybe
41:06 the person who asked already knows about
41:08 the hacking phase course
41:10 yeah yeah yeah it's so basically the
41:13 how can i say
41:15 um
41:17 raza has a good uh course right but raza
41:20 is for chatbots like building chatbots
41:22 in general yeah like if you want to
41:24 solve problems it usually goes from
41:26 transfer learning and
41:30 like um how can i say there are a couple
41:32 of libraries you can use to do that
41:36 like spacey is one of them i think space
41:39 also has a course um
41:42 that they can
41:44 use
41:45 but like again most of the time like i
41:48 come to a realization that it's mostly
41:50 about the data representation i've i've
41:53 read so many uh books about this like
41:56 for instance i read the nltk book
41:59 uh that is like the most famous nlp book
42:02 i think to this state
42:04 and it was again like mostly about the
42:07 data representation and um optimizing
42:10 your neural network
42:11 and today we have
42:13 like pre-trained models like birth or
42:15 gpt
42:16 and we
42:18 just fine-tune
42:20 them on
42:21 the downstream tasks like named entity
42:24 recognition or sentiment analysis and
42:26 you usually get better results than just
42:29 training from scratch to be honest
42:31 so that's why i think someone needs to
42:33 learn about transfer learning
42:36 in general or maybe like if you're
42:38 starting deep learning from scratch just
42:41 you know there's like so many nlp with
42:44 tensorflow stuff i'm going to
42:46 send that
42:49 in coursera it is also another good
42:52 course
42:54 uh taught by lawrence
42:58 i'm sending you the link
43:01 yeah thank you
43:02 and i think at the beginning you
43:04 mentioned when answering this question
43:06 you mentioned that you need to first
43:08 also ask yourself what do you need to do
43:11 and pick the task suitable yeah yeah or
43:14 for what you want to do
43:15 and
43:16 they have some ideas like what exactly
43:19 what could be like good projects like
43:21 let's say i want to learn the nlp and
43:22 it's pretty abstract
43:24 as abstract as it can get so i just want
43:26 to learn nlp so what could be a good
43:29 first project like should it be i don't
43:31 know named entity recognition or
43:33 i think the good first project is
43:35 definitely sentiment analysis because
43:38 the
43:38 easiest representation of data is going
43:42 through sentiment analysis like you have
43:43 sentences and labels
43:46 and it's seriously like not much in a
43:49 named entity recognition you have
43:51 the sentences
43:53 and inside there are like spans of text
43:56 and same with also like their labels
43:59 and same with question answering as well
44:01 or for instance
44:04 um
44:05 let me think like for instance
44:06 summarization or paraphrasing these are
44:08 also hard tasks most of the time because
44:11 like in summarization like there are
44:13 different types one is extractive
44:15 summarization
44:17 in which you try to pick the important
44:19 sentences from a big paragraph paragraph
44:22 of text and representing that is also
44:24 hard
44:25 so i would say sentiment analysis and
44:28 anything
44:29 that is like on sentence and the label
44:31 is an easy way to get started with
44:35 yeah and
44:38 that's what i did as well like i have a
44:40 couple of github tutorials
44:43 i can send them
44:45 so it's more like classification right
44:48 yeah yeah it's about classification yeah
44:50 yeah
44:51 i have a poetry classification notebook
44:54 that is like a tutorial sort of
44:56 i'm going to send that
44:58 yeah
45:00 they are also on kaggle's
45:02 um i just sent that and there is not
45:05 much to analyze about text as well to be
45:08 honest like it's not like uh you know
45:11 very
45:12 big tabular data sets
45:15 because like in text most of the time
45:17 your features are universal it's not
45:20 like very specific and the distributions
45:22 are
45:23 also not
45:24 very specific to the data sets like the
45:27 tabular ones
45:30 so yeah like i sent you my github
45:33 project this was this i already shared
45:35 it yeah this was like the first tutorial
45:38 i have written about the
45:41 how can i say nlp
45:44 and yeah
45:46 okay
45:48 yeah thanks uh the next question is what
45:51 is the best way for a newbie to get
45:52 involved with an open source project i
45:54 think we mostly answered that so we
45:56 talked about sprints
45:58 we also talked about non-con non-code
46:01 contributions we talked about
46:03 hacktoberfest we talked about um google
46:06 summer of code and good first issues
46:08 yeah
46:10 is there anything we forgot to mention
46:13 or we can just move on to the next one
46:15 yeah we can move on
46:17 okay
46:18 what is the most important topic what
46:21 are the most important topics in nlp
46:23 right now
46:25 yes this is a very good question
46:29 so lately hugging face
46:31 is just got really away from nlp i would
46:34 say but
46:35 it does vision multi-modal stuff
46:38 reinforcement learning
46:40 so
46:41 i don't know i seriously like for um how
46:44 can i say
46:45 i am not super up to date
46:48 with it
46:49 because like lately if you have noticed
46:52 you know on the internet as well like
46:55 most of the trending models are
46:57 multi-modal or
46:59 generative models like dali
47:01 i don't know
47:02 i read two good papers this year one was
47:05 flamingo uh i'm going to write
47:10 by deep mind yeah it was a great paper
47:13 i'm going to send it it's also
47:16 multiple
47:18 it's like a multiple solved thing
47:20 multiple tasks with one model
47:23 and t0 model by hugging face uh
47:28 is also like a multi-task model it's
47:30 like a very big chat what you can speak
47:32 to
47:34 i am sending you another link
47:37 so i would say it's focusing on most
47:40 generalization
47:42 without further fine-tuning your models
47:45 so we call it zero shots
47:48 so i just want to speak to this model
47:50 and let it answer me
47:53 and this is like a very big trend
47:55 in flamingo model or there was also
47:58 google's palm model
48:01 i'm going to send that as well
48:03 it's a very good model it's like i read
48:06 that you know like i'm usually not
48:09 impressed by the
48:11 models anymore but i was really
48:13 impressed by this it was doing
48:15 arithmetic
48:17 you know code compilation
48:20 i don't know
48:21 it explains jokes and stuff
48:23 and you do not strokes yeah yeah it's a
48:25 very big model it's like
48:27 and they benchmarked the skills of the
48:30 model against uh the number of
48:32 parameters as well so when you go to the
48:34 website you will see these three
48:37 like for instance in
48:39 540 billion parameters it pretty much
48:42 does it does everything
48:44 so i would say the latest trend is to
48:48 um just have like a very big model that
48:52 can do anything any task
48:54 but these are obviously not
48:57 released open source most of the time
49:00 like we do with hugging phase so
49:01 currently we are training a model i
49:04 don't know if it ended
49:06 to begin
49:07 we are training a very big model i think
49:09 it's released
49:11 um that has like uh
49:14 a lot of parameters that i don't
49:16 remember because that's what the
49:18 big science team does so yeah
49:22 yeah that's quite comprehensive answer
49:25 yeah thanks
49:27 as next question is what is the
49:28 difference between what you do
49:30 as an lp ml engineer this is what would
49:34 an nlp data scientist do
49:37 to be honest i don't think
49:39 there is something called nlp data
49:40 scientist
49:42 it's mostly
49:43 you know like nlp ml engineer i feel
49:45 like you know i have this perception
49:47 that data scientists are mostly people
49:49 who are doing
49:50 exploratory data analysis visualization
49:53 and analytics meanwhile ml engineers are
49:56 like training models optimizing the
49:58 inference time or
50:01 deploying them
50:03 um
50:04 so it's the answer seriously depends on
50:07 companies like if you are working in a
50:08 very big company
50:10 then uh your job becomes much much
50:13 scoped but if you are
50:15 working in a startup then you pretty
50:18 much do everything you are like both of
50:20 them
50:21 i would say
50:22 so it's it really depends and i i have
50:26 never seen like a nlp data scientist
50:29 add a job application ad
50:32 in general so yeah
50:34 yeah i think at some point of this
50:36 conversation you mentioned that there is
50:38 not so much exploratory data analysis
50:40 happens in nlp so it's most modeling
50:43 right yeah and then they usually
50:46 analyze the model you know the biases it
50:48 has with specific
50:50 inputs about you know like genders races
50:54 and everything so
50:56 i would say it's mostly post processing
51:00 like after you train the model you do
51:02 stress tests on the model to see if it's
51:05 biased or not so i would say the
51:06 analysis is mostly after training the
51:09 model
51:11 yeah
51:13 what type of project would they
51:15 recommend that new data scientists
51:17 attempt when trying to
51:19 catch the eye of employers for
51:21 entry-level data science positions
51:24 honestly like uh
51:27 anything works i would say um
51:31 and seriously anything
51:32 yeah yeah exactly like most of the
51:34 people don't even do that so it's a plus
51:38 if you do it's i think kaggling is like
51:40 really helps like there is a lot of
51:44 good stuff on kaggle and like
51:47 um i think the companies actually keep
51:49 an open source thing or not
51:51 and kaggle like everything is open over
51:53 there another thing is uh at hugging
51:56 face we have something called spaces i
51:58 have told you about this
52:00 but basically in spaces
52:03 we host your model demos
52:06 made by streamlit or gradio
52:09 open to everyone so it's i use it so i
52:12 used to use it sort of like a personal
52:13 portfolio of
52:16 of my models because i don't think that
52:19 you know the technical recruiters
52:20 actually go to your github profile and
52:23 run your models and try to make
52:25 inference out of them
52:27 so it's actually good to have yeah yeah
52:30 it's actually good to have like a ui of
52:32 what your model does not just recruiters
52:35 like as a hiring manager i wouldn't do
52:37 this this is just too much time like
52:39 yeah yeah first of all i need to have
52:40 the environment and this is already
52:42 tricky like pretty tricky like what if
52:44 like even if you have the
52:46 requirements.txt file right it's not
52:49 it doesn't mean it's easy it's going to
52:51 run more
52:52 like i need to do git clone and i need
52:54 to create a virtual environment then i
52:56 need to install like everything and then
52:58 i need to i don't know figure out how to
53:00 run this thing yeah yeah exactly exactly
53:03 so it's actually like
53:04 instructions right it will take like 10
53:07 minutes of my time yes i might not have
53:10 but as you said if it's already hosted
53:12 if there is ui that's that's really
53:14 great also like if i were applying to a
53:17 new job and if i would you know like if
53:19 they would expect me to build something
53:22 i would definitely build the stream
53:25 little gradual ui and just send them
53:27 that instead
53:29 where you can just
53:30 run python app.pi and it's just
53:34 runs but like having uh an open hosting
53:37 of these models with
53:39 you know it literally takes one minute
53:40 to upload those files and hacking face
53:44 builds it for you
53:45 or like you can also use other cloud
53:48 providers and stuff
53:50 extremely it has some cloud yeah yeah
53:53 it's just
53:54 i think very convenient and you know
53:56 like the recruiters go hey this person
53:59 actually does what i am looking for you
54:01 know
54:02 which is
54:03 like already a proof of
54:05 what you can do
54:07 i would say
54:08 yeah i remember so to alex as a part of
54:11 our recruiting process we have a home
54:14 home assignment right yeah
54:16 and most of people just do what
54:21 ask please you know train a model and
54:24 then answer a couple of questions right
54:26 very few people
54:28 deploy this model
54:30 and
54:31 only one person in
54:33 three and a half years that have been
54:34 working at helix and take part in hiding
54:37 process only one person deployed this is
54:40 a stream delete application
54:42 and that person was hired because it was
54:44 so nice to just
54:46 i don't know if there is correlation or
54:48 causation probably
54:50 probably he wasn't hired just because of
54:52 that
54:53 but it was so nice that okay here is the
54:56 like when he sends the email
54:59 here is the zip archive and here is the
55:02 stream lead up that you can play with
55:03 this and the first thing i did was i
55:05 just clicked on this and then uh so it
55:07 was like i think
55:09 firstly i mean
55:11 like as a machine learning engineer
55:13 previously i hated to build flask
55:15 applications for hours just to show it
55:18 to the client for five minutes and not
55:20 even take it to production i could have
55:22 just done a stream that good looking
55:24 streamlet or graduate application and
55:26 just give them a link
55:28 and secondly i am currently like for
55:30 instance for my master's projects i was
55:33 hosting them openly on hugging face
55:35 spaces and people were incredibly
55:38 impressed that i actually did that
55:39 because
55:40 cas have hard time or like professors
55:43 have really hard time trying to get your
55:45 application up and running
55:47 you know
55:48 yeah yeah i i do know yes
55:52 so you're doing streams on twitch aren't
55:54 you
55:55 yeah i mean i gave up because like uh
55:59 the
56:00 how can i say so basically i recently
56:03 moved to paris and if prior to that um
56:07 in turkey my internet was extremely bad
56:10 and you know like i was i even had the
56:12 talk with abhishek and i had to cancel
56:15 it for once and i said this is the last
56:17 thing i'm going to do because
56:19 it is actually quite disappointing
56:21 so i think i'm going to get back because
56:24 now i'm in paris and my internet is
56:26 actually stable i will probably get back
56:28 to doing podcasts with uh awesome people
56:31 because like i met really good people in
56:33 here from
56:34 i don't know like psychic learn that i
56:36 could you know like there's also people
56:38 from big science in hugging face so yeah
56:42 did you bring your microphone with you
56:45 i'm going to go back to my hometown to
56:48 bring it because it's too heavy like
56:50 it's it's it costs a lot so
56:54 initially i thought you know i will
56:55 bring my essential stuff and come back
56:57 and you know take it
56:59 but i can just you know do it anywhere
57:01 so
57:02 you know
57:03 it should be fun
57:05 okay
57:06 um so yeah my next question was uh about
57:10 the podcast yeah podcast influence
57:12 podcast but i guess this is something
57:14 you're not doing
57:18 for a while yeah for a while i i gave up
57:21 because of my internet again i had to
57:24 cancel episodes because of how the
57:26 internet was incompetent
57:28 but now that i'm here i'm planning to
57:31 have them physically actually so for
57:33 instance i might just to visit berlin
57:35 and we can have like a physical uh
57:37 you know
57:38 podcast session together
57:42 that would be nice okay tell me when you
57:44 go to berlin
57:47 okay i think we should be wrapping up i
57:50 don't know
57:51 maybe is there anything you want to say
57:53 before we finish
57:55 um people can uh like
57:58 for the last month i was very busy but
58:00 like if you have any questions you can
58:02 reach out to me through the data talks
58:04 club slack
58:05 directly or
58:07 the
58:08 my twitter account i usually respond so
58:12 yeah
58:13 okay
58:15 yeah there is actually one question
58:17 um so why are you so bad at mario kart
58:22 it's like it depends on the speed like
58:25 at 50 and 100 cc i'm actually not that
58:28 bad but after 150 cc i'm really bad
58:33 because it's like uh
58:34 it's like a trip you know it's like a
58:36 psychedelic trip to actually
58:38 you know play it that fast so basically
58:41 in hugging face office people love mario
58:43 kart and we are planning a tournament
58:45 really soon and they are like really
58:46 good at this like you cannot believe
58:49 so i am trying to just improve myself in
58:52 the meanwhile and uh yeah
58:56 that was a question from one of your
58:58 colleagues
58:59 i don't know i posted about how i'm bad
59:02 at mario kart so
59:04 it can be that or from my colleagues
59:08 yeah they bash me a lot
59:10 okay
59:12 okay i think that's all we have time for
59:14 today um like i actually didn't ask half
59:17 of the questions i prepared but maybe
59:19 next time who knows yeah
59:21 so
59:22 yeah thanks a lot thanks for joining us
59:24 today thanks for finding time to answer
59:26 our questions that is always pleasure
59:28 talking to you yeah i need to see you
59:31 yeah goodbye every