0:00 hi everyone Welcome to our event this
0:02 event is brought to you by datadox club
0:04 which is a community of people who love
0:05 data we have weekly events and today is
0:08 one of such events if you want to find
0:09 out more about the events we have there
0:12 is a link in the description so go under
0:15 the video you will find it just click on
0:17 this and you'll see all the all the
0:19 events we have in our pipeline
0:21 very important thing if you haven't
0:23 subscribed to our YouTube channel do it
0:25 now so there is this big red button I
0:27 think on YouTube on Android now it looks
0:29 a bit different at least on a tablet but
0:32 yeah I'm sure you'll find it
0:34 uh and I recently checked check
0:36 statistics and I think 50 of people
0:39 watch these videos without subscribing
0:41 so you're missing out
0:43 then of course we have a pretty cool
0:45 flag when you where you can hang out
0:48 with other data enthusiasts so there is
0:50 also a link in the description and
0:52 finally during today's interview you can
0:53 ask any question you want there is a
0:55 link in the in the live chat it's pinned
0:58 so just click on this link ask your
1:00 questions and I will be covering these
1:02 questions during the interview
1:04 okay
1:05 that's the the intro
1:09 so now I will open the questions we
1:11 prepared that you don't make prepared by
1:14 the way today say Tomic or tomash what
1:17 do you prefer
1:19 whatever works for you
1:22 stomach is a bit easier to say and I
1:25 guess this is how I usually call here
1:28 right
1:29 so I also company sometimes alphabet
1:32 yeah right
1:34 okay make sure the story later on
1:37 to the start
1:40 okay so this week we'll talk about data
1:42 Ops and we have a special guest today
1:44 Thomas or tomek he is a data Ops who
1:48 lives in Poland in Bosnian after working
1:51 in product analytics data engineering
1:52 data science and machine learning he
1:54 fell in love with operations and he
1:57 finds peace in fixing poorly written I
1:59 am roles and teaching people so I really
2:01 loved that that line so welcome welcome
2:05 talk
2:06 hello everyone thanks for joining us
2:09 like on this kind of Niche topic still I
2:13 hope after this let's say podcast or
2:16 like video Vlog it will become less
2:20 niche
2:22 attention in my OPM news probably as
2:26 well yeah I'm surprised that you worked
2:29 in so many different roles so probably
2:31 we will start our interview with that so
2:33 before we go into our main topic of data
2:36 Ops and becoming data Ops let's start
2:38 with your background can you tell us
2:40 about your career Journey so far
2:42 sure
2:44 well
2:44 [Music]
2:46 if you count to the career uh also the
2:49 university university time uh that I
2:52 studied
2:53 econometrics uh which is kind of exotic
2:57 or at this was back then
3:01 um then uh kind of accidentally uh
3:03 because a friend of mine uh shared the
3:06 information with me that yeah this
3:07 company or Alex is actually hiring I was
3:11 never heard of it
3:13 so by totally uh like total accident I
3:17 joined the company
3:19 um as a machine learning trainee
3:22 but then I um was working as a junior
3:27 data engineer uh there was a data
3:30 scientists and so on and so on so
3:33 um so basically alongside like all this
3:36 rows and and positions I I've I believe
3:40 I touched
3:42 um
3:43 a lot of
3:46 let's say uh
3:49 both positions are like Steps in that
3:52 whole cycle of like analyze data
3:55 accurate the model publish the model
3:57 created the product
3:59 yeah the idea so because I'm getting
4:02 easily bored
4:03 with stuff I try to touch as much as
4:07 possible and like don't don't be an
4:09 expert on one particular domain or or
4:13 like uh once One Step only so like only
4:17 building models or only doing the
4:18 analysis I wanted to know like what the
4:21 other folks are doing
4:22 and why and how this connects to my own
4:25 work
4:26 so that's that's why the scope was
4:28 rather Broad
4:31 so you tried all these positions all
4:34 these roles while working at the same
4:36 company
4:37 mostly uh
4:43 it worked at the central statistical
4:46 office in Poland to to like see how the
4:50 government statistics look like uh also
4:53 some some some like brief episodes with
4:56 University and some other companies and
4:59 yada
5:00 but many Rolex
5:03 and you mentioned that you got into OLX
5:07 SML ml trainee by accident
5:10 and um I think many of our listeners or
5:13 people who are watching this might be
5:15 wondering what was this accident so can
5:19 you tell us more about that sure
5:21 um like by accident because I mentioned
5:25 the accident because I I heard actually
5:27 searched for like specifically like yeah
5:30 I definitely want to be into machine
5:31 learning and
5:33 stuff I just received the link that says
5:36 hey some company is hiring for the
5:38 machine learning uh internship I was
5:41 like why not
5:43 um why this might be a little bit
5:45 awkward uh to some of the listeners
5:48 because right now machine learning is
5:50 well whole topic back then it's probably
5:54 hard to believe but at least in Poland
5:58 um nobody heard of pretty much about
6:01 machine learning it was called back then
6:02 a multi-dimensional analysis
6:05 at the University so you basically took
6:08 uh the course like uh doing all the
6:11 principal company analysis
6:14 classification clustering
6:16 but it wasn't called even machine
6:19 learning it was like multi-dimensional
6:21 analysis because a lot of folks there
6:24 were from the statistical background and
6:26 they got just different naming
6:28 conversion let's say
6:30 so that was that was the accident
6:34 yeah so you work this machine learning
6:36 trainee then you worked as a junior data
6:39 engineer then you also worked as a data
6:41 scientist right so can you tell us what
6:44 you did and uh like after working as a
6:46 data scientist you became in interested
6:49 in data Ops and you became a dead Ops
6:51 but before you became a data Ops uh
6:53 engineer I don't know how to properly
6:55 call this role so before you started
6:57 doing data Ops either a data scientist
7:00 so maybe can you tell us what you were
7:01 doing what kind of tasks you had what
7:04 kind of response responsibilities you
7:06 had sure I was working mainly in the
7:10 uh under Motors customer unit because
7:13 our legs got different units and so on
7:15 and so
7:16 um so I wasn't in the like core data
7:19 science team rather under the one of the
7:22 business units
7:24 uh I was doing mainly uh behavioral
7:27 analysis so like analyzing
7:30 um click stream data trying to do some
7:33 models
7:34 uh on that like like to capture some uh
7:38 interesting uh
7:40 signals trying to catch people
7:43 um who might perform action we actually
7:46 are interested in and like
7:50 trying to grab them basically
7:53 uh also a little bit of product
7:58 analytics alongside the work plus
8:01 obviously uh some operations because
8:04 that's how uh the data helps Journey
8:06 started for me because
8:08 a lot of people might might believe that
8:11 well if you are a data analyst or data
8:14 engineer or data scientist you don't
8:16 need like a lot of operation skills
8:19 that's a little bit of
8:22 misunderstanding because
8:26 even if you are working in data because
8:31 if you are a programmer you are
8:33 perfectly aware that they are like the
8:35 sres devops and stuff like that uh in
8:40 data not really but
8:43 that might be a little bit of context
8:46 story but I guess it's relevant for the
8:49 further discussion
8:51 uh so let's say you are a data engineer
8:54 and out of the sudden you start a new
8:57 project and you need to create a new S3
9:00 bucket or kinazi's stream or whatever
9:04 um you believe that you won't be like
9:06 deeply uh involved in that because uh
9:10 there is a platform team or there is an
9:12 infra team or whatever other team like
9:13 the central core
9:15 stuff
9:16 um and they go to them like hey folks I
9:19 need a new LC bucket and they are like
9:21 that's cool uh so here's the link to the
9:23 repository uh create the match request
9:25 and we'll do the review like on priority
9:27 because we like it
9:29 uh then you're like oh crap uh I need to
9:33 learn like terraform telegrams Atlantis
9:35 and obviously a cloud provider yeah so
9:38 uh that's about like not needing to
9:40 learn uh operation skills or maybe as a
9:43 data analyst
9:44 um you also believe that I don't need
9:46 that stuff
9:47 um but then out of the sudden you need
9:49 to actually understand the data flow in
9:51 the company right
9:53 and because you prepared like awesome
9:55 report uh providing view on some uh part
9:58 of the business unit
9:59 and the results are pretty important
10:01 they are about to be shared to the
10:03 leadership and your boss asks you like
10:06 the one really uncomfy question how
10:09 confident you are with the results
10:10 because that's 10 drop in the revenue
10:14 looks kind of suspicious so maybe some
10:16 ETL job style this and that maybe some
10:20 server were basically down maybe there's
10:23 an issues tracking or or stuff yeah so
10:27 then out of the sudden you need to uh
10:29 understand not not only your part but
10:32 like the whole pipeline
10:35 um or even as a data scientist uh you
10:38 might think you might be thinking that
10:40 okay my job ends pretty much at the
10:43 level of Jupiter notebook and that's it
10:45 then then there will be some Almighty
10:47 Big Data team or like machine learning
10:50 Engineers who will take that stuff and
10:53 put it on plot
10:55 yes and no because for instance you
10:59 created the model that prediction time
11:01 is like one second
11:03 and products folks came to you that okay
11:07 it's cool but you need to go down to
11:09 like 300 milliseconds oops uh or you
11:12 create a model that is outputting let's
11:14 say a list of cookies
11:18 um and some folks from marketing are
11:21 like that's super cool but we don't have
11:23 any possible tool that is able to
11:25 consume it we can consume some rules
11:29 so now now you've realized that okay I
11:31 created the model that is pretty much
11:33 useless
11:34 because the output format doesn't match
11:37 so those are pretty hard tasks and it's
11:41 pretty unexpected for a lot of folks
11:43 because they are believing that there
11:45 will be someone else uh who will be
11:47 doing that that's not necessarily true
11:50 and pretty much that's where the data
11:52 Ops come in because people need help uh
11:57 in that manner and also that is a
12:00 possibility for a huge misconception
12:02 here because uh some listeners might be
12:06 thinking now that okay so now I finally
12:08 get what data Ops is about this will be
12:11 the person who will create the
12:13 infrastructure for me uh or who will do
12:15 the maintenance for me not necessarily
12:19 data is the person who will help you
12:23 to work effectively uh who will help you
12:26 to design the solution uh who will
12:29 basically make your work less scary
12:32 you will not do something like for you
12:36 he will teach you how to do it
12:39 effectively so for you the teaching was
12:42 you you needed like in your story you
12:45 needed the bucket you need the Keynes
12:46 stream and then you ask somebody for
12:48 help and then they said we're busy
12:50 create a pull request right or Mercy
12:52 Quest whatever and you were expecting
12:54 that this these people would help you
12:57 what they kind of said okay just do it
13:00 yourself and then here is there report
13:03 you need to create a merge request for
13:05 right
13:06 sort of but also to to maybe defend a
13:10 little bit the platform teams or the
13:12 security or the sres
13:14 they are not supposed
13:17 um let's say you are asking like the
13:19 security team to to like create a
13:21 service role for you
13:23 uh it will be not very responsible from
13:26 uh their site to like just throw out of
13:29 the fence uh some relative stuff to
13:33 someone who doesn't know how this
13:35 thing is supposed to work they should
13:38 help you they should do the review they
13:40 should guide you but not necessarily
13:43 like do the job
13:46 like for you
13:49 okay so you as a in all these roles that
13:53 you had over over this time like as a
13:56 data engineer as a data analyst as a
13:57 data scientist in all these instances
14:00 you needed to touch the infrastructure
14:01 right and this is how you
14:03 learned how to do this and this is how
14:05 you fell in love with
14:08 doing all this stuff right that's
14:10 totally correct
14:12 so when did you realize that you
14:14 actually enjoyed doing this stuff more
14:16 than your work as a data scientist how
14:18 did it happen
14:20 I'm glad that this question was uh
14:24 stated because
14:26 um
14:28 there are again a lot of misconceptions
14:31 about which which role in data is more
14:33 important than the other role uh so
14:37 before answering the questions
14:39 the question directly no role is more
14:43 important than than other role and
14:45 especially you might be thinking that do
14:48 I need to have a data office in the
14:50 company uh answer is no
14:52 uh which might be surprising for some
14:55 folks uh it's not a mission critical
14:58 role uh it's rather
15:02 support or
15:04 [Music]
15:05 imagine that you are playing a game and
15:09 you are going to the boss fight
15:11 and you are going to the to the
15:14 particular boss fight with a broken
15:15 sword and without potions is it doable
15:19 yes will it be fun probably no so data
15:24 Ops is kind of that's buff that's a
15:27 fixed sword that's uh plenty of potions
15:30 and stuff like that yeah so it's useful
15:33 but not necessarily mandatory and now
15:36 I'm asking the question maybe
15:39 um
15:40 why because I
15:43 I wanted to solve problems and it turns
15:46 out which kind of correlate with what
15:49 Andrew and if I'm pronouncing the
15:53 surname correctly uh kind of discovered
15:56 or like uh is trying to make people
15:59 aware of
16:01 uh is that the whole domain went
16:05 recently from the model Centric approach
16:08 to data Centric approach
16:12 which essentially means that
16:16 uh if you are doing uh work in some
16:20 large company
16:21 and your job is not to create the model
16:24 to explain the behavior of something and
16:27 then forget about it
16:29 you are rather about creating the data
16:32 product uh and surprisingly majority of
16:35 the work uh in like creating the data
16:39 product is in operations
16:40 uh using like data uh not not modeling
16:45 itself
16:46 so kind of naturally uh because we were
16:50 working uh on like data products we're
16:55 solving more and more and more of
16:57 engineering problems not necessarily the
16:59 scientific ones uh so that's how I fell
17:03 in love
17:05 uh in that style but data Ops is not
17:07 necessarily more important than data
17:09 science
17:10 um actually with Alexi we both know a
17:14 perfect example of that because while I
17:16 was transferring from data science to
17:18 dataops we know one person who was doing
17:22 actually the opposite
17:24 uh one SRE with like plenty of
17:27 experience in operations was going like
17:30 through the same bridge but in opposite
17:32 direction
17:33 uh from Ops uh to data scientists so
17:36 like no career is
17:40 worse or better than other yeah so like
17:43 don't get us wrong
17:46 and I think you said at the beginning
17:47 that um for you doing something the same
17:51 thing for a long time is boring because
17:54 you want to do a lot of different things
17:55 and I think many people are like that
17:58 right
18:00 that's also a fact but
18:02 so not everyone but some people are that
18:05 it's just too boring to keep doing the
18:07 same thing uh all the time and our
18:10 colleague
18:12 for him it was probably boring to do up
18:16 stuff all the time right and he he
18:18 wanted to try something else
18:20 yes so for yeah so for you so you were
18:23 doing data science and then you realized
18:25 that uh like you need to spend a lot of
18:27 time doing this infra stuff and this is
18:30 where a lot of problems are and to be
18:34 like you said you love solving problems
18:36 and I guess you saw that there are a lot
18:39 of problems with actually Ops part right
18:42 and then for you the reason you became
18:45 interested is because
18:46 um like maybe I misunderstood you but
18:48 you felt like okay
18:50 I'm more useful solving these problems
18:52 and I actually like doing this
18:54 that's why you started digging deeper
18:57 into this okay and uh we've been talking
19:00 about data hopes for quite some time or
19:03 your transition into this but we didn't
19:05 actually discuss what data Ops is so
19:08 what is data Ops
19:09 uh I love that explanation
19:12 um that was given on one of um the talks
19:16 in the data box club uh which Chris uh
19:21 if I remember correctly the grandfather
19:23 of data
19:26 essentially dataops
19:29 engineer or whoever is taking a look on
19:33 how people work
19:35 not like doing the reports himself or
19:40 herself or like modeling or like putting
19:42 the models on on protea
19:45 he or she is looking at how people work
19:49 uh where might be the inefficiencies uh
19:53 how to overcome them and basically help
19:56 help people to
19:57 produce that meaningful results faster a
20:02 more pleasant way less scary way
20:05 and stuff like that so for me that might
20:08 be the shortest description possible
20:10 which is essentially the same stuff as
20:12 data Pro sorry devops are doing
20:18 and if you think about that uh it's not
20:21 a New Concept like uh programmers knew
20:25 that for some some time that's that's
20:27 how devops came
20:29 uh but even before uh lean uh kaizens
20:34 textigma stuff like that in in like uh
20:39 um companies that that are producing
20:41 something physically
20:43 that was there and the concepts are
20:47 super similar we are producing software
20:49 okay but the philosophy is exactly the
20:52 same emo
20:54 right okay but uh how is it related to
20:58 infrastructure and all these things that
21:00 we talked about because you said okay
21:02 datos is about solving inefficiencies uh
21:06 helping people overcome problems
21:09 than I don't know what was uh produced
21:12 results faster right so how is it
21:14 related to infrastructure
21:16 excellent question uh because
21:20 um like database is not not infra only
21:23 uh it's also like helping people write a
21:26 better SQL queries as simple as that
21:30 helping people in keeping the
21:35 let's say the secrets uh stored in the
21:38 proper uh locations and accessing them
21:41 the proper way and stuff like that
21:43 um
21:45 but it turns out that a lot of um
21:49 confusion is actually about infra like
21:52 okay how should I create the three
21:55 buckets through that full githubs yeah
21:57 so uh someone might be like okay let's
22:00 try some example and just switch the
22:04 names and try to apply that and see what
22:06 will happen but then Atlantis is
22:12 like ah crap I uh just broke the
22:15 production
22:16 no everything is okay but um for the
22:20 first time if someone never used like
22:22 like the githubs to make some changes in
22:24 the infra it will be scary honestly so
22:27 that's why it's rather a good idea to
22:30 sit with that person like literally on
22:32 the zoom call
22:34 and I go step by step uh do you know
22:36 terraform no so let's go do you know no
22:40 and blah blah blah and out of the sudden
22:44 um all the errors all the concepts uh
22:46 became less like ambiguous and like for
22:50 the
22:51 um specialist like it's getting
22:54 technology more familiar to to like
22:59 people to to give it more human face
23:04 yeah so you mentioned gift tops creating
23:06 a bucket through the Atlantis and maybe
23:10 can you walk us through the process like
23:12 how exactly this process looks like
23:14 maybe high level without going to
23:17 technical uh just for those who don't
23:19 know well I I think I know a little bit
23:21 maybe I will also check if I know how
23:24 this thing actually works
23:26 I guess you do uh
23:28 so essentially you don't need any stuff
23:31 like infrastructure as code because uh
23:34 that's that's what terraform paragraph
23:36 and all that jazz is for or cloud
23:38 formation if you are working with uh AWS
23:41 uh you can what is infrastructure is
23:44 called maybe before we even go there
23:46 like for those questions so
23:50 if you want to create something you can
23:52 you can as well go to the to your web
23:55 browser authenticate a
23:58 and you can just click here and there
24:00 and create some resources some roles
24:03 some buckets some kinesi streams
24:06 stuff like that with your mouse right in
24:08 exactly in Amazon web interface right
24:11 but then imagine that um
24:14 uh you want to create one bucket on
24:17 staging and one bucket on production so
24:19 you are doing essentially the same stuff
24:21 uh we're changing just a little bit of
24:24 azure bucket name
24:26 or or some tiny details yeah so that's
24:30 that's how
24:32 um a lot of smart folks in operations
24:34 came to the conclusion that maybe if
24:36 we're defining like all the
24:38 configurations all the infrastructure uh
24:41 as code not not as clicking here and
24:43 there uh it will be more manageable uh
24:47 we can do some audits uh we can do like
24:50 uh all that stuff via match requests
24:54 which can be reviewed
24:56 and like everybody in the company will
24:59 be able to create a match request as
25:01 they say it and then someone
25:04 um
25:05 from like more infra teams will go there
25:09 and like to check your match requests
25:11 but everybody can do so that's that's
25:13 the enablement
25:15 so essentially you are writing some
25:17 terraform uh which is a huge config file
25:22 let's put it that way Terra Grant is
25:25 putting some variables uh to the
25:28 telephone code
25:29 and what Atlantis is is doing is uh
25:34 displaying all the changes that are
25:37 about to be made
25:39 um in the pull request or the match
25:41 request
25:42 when you can review what will be changed
25:46 if the given procedure let's say will be
25:49 would be applied so it's kind of dry run
25:53 and then after like uh forgetting the
25:56 approval from
25:59 from a series or whoever
26:02 you can just click on
26:05 uh match or before that Atlantis apply
26:09 and the changes are made
26:12 um to the infrastructure
26:14 it's probably the kind of a little bit
26:18 technical still but
26:19 I guess I'll try to summarize
26:22 um
26:23 so we have infrastructure is called
26:25 tools and Terra terraform is one of them
26:27 so with terraform we can create a config
26:29 in which this config we create a bucket
26:31 we create around this Kinesis stream
26:33 that you mentioned as a config like as
26:36 code right and then typically without
26:38 git what we would do is we would do
26:41 something like terraform apply on our
26:43 computer right but with githubs the way
26:47 we do it is instead of getting this code
26:50 and running this locally we create a
26:54 branch thread in this Branch we put this
26:58 piece of code and then we create a pull
27:00 request for a merge request right and
27:01 then what that line This is doing it's
27:04 apply and terraform or trying to see
27:07 what would happen if we apply this to
27:09 the to the to our I don't know cloud
27:12 account right and then somebody comes
27:14 and says okay you're like in the city
27:16 some devops person comes or data Ops if
27:19 you will it comes sees that your code is
27:22 not breaking anything they accept the
27:25 merch request and then you merge and
27:27 then at the end of this process you have
27:29 a bucket and the Kinesis stream in uh in
27:32 your account right that's exactly like
27:35 that uh you mentioned something that
27:37 that I haven't uh because essentially
27:40 without githubs you will be
27:43 um as Alexa said uh you will be doing
27:45 all that stuff from your laptop so like
27:48 you will have to have the proper
27:49 telephone version uh and all other tools
27:53 now imagine some poor data analyst uh
27:57 trying to install
27:58 uh data from configure that whoa that
28:02 that would be painful
28:05 yeah I think um it was the biggest
28:07 problem so at elix we thought that it's
28:11 a good idea to ask data scientists to
28:14 work on infrastructure and for that they
28:16 needed to clone this repository with
28:19 terraform
28:20 and then do terraform apply and then
28:22 apply these changes to the cloud right
28:25 and then the biggest problem was
28:26 actually installing this and making sure
28:28 you can apply and then many people
28:31 couldn't do this because it's just too
28:33 difficult and this this is not what data
28:34 scientists are trained to do typically
28:37 right so this is not what we learned in
28:39 University right but I guess for you
28:42 this part you liked it right so you
28:44 enjoyed doing this thing
28:46 uh
28:48 yes uh to be
28:53 a little bit more specific
28:56 um I haven't enjoyed uh you didn't
29:00 enjoyed
29:03 just it wasn't that much about like
29:06 creating the info I I haven't felt in
29:08 love in creating telephone code it was
29:12 rather about
29:14 um helping people do it like uh making
29:18 them comfortable with that stuff that
29:20 was the
29:22 um the the let's say the
29:25 [Music]
29:25 um
29:27 the main part of uh doing data Ops uh
29:31 more for me
29:33 okay so one of the questions I wanted to
29:36 ask you is how did you actually learn
29:37 this thing how did you become a data Ops
29:41 but I think from what I understood is uh
29:44 you just simply had to do this and then
29:46 you would have a zoom call with some
29:48 sort of a SRE or platform engineer who
29:51 would guide you through this process who
29:53 would explain you what terraform is what
29:56 other things are how exactly you need to
29:58 create this merge request to get your S3
30:00 bucket and this is how you learn right
30:03 more or less yes uh but also to like um
30:08 maybe to to to to to make the process
30:11 easier uh if I would start again uh
30:15 like the same stuff uh I would
30:19 I would definitely uh narrow this the
30:22 scope because uh
30:24 if you are asking some some devops
30:26 engineer like uh okay I want to be more
30:28 into operation stuff like what should I
30:32 learn
30:33 um Linux done some cloud provider uh
30:38 Docker kubernetes
30:40 I was like okay so after five years I
30:44 will maybe become useful finally
30:47 some misunderstanding emo
30:51 uh because if you are working in the
30:53 data space uh AWS got what probably like
30:57 200 plus different services
31:02 and then answer your uh yourself if you
31:07 will be like spawning some fleet of uh
31:09 iot robots uh I have doubts if you will
31:13 be working on the quantum Computing I
31:16 have doubts uh if you will be working on
31:18 the ground stations probably not and
31:22 after that pre-filtering you came to the
31:25 conclusion that okay I need really uh
31:27 the IM roles uh Institute machines S3
31:31 buckets you know this may be
31:34 EMR so out of 200 Services you will you
31:38 will learn that okay I actually need 20
31:39 of them
31:40 so that that narrows the scope a bit a
31:44 good example of uh the possible roadmap
31:47 uh could be the roadmap.sh slash devops
31:51 probably uh it's pretty accurate also
31:54 for the data domain
31:58 um but I would say good enough is quite
32:00 okay so uh you don't have to have to
32:03 have to spend like five years in uh some
32:06 kind of basement uh constantly training
32:10 learning and then finally you became
32:13 like a useful guy uh not necessary
32:18 um every single
32:20 team like security sres platform team
32:24 whoever
32:25 got their list of like least favorite
32:27 tasks
32:29 uh for a series it might be like okay
32:31 every single resource uh need to be
32:33 tagged with like name equal assumption
32:37 the owner equals something
32:39 uh the on-call guy equals something uh
32:43 and every single resource needs to be
32:45 tagged this is the sort of the task that
32:48 nobody likes to do
32:50 so if you are a junior uh in the
32:54 operations domain
32:56 and uh you're basically going there and
32:59 asking for the
33:00 um that's um
33:02 kind of rookie tasks and they will be
33:04 super helpful to give you that alongside
33:07 of doing so you will learn a ton and
33:11 everybody will basically love you
33:12 because you are you are taking the
33:14 crappiest work possible out of them
33:17 and at the same time you are actually
33:18 learning so uh it's a win-win situation
33:22 or like let's say a security team they
33:26 might have problems with like hey folks
33:29 are using like uh privileged mode in the
33:31 kubernetes runners like Docker Docker
33:33 it's kind of not okay
33:37 so you have to identify uh all such
33:41 cases and like go team by team and
33:44 explain to them how can you go works so
33:47 it's also a completely rookie task uh
33:49 you will learn a tunnel while doing so
33:53 you will know uh better the people you
33:57 will be working with
33:59 and again it's a win-win situation so
34:02 establish the connections make people
34:05 from the technical teams know about you
34:10 um plus teach others obviously
34:13 and also
34:15 start simple you then have to start from
34:18 administrating the kubernetes cluster uh
34:20 you can just build the docker image on
34:22 your laptop then push it to some
34:26 registry then proceed to the different
34:27 registry like first time to the maybe uh
34:31 GitHub producer then to ECR then try to
34:35 apply the some security scanning then
34:38 create that on the CI pipeline instead
34:41 of your laptop then also steps and blah
34:43 blah blah yeah and out of making some
34:46 little steps you will finally like go to
34:49 the
34:50 more or less the end of the path into
34:55 the that particular task uh one last
34:59 piece of advice on the learning process
35:01 accept that it will be uncomfortable uh
35:05 if you are from the data domain you are
35:07 probably closer to the PHD in stats than
35:11 to the Linux admin
35:13 so now out of being like senior data
35:17 scientists or super powerful Master
35:19 learning engineer or whoever you are
35:23 going to some very different domain so
35:26 it will be it will be like uh forcing a
35:29 weightlifter uh to look at the training
35:33 um
35:34 it's a different specialty
35:37 so it will be uncomfortable and it
35:39 doesn't mean that you are I don't know
35:41 uh unqualified or the worst case stupid
35:45 or whoever yeah
35:47 it's just a different domain it will be
35:50 tough you've got some folks out there to
35:52 support you
35:54 yeah speaking of that so right now we
35:57 have a machine learning engineering
35:58 course and we are currently covering the
36:01 deployment module
36:03 and so far for many students up to week
36:06 five we're on week five now right now it
36:09 was fine because it was jupyter notebook
36:10 but now all of a sudden from this
36:13 convenient comfortable environment of a
36:15 Jupiter notebook we end up doing things
36:18 in the terminal and for you I have a
36:21 question so you have I think you studied
36:22 econometrics right you mentioned that
36:24 then you worked as a data scientist
36:26 and I guess this Linux admin stuff
36:30 wasn't something you like live prepared
36:33 you had to do right so how did you how
36:35 did you learn this uh thing so how did
36:38 you make how did it become comfortable
36:40 for you to work in this environment uh
36:43 Honestly by doing every possible mistake
36:45 uh that was about to to be done
36:49 um
36:50 I know this might sound stupid but uh
36:54 that's how it was but
36:56 um
36:57 what may help you uh in like feeling
37:01 more comfortable with uh the command
37:03 line is to do the proper setup if you
37:06 look at the terminals of some uh studies
37:10 or
37:15 something they will have like plenty of
37:18 [Music]
37:19 um
37:23 that sort of dress and that's for the
37:26 good reason uh it is probably doesn't
37:28 have like uh Auto completion uh the
37:31 syntax highlighting I think I don't know
37:33 if it's just me or everyone but we lost
37:36 a part of at least I lost a part of this
37:38 so just to make sure we we get it so you
37:40 said talk to SRE who already has proper
37:43 setup in the terminal and then I lost it
37:47 uh and ask them basically what what they
37:49 have installed and what for uh because
37:52 uh honestly terminal without Auto
37:54 completion without syntax highlighting
37:56 without proper bash RC
37:59 uh it's not a comfy place
38:03 so with the proper buffer C setup uh it
38:08 will be much more friendly you will see
38:10 immediately the possible mistakes uh
38:13 commands will auto complete uh it will
38:17 be just just better
38:19 okay what if I don't have any City who
38:22 already configured Basher CM can just
38:24 share this information for me what is
38:26 the best place to look this kind of
38:28 information I'm super glad you asked
38:30 this uh because my answer in that
38:33 uh for that kind of questions is always
38:37 the same uh there's a place called
38:39 YouTube
38:40 which is awesome I thought you would say
38:42 Google
38:43 yeah Google is so well it's also awesome
38:46 because you you got like articles on uh
38:49 Hacker News medium whatever
38:52 but it may happen that someone on
38:56 YouTube already created uh like full
38:58 blown uh our video about how to
39:01 configure let's say all my data Sage or
39:06 anything about that so basically type
39:09 [Music]
39:11 command line
39:14 tutorial course setup uh whatever and
39:19 I bet you 50 bucks that there would be
39:22 something in the first 10
39:24 results
39:26 yeah I think I saw a couple of videos
39:27 like that that are one hour long some of
39:30 them are even longer that show you how
39:32 to set up your environment from nothing
39:34 from a clean windows Mac OS Linux
39:37 whatever you use like I saw a video with
39:40 Windows because one year ago I switched
39:42 from Ubuntu to Windows and I needed to
39:45 prepare the environment for that and of
39:47 course for me everything was alien and I
39:50 found the tutorial that just walked me
39:53 through the entire thing like how should
39:54 I set up a terminal and then at the end
39:57 it was like uh you know usual Linux so
40:00 yeah just that single video I
40:03 unfortunately don't remember in that
40:04 video but it wasn't difficult I think it
40:07 was one of the top results
40:10 well people just love to create videos
40:13 uh and there's a high chance that
40:16 someone already created something for
40:19 exactly your case like how to set up
40:21 kubernetes cluster from from scratch or
40:24 the full course about how to prepare for
40:27 uh AWS Solutions architect certificate
40:31 there you go like for free
40:34 on basically YouTube like the most
40:37 common place to search for the car
40:39 teachers or actually movies or pictures
40:43 yeah we have a few questions and two of
40:46 these questions and they ask about the
40:49 overlap between data Ops and data
40:51 engineering what is your in your opinion
40:53 the overlap is there any overlap and if
40:56 there is what is it
40:58 and between data apps and data
41:00 engineering uh email data engineer will
41:03 be more operational like
41:06 meaning he or she will be actually doing
41:09 some pipelines uh preparing some quality
41:12 tracks or whatever
41:13 data Ops will honestly gee if I remember
41:17 correcting my statistics from
41:19 um from Google calendar when I was
41:22 working as
41:23 as our databs in OLX it was like 25
41:26 hours average
41:28 per week on Zoom calls
41:31 so data engineer will probably spend
41:33 more time uh actually in pycharm and
41:37 databs will spend more more time on
41:39 slack and zoom
41:42 anymore
41:43 okay and this Zoom calls uh like what we
41:48 were doing exactly like helping others I
41:50 guess to with problems exactly most most
41:53 often uh honestly live coding uh
41:57 designing some solutions because if you
42:00 think about like which uh which domains
42:04 maybe uh data of touch it's essentially
42:07 uh past meaning absorbing the technical
42:10 depth uh depth uh the present meaning
42:14 handling the user's requests like the
42:17 daily problems and you are also thinking
42:20 about the future So like
42:21 um maybe you just prepare the kind of
42:25 summary of how the past month went uh
42:28 okay so most most people have had
42:30 problems with some service or roles for
42:33 the Gita planners because like they have
42:35 to go like to 12 different suppositories
42:38 so now you are talking with the series
42:40 that guys uh we might want to simplify
42:43 that because there is there is a like
42:45 yet again uh there's a problem with like
42:49 this process being like too complicated
42:51 so maybe we should do something about
42:53 this and also you are educating people
42:55 so imagine you got like newcomers
42:58 uh to the company like uh someone uh
43:01 have been on onboarded by HR but HR can
43:05 onboard you about like uh what are the
43:08 teams what are the structure uh who your
43:11 boss be uh where you can ask for this or
43:13 that but the technical one uh the
43:16 technical onboarding will be probably on
43:19 the uh shoulders of data Ops and it's
43:22 also your job then to catch the
43:26 newcomers to the company and like
43:29 make them comfortable
43:32 okay so I don't know if that answers the
43:34 question but
43:36 um yeah so maybe I'll try to summarize
43:39 so data Engineers actually work in the
43:42 pie charms vs code and so on but data
43:45 hopes mostly use zoom and slack and
43:47 other things that that is how like for
43:50 me it was the summary but I guess there
43:53 is more to that so there is mostly like
43:54 support uh also from at least with the
43:58 series that I see my colleagues they
44:01 always have something open like some
44:03 sort of dashboard or something like this
44:04 so they not only grafana exactly uh or
44:09 New Relic um so not only they help
44:11 people who come with ad hoc requests to
44:14 them but also they see okay yeah
44:16 something is off here let me take a look
44:19 proactive yeah exactly exactly
44:23 and you said you spent quite a lot of or
44:26 like when you in this position you spent
44:28 a lot of time doing live coding uh
44:31 supporting education
44:34 um so it depends from what I understood
44:36 you already need to be quite experienced
44:38 person to work as a data Ops
44:42 and do you need to be
44:44 a data engineer in the past to be
44:47 successful in this role or what kind of
44:49 background is actually useful for this
44:51 role or not useful but how to say
44:53 suitable
44:55 any background in any data position
44:57 anymore
45:00 whether you were like student analysts
45:03 your data engineers in your data
45:06 scientist whoever
45:07 uh it will be useful
45:10 uh why because
45:13 you don't have to you don't have to be
45:15 an expert
45:16 uh you will be serving as a middleman uh
45:21 between let's say the platform team the
45:23 security team industry team and the
45:25 users mainly like the analysts Engineers
45:28 scientists
45:29 uh so majority of the cases are
45:34 literally not that hard so it's like
45:36 more than enough to like be able to read
45:38 the log and try to figure it out what is
45:41 actually happening you are you are
45:43 actually working uh previously as a Java
45:46 developer if I remember correctly so you
45:48 definitely know how verbose uh the error
45:52 messages
45:55 yeah it's sometimes even worse
45:59 which basically mean that
46:02 uh if you are able to help people
46:04 understand their logs if you can help
46:06 them understand how the cross accounts
46:09 uh roles will work in AWS it's more than
46:13 enough you don't have to be like super
46:16 expert meaning uh
46:18 if you will
46:21 out of the sudden need to like set up
46:24 DNS
46:26 records here and there there will be
46:29 some SRE who will be super glad that you
46:33 are asking him or her about the
46:36 technical questions because a students
46:38 like technical questions they typically
46:41 don't super there they are not super
46:45 thrilled to explain someone like over
46:48 and over again how assuming growths work
46:51 they are more thrilled about some really
46:53 complex issue in kubernetes cluster
46:55 that's keep them up and running
47:00 and you are taking that's let's say
47:03 unpleasant or less favorite part from
47:06 them leaving them with them more
47:09 technical side which is like again a
47:11 win-win
47:13 actually
47:14 for both sides yeah so how does the Sim
47:16 roll work
47:19 okay you don't have to explain that
47:21 no I guess you did where amount of
47:26 explanation already right
47:29 uh
47:31 yes
47:31 [Music]
47:33 so I
47:36 you wanted to actually answer that or
47:39 we can but everybody will find a better
47:43 explanation than I will do now on some
47:48 YouTube channel probably
47:50 so I guess it's a good place to start
47:54 yeah there are a few things I still want
47:57 to ask you so we talked a bit about
47:59 skills and there is actually a comment
48:01 so what you said uh because there is 200
48:05 or even more services in AWS you don't
48:07 need to use all of them and then
48:10 somebody commented that this is the
48:11 Pareto Principle applied to AWS services
48:14 but still like the apart from the
48:16 services that you mentioned like I am
48:18 raw cc2 S3 EMR we also have Docker we
48:23 also have kubernetes we also have cicd
48:25 tools we also have this Primitives
48:27 grafana and I even haven't started uh
48:31 you know mentioning data specific tools
48:33 right these are all like uh General
48:35 software engineering tools right
48:37 generosity devops tools
48:40 so like how to to actually start
48:43 learning you know that do you have any
48:45 suggestions uh like how to like what are
48:50 the minimal operational skills that I
48:52 need to have to be able to work in this
48:54 role
48:56 uh
48:57 if they have to be minimal uh then it
49:00 has to be a really narrow Set uh
49:04 a little bit of context I've read some
49:08 time ago an excellent article uh it was
49:11 ibdf called good enough practices in
49:14 scientific Computing or something like
49:16 that
49:17 when someone went through like all the
49:19 best practices
49:21 uh they advise the best practices or the
49:24 best set of tools to someone and then
49:27 after some weeks checked uh if if that
49:31 list and that training actually changed
49:33 anything
49:34 uh answer was only partially because if
49:37 you introduce someone to like oh all the
49:40 best and in class
49:43 it might be complicated so
49:47 if someone never worked with any Version
49:49 Control System let's start with Dropbox
49:52 honestly if you are still keeping
49:55 passwords in some uh passwords.txt
49:59 uh password manager please
50:01 ubiki maybe but start with password
50:04 manager honestly uh command line like if
50:07 you set up properly the command line
50:10 then
50:11 it will make you work and and your life
50:14 risk is so much so much easier and if
50:16 you would be working then with some
50:19 data Ops devops sorry whoever he or she
50:22 will be also like super happy because
50:24 they will not spend time uh trying to
50:28 figure it out what have you done in your
50:29 command line they will recognize the
50:31 common uh stuff that they already know
50:35 meaning like you are some somehow
50:37 experienced already which will be then
50:40 easier for them to diagnose the real
50:43 problems
50:44 here and there so I would say the
50:47 minimal operational skills for everybody
50:48 whether this will be like data Ops or
50:50 just data analyst or whoever
50:52 um version version control system
50:54 probably get
50:56 a command line
50:58 to some extent it will be enough to just
51:01 know how how to uh how to move between
51:04 the directories how to grab something uh
51:08 how to cut something how to assume the
51:11 role in the Adobe CLI
51:15 um
51:16 pretty much it plus password manager
51:19 plus as I said imbrose which essentially
51:22 means I am uh why I'm like stressing
51:26 that I am parked over and over again
51:28 because like honestly uh 90 of the
51:31 errors are like about access denied
51:34 and being able to run AWS SDS get caller
51:37 identity to know at which role I'm I'm
51:40 currently in
51:41 is super powerful and super simple
51:43 that's the same time uh and just drawing
51:48 the different like okay this will can be
51:51 assumed from that role and that role can
51:53 be from that role uh just writing that
51:57 down on some piece of paper uh creating
52:01 some dots
52:10 I think we'll lost you again
52:16 or maybe it's uh something with me
52:19 I don't know
52:24 I hope it's not me
52:28 yeah I think it's uh Atomic also froze
52:31 on YouTube
52:35 yeah he recovers the connection so
52:53 okay we actually lost Thomas so I hope
52:56 he will be able to rejoin us now
53:01 um
53:02 yeah he's gone indeed so I'm wondering
53:04 how to keep you entertained while he's
53:07 joining
53:10 yeah so Adonis uh you mentioned that it
53:13 sounds like kind of a manager for data
53:16 and this is what exactly this is exactly
53:18 what I was thinking about that it sounds
53:21 very like
53:22 very similar to a management role always
53:25 dedication all this
53:28 calls and zoom uh figuring out what the
53:31 process should look like all the support
53:33 it does look like
53:36 like management
53:39 okay now I am getting I'm starting to
53:42 worry a little bit
53:46 so I hope the connection will be back
53:48 atomic
53:54 okay I think it's the first time it
53:57 actually happens in a stream
53:59 okay you're back
54:02 are you back okay phew I just created a
54:06 hotspot from my telephone sorry for that
54:09 quickly yeah internet life happens
54:13 okay yeah so you were talking about
54:16 um a sumero how I see him roles and then
54:18 how you can just draw in different roles
54:22 and how one can assume another how
54:25 mentally it helps to figure out what the
54:28 problem is
54:29 okay so actually uh all the content uh
54:32 State
54:35 yeah and while you were away I was
54:39 trying to keep people on the call
54:40 entertained and uh so one thing I don't
54:44 mentioned that uh so what we talked
54:48 about largely sounded like a data
54:52 management role so all these Zoom calls
54:54 all this uh support and slack all this
54:57 trying to I don't know life code with
54:59 somebody
55:00 from what I see managers also often do
55:03 it especially with somebody who's let's
55:05 say a senior so they often do it with
55:07 Juniors with maybe middle level people
55:09 like all this kind of work so what is
55:11 what would be the difference what is the
55:14 difference between a data Ops person and
55:16 a data manager person somebody who is
55:18 mentoring people
55:21 that will be quite quite simple to
55:24 explain because usually data data
55:26 managers has their own team like a fixed
55:31 set of like five or six or ten or
55:34 whatever
55:35 [Music]
55:37 of some folks
55:39 and you are the dataops will be working
55:42 like cross
55:44 across different teams across different
55:47 business units
55:49 um whatever yeah so you will be
55:51 observing some slack channel uh like
55:54 data support something
55:57 and that will be pretty much for
55:58 everybody uh you will definitely have
56:02 some splits uh if if you are not not the
56:05 only one doing data in the company so
56:08 someone will take
56:09 the request from WS unit someone will
56:13 take from other business unit but
56:15 essentially you will be working with the
56:17 larger number of people like not only
56:20 the fixed set of like five or six so
56:23 data manager will also go like to 101s
56:26 prepare promotion plans
56:29 um I don't know
56:31 um plan some Sprints uh or stuff like
56:35 that and you will be rather like working
56:37 with multiple different teams but also
56:40 with data management it's definitely yes
56:43 but also maybe data managers at least a
56:46 typical data manager might not have Cube
56:49 CTL installed they might not have
56:51 kubernetes success
56:53 configured right so they might not be
56:55 able to actually
56:57 log into kubernetes and check what are
57:00 the logs there and right what could be
57:03 happening there
57:04 but in a data Ops person will probably
57:09 have these things right
57:11 probably yes but also a fun fact
57:16 kubernetes is not that present in the
57:18 data domain if you are like in the
57:21 software engineering side and you are a
57:23 devops not data Ops definitely
57:25 kubernetes will be your bread and butter
57:28 like uh every single thing will be on
57:31 that that
57:33 platform let's say because what we are
57:35 doing essentially some front-end some
57:37 apis uh which is what kubernetes is suit
57:41 for
57:42 um if you are in the in the data domain
57:43 you get like a ton of batch jobs which
57:47 is not necessarily the first
57:50 use case for for kubernetes it will be
57:53 most probably rather like ECS
57:57 um AWS batch
57:58 or I don't know GitHub
58:01 Jenkins
58:03 thank you you can still run kubernetes
58:06 jobs but uh yeah it's not the first
58:09 choice right so sometimes that I think
58:11 exact services that we can use uh from
58:13 AWS or other Cloud providers that with
58:16 Cube flow obviously you can but it will
58:20 as you said it it won't be the first
58:22 choice
58:25 I guess okay yeah we have a few
58:28 questions so one of the questions is
58:30 what was your most interesting project
58:32 and why
58:35 all right
58:37 well the the one I spent the most time
58:41 on uh was migrating
58:44 a lot of workloads like almost 600 uh
58:49 from like all Jenkins servers uh to
58:52 gitlab CI it wasn't the most interesting
58:56 because of the migration because
58:58 migrations are honestly pretty boring
59:00 what was interesting
59:02 um was working with like pretty much the
59:05 whole company
59:06 at the same time on on one project
59:09 because everybody and their mothers have
59:12 had some
59:13 some
59:15 ETL drops somewhere on some drinking
59:17 instance uh so that was the fun part
59:20 like working with pretty much everybody
59:22 but the most maybe not interesting but
59:25 funny
59:27 um I was with debugging actually
59:31 a series plus contractors plus whoever
59:34 were trying to move
59:38 [Music]
59:41 actually because stream processing is a
59:45 perfect use case for kubernetes okay
59:48 fine
59:50 and I guess the count of people uh who
59:57 are in into that debugging process
59:58 finally reached like nine or ten
1:00:02 and super experienced guys honestly
1:00:06 and they were just okay it's dockerized
1:00:08 we got the helm chart everything is set
1:00:11 service accounts are properly uh done
1:00:15 and deployed yet
1:00:18 um the application just starts and
1:00:20 immediately dies
1:00:21 out of nowhere
1:00:24 then I joined that effort and one of the
1:00:28 first questions I asked was like Hey
1:00:31 guys do we actually know that the
1:00:34 library versions on that ec2 machine
1:00:36 inside that container are actually the
1:00:39 same
1:00:40 as the ones we have there and someone
1:00:44 like was like okay you are a junior in
1:00:46 the operation space you probably don't
1:00:48 talk how Docker actually works yeah yeah
1:00:50 I was like okay maybe I he did don't uh
1:00:54 but then we scanned uh how the docker
1:00:57 file was actually created uh it was
1:01:00 fetching the requirements txt and the
1:01:02 versions weren't
1:01:04 specified there
1:01:07 so out of the sudden uh when we just
1:01:11 packed the library version it was the
1:01:15 pro the problematic one was psychopg the
1:01:18 postgres driver
1:01:21 and the whole fix was like four
1:01:24 characters
1:01:30 relies on a binary like there are some
1:01:33 binary code in Python right that you
1:01:36 don't see any stack traces it just dies
1:01:38 and kills the entire container right
1:01:40 Yeah
1:01:42 the code was prepared to work with I
1:01:45 don't know a version like two seven
1:01:48 something
1:01:49 and without version being like specified
1:01:53 then if
1:01:55 um when when folks try to run the that
1:01:59 stuff on kubernetes it fetched the
1:02:01 latest
1:02:02 and the API changed which is the perfect
1:02:06 use case I'm I'm telling everybody uh
1:02:09 when they are asking questions like why
1:02:11 we have to use like fixed versions
1:02:13 because it's boring
1:02:15 why we just just can't use like any
1:02:17 version like latest
1:02:19 that's that's exactly why
1:02:22 that's a that's a good story so do you
1:02:25 have a couple of more minutes
1:02:27 true
1:02:28 okay so last question for the way at the
1:02:32 beginning you told us a story when you
1:02:36 worked in analytics and somebody the
1:02:39 management asked you how confident are
1:02:41 you in the results so how do you usually
1:02:44 answer this question
1:02:46 foreign
1:02:50 that I'm as comfy as we get money to
1:02:53 check all the uh the edge cases
1:02:57 because uh the same the same manager who
1:03:01 asked me uh that's the question
1:03:03 uh was the same who like healed me
1:03:06 totally from checking every single
1:03:09 possible
1:03:10 um Edge case and like error because if
1:03:13 you are working in like a basic research
1:03:15 and by basic I I mean rather
1:03:19 that's the name but it implies that it
1:03:22 might be simple it's not it's like the
1:03:24 fundamental research in the Academia
1:03:26 then you basically are receiving the CSV
1:03:30 file and and that's it if you are
1:03:32 working in the company data is Flowing
1:03:33 constantly so tracking changes policy
1:03:36 changes whatever changes uh pipeline
1:03:38 changes schema changes like everything
1:03:40 changes constantly and the failure is
1:03:42 the only constant
1:03:44 like as Werner of August said everything
1:03:46 fails all the time
1:03:48 and vogels is the CTO I guess currently
1:03:51 also of AWS
1:03:54 so he probably knows what he was talking
1:03:56 about uh so
1:04:00 I remember when I was checking some some
1:04:03 click stream
1:04:04 um data and I was looking for errors
1:04:06 like before like presenting uh the
1:04:10 results maybe let's check if uh all
1:04:13 things are like set properly
1:04:16 I was like doing that my manager came in
1:04:20 and like okay are you done like
1:04:24 almost uh still I got some some like
1:04:27 edge cases in in 10 000 records and he
1:04:32 asked like out of how many
1:04:34 I was like 400 million
1:04:38 he's like okay so we were checking like
1:04:40 10 000 records out of 400 million uh and
1:04:44 they were spending time on that
1:04:45 congratulations
1:04:47 uh so like okay that's just it won't be
1:04:50 perfect like
1:04:52 never
1:04:54 but definitely is a wise idea to
1:04:59 uh for instance if we are working with
1:05:02 with airflow
1:05:05 uh and you are seeing that your your
1:05:07 pipelines are like all green and so on
1:05:09 so uh what does it actually mean that
1:05:12 the records were inserted uh in into the
1:05:14 given table or that's the network didn't
1:05:17 fail as it did for me today
1:05:19 uh the answer might be sometimes
1:05:22 surprising that's okay uh Zero Records
1:05:25 inserted drop is still green
1:05:27 so that's kind of stuff might be checked
1:05:30 before presenting uh some
1:05:33 extraordinary results to leadership
1:05:35 because extraordinary claims require
1:05:37 require extraordinary proof
1:05:40 that actually happened to me
1:05:43 green drops an airflow with zero record
1:05:45 sensors it
1:05:47 I know that I guess everyone had to
1:05:50 experience this
1:05:53 okay
1:05:54 um
1:05:54 do you like before we finish now it's
1:05:57 time to to wrap up before we finish to
1:05:59 do you want to maybe you forgot to
1:06:01 mention something and you want to bring
1:06:03 it up
1:06:05 um
1:06:06 paraphrasing what you said at the very
1:06:09 beginning of our talk like uh only like
1:06:13 one half of the people who are who are
1:06:16 watching are also subscribing so uh if
1:06:19 you are in that group in that particular
1:06:22 group like then don't forget to smash
1:06:24 that like button and subscribe
1:06:28 always wanted to say that honestly
1:06:32 uh
1:06:33 and yeah and well that was that was in
1:06:37 like super super super pleasure to be
1:06:39 here and thank thank you for invitation
1:06:41 and thank you like all uh for being here
1:06:45 with us in that Friday evening yeah plus
1:06:50 one two everything you said
1:06:52 yeah so we should be finishing so thanks
1:06:55 tomek for joining us today thanks
1:06:57 everyone for joining us today too for
1:06:59 asking questions and I wish everyone to
1:07:03 have a great weekend
1:07:05 thank you bye