0:00 Hi everyone, welcome to our event. This
0:02 event is brought to you by data talks
0:03 club which is a community of people who
0:05 love data. We have weekly events and
0:07 today is one of such events. You can
0:09 check out more events that we have in
0:12 our pipeline. Um there's a link in the
0:13 description. Go there, check it out. Um
0:16 already today we have one more event
0:18 which is about uh a launch of our
0:21 hackathon that we have for the summer.
0:23 So if you're interested in this kind of
0:25 stuff and if you want to build something
0:27 with AI, um check it out. Of course, do
0:31 not forget to subscribe our YouTube
0:32 channel. This way you'll be notified
0:35 about all the future streams we will
0:37 have. Um and last but not least, we have
0:40 an amazing community uh Slack community
0:43 where you can hang out with other data
0:45 enthusiasts.
0:46 During today's interview, you can ask
0:48 any question you want. There is a pinned
0:50 link in the live chat. Click on that
0:52 link, ask your questions and we will be
0:54 covering these questions during the
0:56 interview. And right now I'm going to
0:58 stop sharing my screen.
1:01 Um
1:03 yeah. Um another thing is uh like this
1:06 was supposed to happen actually last
1:08 Monday, not this Monday. So thanks Leor
1:10 for um accommodating. I was uh sick so I
1:15 just slept all Monday. Now I'm back to
1:18 normal. So actually I run yesterday. So
1:21 I'm fully functioning.
1:22 >> Good.
1:23 >> Yeah. Uh so um where
1:27 >> I'm preparing for a marathon. So I run a
1:30 lot.
1:32 >> So no, forget about me. My max is 9K and
1:36 this is more than enough for me. I think
1:37 sometimes.
1:39 >> Yeah, like for me it was a little
1:41 yesterday was 30.
1:44 It was exhausting. But anyways, like I
1:47 don't know why I signed up for that.
1:51 You have to do it at least once in in
1:52 the life I think.
1:53 >> Yeah. So I signed up for the lottery. So
1:56 there you have a lottery. And I was kind
1:57 of hoping I will not win but then at the
2:00 end I was selected.
2:04 >> Good.
2:04 >> I have to prepare.
2:06 Anyways, uh let's start. Um so this week
2:09 we'll talk about mindful data strategy
2:11 and how teams can shift from building
2:13 pipelines to delivering real business
2:16 outcomes.
2:17 and we will look at how principles from
2:20 the Zen philosophy can help teams work
2:22 more intentionally and effectively. We
2:24 have a special guest today, Leor. Uh
2:26 this is not the first time we have a
2:28 conversation. Last time we spoke a few
2:31 years ago, more than two and a half. Uh
2:35 primarily about humus if I remember. Uh
2:37 I don't remember other things. Funny
2:40 thing is today I ate hummus. The one
2:42 from Revit probably you will not
2:44 approve. I don't know what's your
2:46 opinion about humus.
2:48 >> I'm already disappointed
2:50 >> but this is at least some humus like
2:53 it's better than the humus, right?
2:55 >> Yeah.
2:57 >> So Leor is a data strategy consultant,
3:00 former practitioner. Why former though?
3:02 Maybe you'll tell us. And the author of
3:05 data is like a plate of humus which is
3:08 why we talked primarily about humus last
3:10 time. uh he developed the data existing
3:12 vision board and the data product life
3:14 cycle manager frameworks designed to
3:16 help teams move from re reactive tasks
3:19 to more strategic lo long-term thinking.
3:22 Welcome.
3:24 >> Hi, happy to be here. I think the last
3:26 time we also discussed about something
3:28 very similar to this topic about
3:30 efficiency in uh teams uh structure. So
3:34 how to make sure communication is
3:36 working well if I remember correctly.
3:38 >> Yeah. Uh I think when uh I was um doing
3:43 that interview I was actually a
3:45 practicing manager. Um so let's see how
3:48 much of the things we talk about today
3:51 will resonate with me and uh let's see
3:53 if I actually was able to follow your
3:55 advice. Um but let's start with your uh
3:58 career journey so far. Um can you tell
4:01 us more about that?
4:03 Yeah. So, uh in the past year I'm more
4:06 working with
4:09 smallcale to mediumsiz companies trying
4:12 to basically write down my second book.
4:14 So when we spoke the last time we spoke
4:16 about my first one which was data is
4:18 like a plopumus h and I'm working on a
4:22 new book which is more taking the
4:26 strategic aspect of data. Uh and I think
4:29 that this is where I am today. Before
4:32 that I was working at Idialo. I was the
4:34 head of product for the data platform
4:37 team. Erh just before that I was
4:41 basically launching my own company tele
4:43 data which the entire idea behind it was
4:45 create the infrastructure and the tables
4:48 in an automatic way which will save a
4:51 lot of cost for small companies that
4:52 don't have the scales and the ability to
4:54 hire the engineers.
4:56 uh and before that I was working for
4:57 Zelando leading there basically the
5:00 entire marketing data operation uh from
5:04 the product perspective. So setting up
5:06 basically the infrastructure first time
5:08 ever to have an infrastructure for data
5:10 instead of having a horrible Excel sheet
5:12 and pythons that breaking every week
5:14 that you need to run them. And then we
5:17 moved to ROI forecasting creating
5:19 basically a better understanding of data
5:20 and then ended up in the marketing
5:23 automation part of how do you actually
5:25 steer the budget of the campaigns and
5:27 making the best decisions. So this is
5:28 more or less my story path and today I'm
5:33 practicing a lot of mindfulness in data
5:35 and I think uh we're going to have a
5:37 very interesting conversation today.
5:40 >> Mindfulness of data.
5:42 >> Yes.
5:43 >> Okay. That's interesting. So you're more
5:45 like a product person if I got from
5:48 yourself description.
5:50 >> I I am I started my career actually as
5:54 an engineer. I wasn't as good and I
5:57 wasn't as fast as other engineers. I
6:00 must to admit it. I moved more towards
6:02 the analytic side and then I found
6:04 myself somehow in product
6:07 something like seven eight years ago and
6:09 since then I'm I'm staying in the
6:11 product area. I'm very technical. So if
6:13 you're going to ask me questions about
6:14 tools I will be able to evaluate them
6:15 architecture as well but my passion is
6:18 really to bridge the gap between the
6:21 business and the tech I think and and
6:23 actually bringing everybody together.
6:25 >> This is not one of the questions we have
6:28 prepared for you in advance but I
6:30 recently saw a question in one of the
6:31 data science communities and the
6:34 question was from an engineer. Uh he was
6:37 asking uh what he was asking the
6:41 community to recommend him some
6:42 resources about product management if so
6:45 do you have any suggestions for somebody
6:46 who works as an engineer or data
6:49 scientist if they want to learn about um
6:53 product management and the product side
6:54 of things do you have any
6:56 recommendations for them
6:58 >> I will be honest uh there is the product
7:00 camp Berlin that is once a year if any
7:03 if somebody's interesting and I know
7:04 they're doing it a tour all over the
7:06 world as well. Uh but there's nothing
7:09 something specific for data path
7:12 dictionaries because I think this is
7:13 very different than the regular product.
7:15 If you're learning product, it's a lot
7:16 about users and communication which is
7:18 very important for us to be able to
7:20 know. Uh but there is nothing specific
7:22 yet that I'm aware of but I would love
7:24 to learn if somebody finds something as
7:26 well.
7:28 >> Product camp is this the people cuz I I
7:32 remember attending a workshop when I was
7:34 still at Elix. I think it was mind the
7:36 product something like that.
7:38 >> This is also another one. There is also
7:40 buzzwords
7:43 that is also happening somewhere in June
7:45 I think usually that also connecting
7:48 tech and data together.
7:50 >> So there is different uh different ways
7:52 to learn it but for product specifically
7:54 for data product I haven't encounter
7:56 anything.
7:57 >> Mhm.
7:58 >> Maybe this is something I can do next.
8:00 >> Yeah. after this uh the current book
8:03 right which I what what is the title do
8:06 you have the the title already
8:08 >> I I have a a temporary one for now it's
8:11 called wabishabi your data uh
8:14 >> what what can you say it again slower
8:16 >> wabi sabi your data which is basically
8:20 >> but uh
8:22 >> it's a it's a Japanese concept about
8:24 basically uh accepting the imperfections
8:29 uh so the perfect imperfections
8:31 basically and this is basically what I
8:33 think about data. This is a lot of my
8:35 philosophy about
8:37 how to handle data. It's imperfect and
8:39 we need to accept it and we should not
8:42 fight always to have it perfect before
8:44 we going forward. And I think this is
8:47 one of the biggest issues that we we
8:48 going through today.
8:50 >> I see where the mindfulness um part
8:53 comes from
8:55 >> the acceptance, right? So don't fight
8:58 it, accept it, right? be aware of it,
9:00 accept it and communicate it. I think
9:02 this is if if we need to take a core out
9:04 of this uh conversation today is be
9:07 aware, accept it. This is the reality
9:10 and make sure you're communicating it
9:12 correctly. Uh and I think that this is
9:14 my biggest learning from leading a lot
9:19 of data initiatives and failing many
9:21 times as well. And I yeah I I think that
9:26 we always expecting to have this perfect
9:28 product. We always in the hunt for the
9:30 perfect product, the next technology,
9:32 the next thing, the next always
9:33 something next. And actually we should
9:36 sometimes stop and accept the reality.
9:40 >> So how do we do this?
9:42 >> Oh, that's a great way. So when we
9:45 starting and we're talking about it, uh
9:48 le let's talk about the trust crisis,
9:50 right? So
9:51 >> I think that there are there is a very
9:54 big trust crisis right now in the
9:56 industry. I saw a research a few weeks
9:58 ago that 97% of the CEOs mentioned they
10:03 using data but only 24% said that they
10:07 are data driven. This is already saying
10:10 something to us about the difference in
10:12 how data is being used in the
10:15 organization and how it's being
10:16 leveraged. And there is a data trust. We
10:18 we launching products uh yes we have
10:21 data governance yes we have monitoring
10:23 yes we have a lot of tools around it to
10:27 make sure the data arrives uh to the
10:29 users safely but in the reality the data
10:33 or or the people using this data don't
10:36 trust it and they basically this is
10:40 where the crisis starts and this why we
10:43 need to be aware of it. We need to aware
10:45 of where the flows are. So data will
10:47 never be perfect. It's enough that
10:48 somebody changed one tab in the data
10:51 that being ingested into the data uh
10:53 warehouse and everything can break and
10:56 we need to learn here how to communicate
10:58 it better.
11:00 >> And the fact is that we looking at many
11:05 many issues right now that
11:08 related to the trust of people in the
11:10 data. I don't trust the data team
11:12 because uh they in constant migration. I
11:15 don't know. I have I have a CEO I spoke
11:17 to now and one of the things that still
11:20 resonated with me was still stuck in my
11:22 head is like he has the data team in the
11:25 past three years running from migration
11:27 to migration. Each each migration taking
11:29 them six to eight months everything
11:31 stuck and this is again this is a loss
11:33 of trust in data team and we sometimes
11:35 we are not even aware of it. we don't
11:37 even see the issue because we trying to
11:39 achieve a goal but during the attempt to
11:42 achieve it we actually missing some of
11:44 the components that need to be there.
11:47 Funny thing when you mentioned the the
11:49 trust crisis um what came to my mind was
11:52 um I have a son he's 9 years old and he
11:56 likes to use chip he speaks with charity
11:59 a lot and chip as we all know
12:02 hallucinates sometimes it comes up with
12:05 things that does not don't do not exist
12:09 um so he was like why should I we pay
12:12 the so cuz I have a premium subscription
12:15 he was like we pay this tool, we pay
12:17 them €20 or something like that. Uh, and
12:23 like it outputs garbage. It outputs
12:25 nonsense. How can we trust how we can
12:27 ever trust what it says if sometimes it
12:30 hallucinates? But then I say it's u my
12:34 objection to that was it's actually a
12:37 very useful tool cuz most of the time
12:39 it's correct, sometimes it's not. You
12:41 just need to how to say cook it. You you
12:44 like cook metaphors, right? just need to
12:46 cook it correctly. Uh and we just do
12:50 accept the fact that sometimes it
12:53 hallucinates, sometimes it does not
12:56 provide the correct answer. So we just
12:59 need we don't
13:02 we must not blindly trust whatever it
13:04 says. We need to always double check.
13:06 But overall it's quite a useful tool and
13:09 we need to we need if we use it, it is
13:13 super convenient. It makes our life
13:14 easier. We just need to be mindful of
13:17 the fact that sometimes it may be super
13:19 wrong,
13:21 >> right? But it does not mean we shouldn't
13:22 stop. We should should not continue
13:24 using this tool, right? Cuz it's super
13:26 useful. So I was explaining I was also
13:28 trying to explain how this uh the model
13:31 under the underlying model works that
13:33 it's a language model. All it does is
13:36 trying to predict the next word and then
13:39 people noticed with time that it's
13:41 actually something useful. How about we
13:42 start using it? because most of the time
13:45 the output is good and sometimes it's
13:47 not. So I think it's maybe a very
13:50 interesting uh example where there's a
13:53 task crisis because the tool is useful
13:55 but then it was wrong a few times and
13:57 then okay it's useless like we should
13:58 stop using it while it's not the case
14:01 like it's a useful tool we just need to
14:03 be accepting of the fact that it might
14:06 be wrong sometimes
14:07 >> correct and you know I always explain it
14:09 and I say also data is like a Lego
14:12 bricks okay we can connect and we can
14:14 build a lot of buildings inside it
14:16 sometimes we falling on fake Lego brick
14:19 that we bought from not Lego themselves
14:21 but some other generic brand that looks
14:24 like
14:24 >> guilty of that
14:26 >> and it's not connecting it's not working
14:28 well right it's not sitting there as it
14:29 should
14:30 >> and that's exactly the problem with data
14:32 right so we're building a castle out of
14:34 Lego bricks and sometimes some of these
14:36 bricks
14:37 >> not going to be the original ones and
14:39 this is something we're going to fill in
14:40 the rest of the construction and it
14:42 happens we cannot change it right and
14:45 maybe we even need this brick because we
14:47 don't have enough original bricks
14:49 >> because we want to build a very big uh
14:50 building. But we need to find the right
14:52 balance on between how much we're
14:55 willing to trust and invest in having
14:57 the bigger picture. So this huge castle
15:00 or how much we saying okay now we are
15:02 stuck and we're waiting until we find a
15:03 replacement for this brick which can
15:05 take now another week until we're going
15:07 to order it until we're going to receive
15:08 it and until we can open the package. So
15:11 this is something that we need to be
15:13 aware of.
15:14 >> Yeah. Have it has it actually happened
15:16 to you like the Lego Lego brick thing?
15:21 >> Oh yes. My kids have huge box and I they
15:24 had one that was supposed to be Lego but
15:26 it wasn't Lego.
15:28 >> Uhhuh.
15:28 >> Then it didn't connect and they were
15:30 sitting there frustrated trying to
15:31 connect it to something.
15:33 >> Okay. Cuz what happened to me, I don't
15:36 know if you're into Lego. I uh built uh
15:40 I had this Millennium Falcon, which is
15:42 like one of the biggest um sets, Lego
15:45 sets you have. And then a part was
15:47 missing. So I had a choice whether to go
15:50 to a store and buy not an original one
15:53 or wait for two weeks till they sent the
15:56 replacement. So I ordered the
15:57 replacement cuz it was free. Uh but then
16:00 I went to a store and then just bought a
16:03 part. The part was not uh actually
16:06 fitting. It didn't fit. So I had to use
16:08 my like a a saw like I I had to like
16:12 manually adjust it to make it fit but
16:15 then it went at the end it worked. So by
16:17 the time when the part arrived I
16:19 actually had the entire set assembled.
16:23 >> That's cool.
16:24 >> Yeah.
16:24 >> So it didn't but like with enough
16:26 perseverance uh yeah I managed to make
16:28 it work.
16:29 >> I will take it I will keep it for the
16:31 next time maybe for the next crisis.
16:34 >> Mhm. But again this is the same as data
16:36 right when you think about it. Yeah,
16:39 >> we sometimes we manipulating the data so
16:41 heavily so it's going to fit to the
16:42 pattern we need it
16:43 >> just we can continue the building and
16:45 then somebody comes like but hold on the
16:48 color is not the same I can notice it
16:50 and this is not the original brick
16:52 >> and then what do we do with it and I
16:54 think that this is really we go a lot of
16:57 this direction of at the end of the day
16:59 we trust it we want this construction to
17:02 sustain what we want to do with it
17:04 >> and we need it to look good at the end
17:06 of the day as well. Mhm. So what do we
17:09 do with this CR trust crisis? So in my
17:12 case, I try to explain my son that this
17:14 tool is useful. We just need to be
17:16 mindful of its limitations.
17:18 Do you also suggest the same like when
17:20 you speak with the CEOs that perhaps
17:23 have this trust issues with the data?
17:26 How do you convince them that um we
17:29 still need to rely on data? I always say
17:32 it's like you know a lot of times when I
17:35 arriving to companies it's like this
17:37 basketball team that everybody lost hope
17:39 about and they keep losing. They know
17:42 they have the capabilities and the right
17:44 people to actually pull the season and
17:46 make it successful. The owners are
17:48 already frustrated. The fans are
17:50 frustrated on them and they were and and
17:53 they're just in this slow moment. And I
17:55 think that what we need to do is much
17:58 better communication. So what I realized
18:00 is that the default of a lot of us is to
18:02 go into tools. Okay. So now we need to
18:04 have a monitoring tool. Now we need to
18:06 have a new governance system to do the
18:10 catalog and the lineage and so on and so
18:11 on. And we keep extending tools instead
18:13 of actually stopping for a second
18:16 stepping backwards and this where we go
18:18 into this mindfulness right and reflect
18:20 on why we have the crisis issues of of
18:24 losing trust. What why what are the main
18:27 drivers of it? Is it specific products?
18:29 Is it specific things? How do we
18:30 actually
18:32 creating the right environment for us to
18:35 reflect and understand? And I think that
18:37 this is something that we are missing a
18:39 lot. And this is also something that
18:42 going to help the CEOs to understand
18:43 where the problems are because when we
18:45 coming and saying okay we need now a new
18:48 tool most of the CEOs freaking out
18:51 already. It's like why they need another
18:52 tool now? I'm already paying 500,000,
18:55 one million, two million dollars on on
18:58 the data team and now adding another
19:00 tool to it. It's another 100,000 or
19:01 200,000 on overheads.
19:04 What is actually the end goal of it? How
19:06 do we actually create something? And I
19:09 think again this is here coming to why
19:12 we creating this crisis because we
19:13 didn't don't communicate clearly. We
19:15 don't really prioritize and clarify what
19:18 is important, what not.
19:20 uh and we don't talk the language of
19:24 impact. So if we saying and we think
19:26 that a data quality tool will be
19:29 important and going to help us then why
19:33 and how it's going to basically impact
19:34 the business goals. It's going to
19:36 improve the stability of the marketing
19:38 data. So they will be able to make
19:40 better decisions and steer the budget.
19:42 By that we're going to save between 20
19:44 to 50,000 euros a month on marketing
19:48 campaigns that basic basically wasting
19:49 our money. We already talking a monetary
19:52 value. We're talking we're having a
19:53 different conversation and now people
19:56 also looking at it now as not as a trust
19:59 issue but more as communication a gap.
20:02 And now let's see how we can figure this
20:04 one out.
20:05 We should get out of the crisis. We
20:07 should get out of the chos. We should
20:08 ride the chaos and flow with it. accept
20:11 it and continue there.
20:13 >> To me it sounds a bit um like a little
20:15 abstract. Do you have um some maybe
20:18 example to make it more concrete? Let's
20:20 say we have this trust crisis but then
20:23 you were saying that yeah we don't
20:25 necessarily need another tool. Um we
20:28 need to think what is important, what is
20:30 not important. We need to take a step
20:31 backward backwards. Do you have an
20:34 example of
20:36 a situation
20:37 where
20:40 how to say like I'm I'm just trying to
20:43 visualize this um and having a concrete
20:45 example will help to understand it
20:47 better.
20:48 >> Think think about let's say the core KPI
20:50 dashboard for the management they need
20:53 to have it on a daily base to understand
20:54 how the operation of the business goes.
20:57 Let's say if this is a company an
20:59 e-commerce company that I don't know is
21:01 selling shoes that could be the number
21:03 of sales
21:05 for example not related to the other
21:08 company
21:09 >> not related to any companies
21:13 but the core the core the core principle
21:15 here is this this dashboard exists in
21:17 there right and this the management
21:19 needs to use it and when there is lack
21:21 of trust what happen in many of this
21:23 organization is that the CEO goes to the
21:24 CFO and ask him are the numbers risk
21:26 rate. Can I trust them? Then the the CFO
21:29 goes to his analyst.
21:31 >> Sorry for interrupting you. The numbers
21:32 >> revenues,
21:33 >> revenues, sales,
21:36 >> stock stock availability and so on and
21:38 so on. So all of these things and the co
21:41 does not have the time to start and
21:43 supervise it, right? And this is why
21:44 this crisis starts because if he needs
21:47 to check or they need to check every day
21:49 basically are the numbers correct yes or
21:52 no can I actually look at them and and
21:54 make decisions based on them or should I
21:57 second guess myself and this is this is
22:00 the main case right so we have this core
22:02 KPI we want to make sure that it's it's
22:04 getting uh corrected and arriving right
22:07 to the to the owners of it or to the
22:09 users of it to the data consumers
22:11 and what we need to check here is like
22:14 what will make it better if we're
22:17 investing in an extra tool if we
22:20 checking actually where the issues are
22:22 in the pipeline in the lineage part of
22:24 it. Is it in the beginning of the
22:26 pipeline? Is it at the end of the
22:27 pipeline? Is it in the middle? So
22:28 something in our SQL code is broken when
22:31 we computing the data into the dashboard
22:34 is it something that happens in the
22:35 ingestion because one of the teams is
22:37 not ingesting the data correctly.
22:39 And we need to start to be much more
22:41 aware of these patterns because the
22:43 solution of a tool is the easy one but
22:46 it's going to take us now into
22:47 implementation process of six to eight
22:48 months. While it could be that we're
22:51 going to identify that our uh product
22:53 team is ingesting the wrong data on a
22:55 daily basis and each day they're
22:57 changing something in the data structure
22:59 which basically screwing the rest of the
23:01 report going upstream right towards the
23:03 the the dashboards.
23:06 And this is when we're talking about
23:07 data trust. This is how it's happening
23:09 most of cases. It's not because the tool
23:11 is not functioning because some
23:13 components in the process are failing
23:16 and we are not aware enough on how to
23:19 fix it or or what to do there and we
23:21 thinking as a default to go to a
23:23 software. Maybe we rebuild the pipeline.
23:26 Maybe we going to bring now a monitoring
23:28 tool on the ingestion data. And this is
23:33 not this is the easiest solution in my
23:35 eyes. this is not the core solution of
23:38 building the trust because still the CEO
23:40 when you're going to arrive in the
23:41 morning and going to look at this
23:42 dashboard
23:44 he will not trust it because he already
23:46 had a bad experience of trusting it and
23:48 figuring out it's not trustworthy
23:50 >> and then we need
23:52 >> why why this question in the first place
23:53 why the CEO asks the CFO if the numbers
23:58 are correct because the numbers look
24:00 strange or because there were some
24:02 problems in the past where the numbers
24:05 turned out to be not correct.
24:08 >> Oh, there are many cases. So there is
24:09 enough that one of the teams came with
24:11 their own numbers. So marketing
24:12 department were presenting different
24:14 numbers than what is on the main
24:15 dashboard or the core KPI dashboard
24:18 >> and you already created the crisis.
24:19 Right? This is one case. The other one
24:21 is that the CEO used the numbers and
24:23 figure out that they were wrong because
24:25 somebody notified him after a month that
24:26 the numbers he showed to the investors
24:29 were not connected to the reality and
24:30 actually it was completely fake story
24:32 what he was telling. There is a lot of
24:35 cases and I encountered a lot of them
24:36 and and I think that this is what a lot
24:38 of us as data people we sometimes we we
24:42 we tend to see it and understand it but
24:44 we don't act fast enough
24:46 >> on how to solve it and how to make
24:48 people feel more trustworthy with with
24:50 what we're serving them.
24:51 >> So the situation is the trust is already
24:54 lost the CFO the CEO is already um
24:57 doesn't believe that the data is correct
25:00 right and we need to to have something
25:02 to to do something Yeah,
25:04 >> let me ask you that somebody give you
25:06 now a dashboard about your podcast
25:09 listeners.
25:09 >> Yeah.
25:10 >> Will you automatically believe it or
25:12 will you go validate the data and make
25:13 sure the data is correct?
25:15 >> Um, so when I see this in Spotify, I
25:18 auto I automatically believe it cuz
25:22 there is no other way for me to get this
25:24 data. I have the only data source for
25:26 this. So like uh I having a number is
25:30 better than having no number. So I just
25:32 automatically believe the Spotify data.
25:35 >> This is the easy solution, the pragmatic
25:38 way of doing it, right? But if you
25:39 running a business,
25:41 >> you want to make sure that the numbers
25:42 you have actually can create the right
25:45 impact. So when you're going, for
25:46 example,
25:47 >> let's take the podcast. When you're
25:48 going to advertisers, you want to give
25:50 them numbers that going to make them
25:52 say, "Oh, wow. It's worth for me to
25:53 invest in this. It's worth for me to
25:55 actually put money on advertisement."
25:58 >> And I think that this is this is the
26:00 important part. And this is also for the
26:02 CEO because when he goes to his
26:03 investors to the people who put the
26:04 money,
26:05 >> he needs actually to explain to them yes
26:07 the business is working well and is
26:09 functioning in the right direction. So
26:11 don't cut your investment because even
26:13 if you got promised for I don't know 80
26:15 million investment
26:17 >> you're getting it in chunks right based
26:18 on milestones that you achieve.
26:21 >> Mhm. And here is exactly the point of
26:24 where you need to be able to say the
26:26 right numbers or or believe the numbers
26:28 that you are saying to people because
26:30 otherwise you're also going maybe to the
26:32 line of fraud
26:34 if you if you're not reporting the right
26:36 numbers. So there is a lot of small
26:38 nuances and this why the CEO at the end
26:40 of the day will ask always the CFO to
26:41 validate the numbers and see this is
26:43 matching what we actually have in the in
26:45 the bank or not.
26:46 >> Okay. Okay. Because otherwise auditors
26:49 come and then they close the company,
26:52 right?
26:54 >> Yeah. It's can end up very bad. So there
26:56 is even a case now I think in the US of
26:59 a CEO that was reporting sales numbers
27:02 uh that were 10 times more than what
27:04 they actually were selling. It was a an
27:08 issue in the database. They discovered
27:09 it at some point that somebody was just
27:12 multiplying it by 9 comma something. I
27:15 don't remember already how much it was.
27:17 uh and he kept reporting on it because
27:19 even after they discovered it, he said I
27:22 cannot go back now with the numbers that
27:23 I reported.
27:25 >> Mhm. Okay. So this
27:28 knew about the mistake and continued to
27:31 report inaccurate data
27:33 >> because he didn't have a solution for
27:35 this but this is one case which is very
27:37 bad and we should not go this direction.
27:39 But when the CEO discovers now that his
27:42 data was multiply by 9.7 or by 10 even
27:46 he will need to go to the investors and
27:48 explain it to them and he needs also to
27:49 be feel secure enough to explain why we
27:52 didn't find this error before. What are
27:55 we doing now to avoid this error going
27:57 forward and how we actually making sure
27:59 that uh we're reporting the real numbers
28:02 going forward and this also may be some
28:04 application on the investment for the
28:07 future.
28:08 >> Okay. So how do we regain trust?
28:12 >> How do we regain the trust? So it's best
28:14 of all uh one of the things that I'm
28:17 always saying is by having a very
28:19 mindful approach to to going and dealing
28:22 with data. It's to understand basically
28:24 have a much better control on what we
28:26 are doing as I see it. Uh it's going
28:29 towards three main components that we're
28:32 doing dayto-day. We're doing
28:34 maintenance, we're doing rollout of
28:36 products and we're doing innovation of
28:38 products. And in each of them basically
28:40 we need to think about the end user. So
28:43 what is the who is the data consumer and
28:45 what they need to receive? What is the
28:48 most uh easy way for us to bring them
28:50 the first uh reports or first data to
28:52 already look at and understand if this
28:54 what they need or not. So what is the
28:56 MVP looks like for us? uh and then it's
29:00 basically to work together with these
29:02 users long term understanding basically
29:06 during the maintenance time where are
29:08 the issues coming from how often is it
29:11 happening what are the root causes of
29:13 this and having a much deeper
29:14 understanding so we're doing data
29:16 incidents right many organ many many
29:18 teams have data incidents today and how
29:20 to handle basically when something is
29:22 breaking
29:23 but how many organizations actually
29:25 taking these data incidents and putting
29:27 them into a huge
29:28 a a a machine that tells them, okay, we
29:33 identify that these three core products
29:36 producing 60% of your incidents or 70%
29:39 of your incidents or even 40% of your
29:41 incidents and once we discover this,
29:44 what are the root causes of these
29:46 incidents? So, is it something that keep
29:49 repeating? And then you are easier to
29:51 actually solve these issues because you
29:53 know where it happens. You're aware how
29:55 often it happens and you can communicate
29:57 to everybody. Look what are you doing?
29:59 60% of the issues are coming from your
30:01 team and this is the impact of them.
30:03 This is what you're actually losing by
30:05 not fixing it or not correcting it for
30:08 once and for all. And this why we need
30:10 your investment and this is something
30:11 that then easier to align a the entire
30:13 company or not because you're creating a
30:15 a shared reality. these numbers this is
30:17 not any longer I feel that uh the
30:21 dashboard is being broken because of you
30:23 it's because you you are the root cause
30:26 in the last 15 incidents for the data
30:29 not arriving correctly to the CEO so
30:31 let's figure out now how we can solve it
30:33 because he is not having the data means
30:35 that we may lose the next investment of
30:37 the 20 million euros that we supposed to
30:39 have to run this company
30:41 >> this is already numbers that actually
30:43 going to make people start working with
30:45 you This is what one part. The second
30:47 part is to solve it for the CEO at the
30:50 end of the day is to receive a dashboard
30:52 or even an easy easy dot on the top of
30:55 his dashboard which is with colors
30:58 green, yellow, red. When it's green, you
31:01 can trust the data. Everything is great.
31:02 We haven't identified anything that has
31:05 an issues. When it's yellow, please wait
31:07 for us. We you can use the data, but be
31:10 aware that it may have an issue. And
31:12 when it's red, everything is broken.
31:14 Don't touch it. don't trust the data.
31:17 >> This is already this is already moving
31:20 the communication from
31:22 he asking you if the data is correct to
31:25 telling him the data is is is not
31:27 correct right now or to to to tell them
31:29 >> hold wait for me to come and I will
31:31 explain to you what's going on. Mhm. So
31:35 let's say if we sell shoes and we have
31:38 different apps like for Android, for
31:41 iOS, for web and then whatn not, I don't
31:44 know.
31:46 Um, postmails. Um, so then all of a
31:49 sudden we realize that our tracking on
31:52 iOS is broken
31:54 cuz like we report more views than there
31:57 are. So what we do is go we go to the
31:59 dashboard and flip, we go from green to
32:02 yellow, right? And then when the CEO
32:05 looks and sees that they see he or she
32:10 um that's it's yellow and automatically
32:13 they know that they don't need to they
32:16 cannot fully trust the data. They can
32:18 still make some like uh some decisions
32:22 but there are some problems that are
32:23 being fixed and they know it. Right.
32:26 >> Correct. So this is a very easy solution
32:28 to give them the ability to actually
32:31 realize where the issues are and we're
32:32 flipping the conversation and once we
32:34 actually telling them listen the status
32:36 is yellow they know that something is
32:39 wrong and we are looking at it.
32:42 >> Mhm.
32:43 >> Because we also see that it's yellow and
32:45 this is a communication that have been
32:48 done correctly. Now we can give an
32:50 update after we observed and we checked
32:52 if actually the suspicious was right or
32:55 wrong.
32:56 >> Mhm. And if it's was wrong, we can turn
32:59 it into green. We can basically
33:00 overwrite the yellow, move it to green.
33:03 If we still suspecting it's wrong, we
33:06 can move it to red and then figure out
33:07 until we fix it. Giving them the
33:09 estimation of the fix, what we have
33:12 identified and now how we fixing it. I
33:14 think that a lot of the things that I
33:16 realizing the more I talk to people, the
33:18 more I talk to data teams in the past
33:20 year and a half, the issue is not with
33:24 the technology itself. It's not with
33:26 having a DBT or Presto or Trino to
33:29 process your data. It's not about having
33:32 Snowflake or Snowplow or or Tableau or
33:35 Locer. It's about the communication and
33:38 this is our biggest gap right now.
33:40 >> We are communicating it completely
33:43 wrong. We allowing many times to the
33:46 teams that using the data to find the
33:49 errors in the data because we keep
33:51 saying they know best
33:53 their data. So if something is broken,
33:56 they will come to us. But what is it
33:58 actually causing them?
34:00 >> They don't trust the data. They keep
34:02 checking it. They investing time in
34:04 testing the data. Think about in average
34:07 I saw teams that investing between two
34:09 to three hours on validating data on a
34:11 daily base. This is instead of them
34:14 working on optimizing campaign or
34:16 checking what next on the feature list
34:19 for release instead of it is sitting
34:22 down and checking data and making sure
34:24 the data is correct. But if they know
34:26 best what and how the data should look
34:28 like they can also give you the patterns
34:31 >> so you can build them this easy traffic
34:32 light.
34:34 >> Yeah. So imagine a scenario where an
34:36 analyst is doing their job. I don't know
34:39 maybe there is a task and they need to
34:41 do some analytics but they see that
34:43 there is some discrepancy
34:45 right so they can flag it to the I don't
34:47 know product manager saying hey look
34:49 something is off I don't know what it is
34:52 I need to take more time to investigate
34:54 it but then the team can the data team
34:57 or whoever is maintaining the dashboard
34:59 can immediately
35:00 go from green to yellow saying then
35:02 there is potential discrepancy we're
35:04 looking into it and when the analyst
35:07 comes back with the more concrete
35:10 examples where it's off then you can see
35:12 okay like should we go to red should it
35:14 stay yellow until the team fixes right
35:16 something like that if there is this
35:18 communication channel if the analyst
35:20 knows who to approach when some
35:22 discrepancies are found then we have
35:24 this communication channel then we can
35:26 report to the high level uh to people
35:30 who actually rely on this data that
35:31 something is off don't completely trust
35:33 this data yet we're investigating the
35:36 problems
35:37 >> correct
35:38 Does it this is how it looks in practice
35:40 or you have some you have seen some
35:42 other ways of building setting this up?
35:45 >> I saw a lot of ways of automation. So
35:47 this is what I'm talking a lot with
35:49 teams about how to actually automate
35:50 this process. So taking the learnings
35:52 from the analyst who is checking the
35:54 data and creating an automation system
35:57 that creating logs at the end of day
35:58 when we ingesting data we're getting
36:00 logs we can analyze them right so we can
36:02 store them into a posgrade SQL server
36:05 very easy very simple and then scan them
36:07 once a day if something doesn't fit to
36:10 what we see as a trend so for example in
36:12 the last seven days something has
36:13 happened we can already flag this stuff
36:15 and then there is the analyst who can do
36:17 the deep dives because they really the
36:18 experts inside this data
36:21 uh and they can come and say okay I
36:23 suspect that something is wrong it's
36:24 green but I'm thinking it still should
36:26 be yellow and we able to do it but at
36:28 the end of the day you're communicating
36:30 in a very easy system for everybody to
36:32 understand because it's not speaking a
36:36 tech language it's not nobody cares at
36:38 the end of day in the in the data
36:40 consumer side if it's h something that
36:43 got broken in the pipeline or in it's a
36:46 certain tool that is not working because
36:48 they don't understand it for They just
36:50 need the data and this is what they care
36:51 about and this is what we need to
36:52 simplify for them as well. And the same
36:56 thing goes with whenever there is a data
36:58 incident, they don't care about the back
37:00 end of how things are being resolved.
37:02 They just know that it needs to be
37:04 resolved. But here again, we have an
37:06 issue because if we need now, for
37:07 example, we had loss of data for two
37:09 weeks and it's going to cost us
37:10 €200,000.
37:12 We need to be able to communicate it. We
37:14 need to be able to actually also explain
37:16 it to the people on why now it's going
37:19 to cost us an extra 200,000 euros to
37:22 recmp compute the data and refix it if
37:24 needed. Uh and I think that there is a
37:27 lot a lot this this is the biggest gap
37:29 that I've identified in a lot of my
37:33 talks. It's the communication is the
37:34 transparency and it's less about the
37:37 tools themselves and how we are using
37:38 them. M and I imagine if this is a green
37:41 field project, if this is a project we
37:44 are starting start starting from scratch
37:45 knowing things we discussed we can add
37:48 this uh traffic light immediately from
37:50 the very beginning. Okay, we need a
37:52 dashboard with KPIs of this product. So
37:55 we put the the the traffic lights there
37:58 immediately from the start and it's good
38:01 right. So we already have this in mind
38:03 but in many cases in the majority of
38:05 cases there is already an existing
38:07 product existing dashboard and already
38:10 existing trust issues. So what do we do
38:13 there?
38:15 >> We need to we need to start figuring out
38:17 the pattern. So this is this is I think
38:19 we one of the things that I'm looking at
38:23 is also the stress level of the team. So
38:25 how much they spending their uh efforts
38:29 or their focus on the three components
38:31 that I mentioned before the maintenance
38:33 the innovation and the roll out because
38:37 what happen is if we starting it
38:39 correctly the maintenance basically it's
38:41 all the product that existing today and
38:43 working and being used by users but then
38:45 we need to have a process that we are
38:47 able to supervise what tickets are being
38:50 created what are the patterns of these
38:52 tickets when do we handle it how do we
38:55 priority porize it. Maybe there are some
38:56 products that keep having issues and we
38:58 should deprecate them because it doesn't
39:00 make sense anymore to hold them and we
39:02 can maintain this one. When we're
39:04 rolling out a new product, when we
39:07 innovate it, we roll it out. Now, we
39:09 need we have more users coming in. We
39:10 need to make sure that uh the traffic
39:12 light is in there. And with the
39:14 maintenance part, we really going to the
39:17 core of how many bugs, how many issues
39:19 are there, start identifying and
39:21 starting building basically the system.
39:22 We can go historically. So I hope a lot
39:24 of teams storing the tickets that they
39:27 have for support and they can go
39:29 historically start accumulating from the
39:31 J and see what's going on there and if
39:33 not it's not too late to start now give
39:36 yourself 60 days or 30 days and you
39:39 already have a balance also to
39:40 understand how well this product working
39:42 where are the issues and what you need
39:43 to flag so it's already giving you a
39:45 very good indication
39:48 >> you're talking about the data team
39:49 stress index right
39:51 >> this is part of it as well
39:54 Mhm. And there um
39:58 >> yeah I'm just trying to understand how
40:00 to implement this in practice. So we
40:02 have these three things teams spend time
40:04 on
40:05 >> maintenance rollout innovation
40:08 and
40:09 as a former data scientist I imagine
40:12 that people mostly want to spend time on
40:15 innovation
40:16 and not really on maintenance or roll
40:18 out. At least for a data scientist roll
40:20 out is like what is that like
40:24 maintenance okay there is a bug in the
40:26 code that I wrote perhaps I'm actually I
40:30 have to fix it right okay like this is
40:32 something I can accept right but roll
40:34 out is um yeah doesn't sound very fun
40:38 right
40:39 >> none of them is still
40:41 >> still this is something that has to
40:42 happen right and uh would you say uh
40:46 stress index goes
40:48 up when it's just maintenance,
40:50 maintenance, maintenance, roll out,
40:52 constant battles like firefighting and
40:55 little innovation or how do you
40:57 >> the the reality is that maintenance is
41:00 much higher than what uh people think
41:04 and this is driven by several things. We
41:07 keep releasing new products and in many
41:09 cases we ignoring the maintenance of
41:11 this product. So we don't think about
41:13 what's going to happen after we released
41:15 it. And this is why I identify these
41:17 three key areas that we need to work on
41:21 when we're talking about maintenance. A
41:22 healthy maintenance will be around 45%
41:24 of your uh day. So anybody who listen
41:28 can go and now check how much actually
41:30 investing in maintenance. And when I
41:32 talk about maintenance, it's bug fixes,
41:34 ad hoc request, even meetings can be a
41:38 disruptors that uh happen. And this is
41:41 the three main uh areas there and how
41:44 you're handling it. Then we're talking
41:46 about the innovation which is the arm
41:48 that basically making you go forward. So
41:51 moving always forward. Reality is that
41:54 uh most teams investing between 10 to
41:56 20% of it. Some teams investing 30% but
42:00 it's unclear for them for example how
42:03 much they have to invest because the
42:05 maintenance keep disrupting them. It's
42:07 keep coming in and basically pulling
42:08 them out from the innovation process.
42:10 >> And then the rollout part of it is
42:13 closing this loop. Right. So you
42:15 innovated, you created something, you
42:16 need to make sure it's actually arriving
42:18 to the users. It's not only a a hit the
42:23 definition of done of the project, it's
42:25 actually being used by them and it's
42:27 being liked by them. And I think this is
42:29 something that we sometimes tend to
42:30 ignore and we should also include it
42:33 inside our thinking.
42:34 >> And this is basically how these three
42:36 components connect to each other. And a
42:38 team that has about 45% maintenance,
42:41 they're already in a stress area. Now
42:42 add to it the new innovation or
42:44 initiatives that they need to achieve.
42:46 Still
42:47 >> you're bringing them to a very high
42:49 stress levels that they are unable to
42:51 actually
42:52 compete or or complete basically sorry
42:55 to complete the task that they have in
42:56 hand because they are stuck. They are
42:59 stuck in maintenance. Maintenance keep
43:00 pulling them back and stopping them from
43:02 going forward. And if you releasing a
43:06 product that is innovative so as you
43:08 said in data science and I had it I had
43:09 a beautiful case of it. So we launched a
43:12 beautiful uh forecasting for marketing.
43:15 Okay. So what is going to be the user
43:17 value you know CLV everybody loves it
43:21 and the team were releasing it. They
43:23 said yeah it's great it's working. We
43:25 can move backwards now. They gave it to
43:28 the marketing team. They gave it to the
43:29 product team. They started using it and
43:30 at some point you know the model started
43:32 to break.
43:34 >> But nobody checked who are the users and
43:36 how to use it. Nobody understood what is
43:39 the use case. Is it actually fulfilling
43:41 the product itself, the CLV that we just
43:43 built, the forecasting and nobody was
43:45 maintaining it in the long term? And
43:47 this is exactly the problem. This is
43:49 where the chaos starts and everything
43:51 collapsing and crumbling and if we are
43:53 not aware of it, we won't be able to
43:55 handle it. And when it explode, it's too
43:58 late to fix sometimes even because we
44:00 know how it is, right? If somebody lost
44:02 trust in your model completely, you will
44:05 need to work really really hard to climb
44:06 back and and make them trust it.
44:08 >> Mhm. Okay. So from what you said uh one
44:12 of the important things I took a note of
44:14 was that a healthy and uh the important
44:17 word here is healthy. Uh the healthy
44:20 level of maintenance is 45%. Cuz if you
44:23 just innovate all the time, you don't
44:24 have time for maintenance. You just keep
44:26 releasing products and then you don't
44:28 fix problems in these products. Cuz
44:30 yeah, we all know it's not fun to fix
44:34 problems. It's more fun to create new
44:36 things. But yeah, we need to take care
44:39 of the existing things too. And if all
44:41 we do is innovate and of course like
44:44 there are bugs, there are problems,
44:46 there are inconsistencies, the data is
44:48 not perfect and this is something we
44:50 need to deal with otherwise uh
44:53 yeah it we lose trust but then if we
44:56 start spending more and more and more
44:58 time on maintenance then we cannot
45:01 innovate anymore and then it's another
45:03 problem.
45:04 >> Correct.
45:05 >> Right. Okay. clear and um actually the
45:09 very first question that I have in the
45:11 list of questions which we I think we
45:13 completely diverge from this list it's
45:16 always fun to see like how the list of
45:18 questions uh and the actual questions at
45:21 the end uh turn out anyways but like um
45:24 you describe yourself as a data Zen
45:27 master
45:28 so what are these Zen principles did we
45:30 talk about them already
45:33 >> we talked about mindfulness right but
45:35 like in practice what do you actually do
45:37 like how do you achieve zen when it
45:39 comes to all these problems that we
45:41 talked about like how do you make peace
45:43 with you yourself and the teams around
45:47 >> so we talked about the wabishabi right
45:48 the the concept of this imperfection and
45:50 I think this is this is the key concept
45:52 that I bring to a lot of the teams
45:54 because
45:55 >> we need to accept the reality that
45:57 >> whatever tool you innovate and you're
46:00 rolling it out is not going to be
46:02 perfect in the long term it's going to
46:03 be at some point it's going to be
46:05 imperfect
46:06 And I think it's talking about a lot of
46:08 being aware of what you're doing and the
46:11 impact of it and what is the long-term
46:13 and the short-term actions that you need
46:14 to do. It's a lot about planning a
46:18 direction. So there is a journey that
46:20 we're walking through. uh having a
46:22 vision for the next three years how the
46:24 data ecosystem going to look like
46:26 understanding that while we committed
46:29 now to innovate this year 50 new
46:32 initiatives we're not going to be able
46:34 to achieve them because we're going to
46:36 be disrupted by different uh parts and I
46:39 think this is a lot about taking the
46:41 mindfulness into our conversation into
46:44 the acceptance of it's not going to be
46:46 perfect it's not about the teams
46:48 becoming now all chill and relaxed and
46:51 everything is great and meditation. This
46:54 is not the point.
46:58 >> The data is wrong but like what can we
47:01 do?
47:02 >> We are just here we are just here for
47:04 the paycheck you know.
47:05 >> Yeah
47:07 >> it's it's about
47:09 >> it it's it's broken but yeah
47:13 >> we have a lot of options to to handle it
47:15 but the idea behind it is really to come
47:17 and say we are aware of it. We know that
47:20 there is a certain I will give you the
47:23 best example. Okay. My grandma uh may
47:26 she rest in peace used to make this
47:28 amazing strawberry cake. Now the
47:30 strawberry season is around 1 month and
47:33 a half more or less no more than that.
47:36 And her work was used to be around three
47:38 days to prepare the cake. Whenever the
47:41 cake was ready, she used to serve it to
47:43 the table. Me, my cousins, my family,
47:45 everybody were there. But you always
47:47 used to tell us, listen, you need to
47:49 hold yourself. You need to slice it
47:51 correctly. Make sure everybody receive a
47:52 piece. So there is no case somebody
47:54 ending up not receiving a piece. You
47:56 need to make sure that you enjoying the
47:58 bites because this is something that not
48:02 going to repeat anytime soon. You need
48:04 to wait for next year. I'm not freezing
48:06 strawberries. I'm using only fresh
48:07 strawberries for this cake. And this is
48:09 part of this experience of the data
48:11 teams that they need to understand we
48:13 have a certain cake of hours we can
48:15 invest. We need to make sure that we
48:18 slicing it and giving the right uh
48:20 impact to the organization. But at the
48:23 same time we also enjoying it on the
48:25 process and not in a constant burnout
48:28 mode. I need to achieve this, I need to
48:29 run, I need to do that. You know like
48:32 crazy
48:34 always high. And we should not al always
48:36 be like yeah what we can do. It's always
48:38 broken. This is This is not the right
48:40 cake. You need to to enjoy this right.
48:43 You need to enjoy the cake. The
48:44 strawberry cake. We talked about
48:45 cooking. That's why I needed to do it.
48:47 I'm sorry. But
48:49 >> yeah, there's no way around with you.
48:51 Like it's always cooking.
48:52 >> But does have to have anything to do
48:55 with cooking?
48:57 >> Sorry again.
48:58 >> Because vabi sounds like wasabi.
49:01 >> It sounds like but it's not. It's a con.
49:02 It's just a concept of imperfection.
49:04 It's it's a way of accepting the reality
49:07 as is. and and basically riding or keep
49:09 the flow running
49:11 >> accepting you know enjoying the seasons
49:13 going outside and looking the seasons
49:14 and this exactly the same thing that we
49:15 need to do with data we need to go there
49:18 accept that it's not going to be perfect
49:19 but whenever it's there we should enjoy
49:21 it as much as possible
49:23 >> and continue flowing and continue the
49:25 movement
49:27 >> you live in Berlin right
49:29 >> outside of Berlin a little bit
49:30 >> outside of Berlin so in Berlin people if
49:33 they want to swim in the sea they go to
49:36 the north, the Baltic Sea.
49:38 >> Correct.
49:38 >> And it's super cold,
49:40 >> but there's no other sea. You're not
49:43 going.
49:46 >> Yeah. So, I was thinking like um
49:49 there's no other sea. So, like you
49:51 either enjoy this one or you don't enjoy
49:53 the sea at all or
49:55 >> Yeah.
49:58 Right.
49:59 >> But but the the idea the idea is that
50:01 there is there's data and people need to
50:05 use the data. This is not going to
50:06 change. On the contrary, now when we're
50:08 talking about all of these LLM models,
50:10 we're talking about Gai, the demand for
50:12 data going to increase massively. We
50:14 already see it. While a lot of teams
50:17 trying to hide it, it's like no, no, no,
50:18 we don't do anything with automation. We
50:20 don't use data. It's because of the fear
50:22 of garbage in, garbage out.
50:25 The reality is that we will need to
50:27 actually fix these issues and we won't
50:29 be able to fix it until we're not going
50:31 to arrive to the awareness of where the
50:33 problems are and how to fix it. So this
50:35 is for me like being a mindful about the
50:38 data being in the zen when it's coming
50:41 to it. It's not the religion, it's the
50:43 philosophy that comes with it. Mhm. And
50:47 one of the things you mention is when we
50:50 were talking about this vab principles
50:53 that you accept that the tool the
50:57 product the dashboard whatever is not
50:59 perfect and um let's say you make an MVP
51:03 you say this is how it might look like
51:05 it's not perfect that data might be off
51:08 but you also have a long-term or and
51:10 short-term plans for okay if this
51:13 project continues to exist this is how
51:16 we are going to improve. This is the
51:17 shortterm plan. This is the long-term
51:19 plan. But how do you know what's good
51:22 enough? How do you know when um the
51:26 thing we want to show is ready to be
51:28 shown that yes, we don't have complete
51:31 trust in the data we have but it's
51:34 actually good enough to make some
51:37 decisions now like when do we know this?
51:39 How do we know this?
51:41 We know it when we measuring impact and
51:44 this is why I said it's not the
51:45 spiritual way of like it's going to be
51:47 what's going to be but it's actually by
51:49 measuring impact of whatever we build.
51:52 So if the MVP is already supplying
51:54 value, so the ROI is maybe still
51:56 negative but it's getting towards the
51:59 positive side, we already did something
52:01 good. In the long term, what we
52:02 expecting it as as you mentioned, it's
52:04 like it's a data product, right? So
52:05 we're going to have iterations that we
52:07 keep developing it. It has a life cycle.
52:10 We starting with the uh development part
52:14 which will be most likely in a negative
52:15 ROI. Then we have the uh roll out when
52:20 we actually introducing it to the
52:21 people, people getting to know it.
52:23 Slowly the ROI should get improved from
52:25 negative towards the the breaking even
52:28 or towards the positive even.
52:31 Then at some point it arrives to
52:32 maturity because people are using it,
52:34 people like it, it's actually creating
52:37 value for them. And this is basically
52:40 where we should be very happy with this
52:42 position and we should maximize it as
52:44 much as possible. need to be aware that
52:46 it's going to create disruption but we
52:48 we are able to handle it and at some
52:50 point this product is going to arrive
52:52 also to decline because it's not
52:54 producing anymore the value that we
52:55 wanted. So the ROI dropping down slowly
52:58 the user satisfaction goes down and this
53:01 is where we need to be aware more on
53:04 this maybe the time to kill it and try
53:05 something new.
53:06 >> Mhm. But the the MVP itself if it's
53:10 produced value and we're talking about
53:13 value and and and and when I'm talking
53:15 about value it could be cost
53:17 optimizations. So it saved us X amount
53:19 of hours that translate to this amount
53:22 of money. It can be we saved some
53:25 infrastructure that we are running it
53:28 whatever it's it's saving you cost or
53:30 generating revenue. So for example if
53:32 you created now a new recommendation
53:34 engine for our users. So we're talking
53:35 about data science, right? And this
53:37 model improved the conversion by 2.5%.
53:41 This is what it's worth for us. So it's
53:43 already we are able to quantify the
53:46 revenue value of this data product
53:50 and we can also forecast from this point
53:52 what we expecting it to create an impact
53:55 in the long term if we're going to keep
53:56 working on it and model it and improve
53:59 the model. So we're going to reduce the
54:00 cost, we're going to increase the
54:02 revenues and the efficiency will become
54:04 much better and much clearer for us.
54:07 >> I think in the example you gave um a few
54:09 minutes ago, it was I think customer
54:11 lifetime value prediction or something
54:13 like that.
54:15 >> Sorry. What
54:16 >> CRV? Yeah. The the
54:17 >> CV. Yeah. Uh I think at some point uh
54:21 there was this I all the the stages that
54:23 you described um like development uh
54:27 early adoption like maturity decline I
54:30 think it had it it went through all of
54:32 them right cuz at the end people stopped
54:34 trusting it and they thought okay like
54:36 yeah we don't want to use it anymore but
54:39 at the beginning I guess it was already
54:42 when you knew it was good enough when
54:44 you could show a prototype to your user
54:46 and they could say to the user of this
54:50 prototype and they could say okay now I
54:52 can actually make an action based on the
54:55 output of this thing.
54:56 >> Correct. Correct. And in more cases
54:59 in most of the cases the the suggestion
55:02 would lead to a positive uh AOI kind of
55:05 action right even though sometimes it
55:07 might not be correct but overall
55:11 >> there were cases where I saw models that
55:13 h people said yes it's it's what I need
55:17 and the ROI at the end of it was
55:19 negative because the cost of producing
55:20 it was higher than the cost of okay
55:23 sorry it was higher than the revenue it
55:25 was bringing in and at this point you
55:28 need to evaluate is it making sense to
55:30 keep uh the model or not.
55:32 >> Mhm.
55:33 >> And when we're talking about the CLV
55:35 before it went through the maturity. So
55:36 it was creating this value it was
55:38 bringing we said okay it's making sense
55:40 but it's also made sense because we
55:42 stopped maintaining it. So we didn't
55:43 invest new uh efforts in optimizing it
55:47 in fixing it in correcting the model and
55:50 this way it arrived also to the decline.
55:52 And this the same thing goes with any
55:54 other product that we we launching,
55:56 right? At some point, if we don't keep
55:58 maintaining it and keep listening to the
56:00 people around us and aware of what they
56:02 need and what are the issues they think,
56:05 they're just going to drop us.
56:07 >> Mhm. Yeah. I noticed that we have quite
56:10 a few questions from the audience. Uh do
56:12 you not do you need to go now or you
56:14 have a bit of time?
56:14 >> We can continue. Let's do it.
56:16 >> Okay. Yeah. So there's an interesting
56:19 one and I think I kind of asked you
56:21 partly a little bit about that. So what
56:24 do you recommend when the problem starts
56:26 on a database from a legacy monolith
56:29 application that continues to be the
56:31 core of the business? And I think I uh
56:33 the the question I asked you at some
56:36 point was like okay it's all cool with
56:38 green field projects but what about uh
56:40 existing projects
56:41 >> like when it's already broken but there
56:44 is nothing else like how do we approach
56:47 this case how do we solve trust issues
56:50 and how do we find the root cause
56:52 >> so in leg in legacy system this is my my
56:55 favorite thing to to look at uh I was in
56:59 an organization a few months ago that I
57:01 will This is a great case. So they had a
57:04 legacy system that was automating for
57:06 them the campaign steering. So they were
57:08 steering the entire budget based on some
57:10 automation system that was built by
57:13 someone who already left the company and
57:15 it started to crack and make issues.
57:18 And the replacement team that arrived
57:21 after he left tried to maintain it but
57:23 it didn't work out for them. And the
57:26 case that we created together was let's
57:28 figure out actually what is the cost of
57:30 these errors and let's now uh create a
57:34 case instead of keep fixing it to stop
57:36 accepting the reality of where the
57:38 product is right now and the errors
57:40 we're going to do the minimum
57:41 maintenance that we need to do and let's
57:43 build something new that going to
57:45 replace it that we can actually move
57:47 forward and I think this is this is
57:48 exactly the problem of legacy systems a
57:50 lot of them it's it's basically dep that
57:52 we carrying with us and we're trying to
57:55 we're trying to survive with it but at
57:57 some point we should say we stop it and
57:59 this is something for example that I did
58:00 also at Zelando when I arrived we had
58:03 this Python script that was requiring
58:06 six six people to copy paste manually on
58:09 a daily basis data from different Excel
58:12 sheets into a main Excel sheet that this
58:14 Excel sheet then used to be going into
58:16 the Python being processed it was a huge
58:20 operation that didn't work out and we
58:22 said at the end of the day the value
58:24 that it's producing right now is very
58:26 low because the issues that we keep
58:28 having were causing a case that it was
58:30 available only once a week instead of a
58:32 daily basis it's supposed to. So why do
58:35 we keep fighting and keeping this one
58:37 working and running and not just stop it
58:40 and say we don't touch it anymore. We're
58:42 building something else to replace it
58:45 and we're making sure that it's giving
58:48 the similar value even higher value than
58:51 what we had before. So it's much easier
58:53 and this was our approach at Zelando
58:56 back then that I instead of it we build
58:57 the data warehouse we stored all of our
59:00 data on a posgrade SQL I remember it it
59:02 was a beautiful thing with simple APIs
59:05 that were pulling the information and
59:06 then we put it into Tableau and instead
59:08 of people now having a very heavy Excel
59:11 sheet that they cannot run the pivots on
59:12 it they had a drag and drop dashboard
59:15 people were going crazy and this is
59:17 exactly the thing that we need to think
59:18 about also when we're doing it what is
59:20 the impact or what is the added value
59:22 that we're going to give to the users if
59:23 we're moving from a legacy system and
59:25 this how you're selling it. This is the
59:27 selling point.
59:29 >> Yeah. The the problem with legacy
59:30 systems is they they kind of work and
59:34 they work well enough but sometimes they
59:37 get broken and when they get broken you
59:40 have no idea how to fix them
59:42 >> and or somebody's fell in love with this
59:44 legacy system and it refused to move out
59:46 of it. Right. This is another case
59:48 >> could be yeah like a genius wrote this
59:50 overnight but then they don't remember
59:53 anything but they still are proud of
59:56 them writing it.
59:58 >> Correct.
59:59 So we need to find the right balance but
1:00:01 at the end of the day we really we need
1:00:03 to think about impact impact and impact.
1:00:05 What is the impact of it? Communicate
1:00:07 this bad impact. communicate the
1:00:10 possible better impact by the
1:00:11 replacement if we summarizing it and
1:00:14 then go ahead and make sure that the new
1:00:16 impact is much higher and much better
1:00:18 than what we have right now.
1:00:20 >> Another question always easy.
1:00:22 >> Yeah. Yeah. Thank you. Another question
1:00:23 is how do you handle ad hoc requests
1:00:25 from the executives?
1:00:28 >> Say no. No, I'm joking. Uh with
1:00:33 excessive it's really there there is
1:00:35 here a double double thing that you need
1:00:37 to understand. First one is why they
1:00:39 need it and what is the core ID behind
1:00:41 the ADOC request and then the second
1:00:43 part what is the impact that they
1:00:45 expecting to have. You cannot always say
1:00:47 no to executive and you should not but
1:00:50 you should come to them and tell them
1:00:52 listen if I need to do this headoc
1:00:54 request these are the initiatives that
1:00:56 going to be postponed or going to be
1:00:57 pushed back this is the impact or the
1:00:59 value they're planning to bring to the
1:01:00 organization does it actually make sense
1:01:03 it's changing the conversation
1:01:04 drastically so this is what I noticed
1:01:06 with a lot of teams that I worked with
1:01:08 in the past when they started talking to
1:01:10 the CEO in his language or in their
1:01:13 language sorry the CEOs started to react
1:01:16 to it because when you coming to them
1:01:18 and you tell them listen if I'm doing
1:01:20 now your headoc request and it's going
1:01:22 to take me five days this is what it's
1:01:25 pushing back are you okay with it if yes
1:01:27 everything is great and I will tell you
1:01:29 that most CEOs will say no and my
1:01:33 headock is not that important just
1:01:34 continue your work because something
1:01:36 else is going to create a bigger impact
1:01:39 but we need to quantify we need to have
1:01:40 a monetary value
1:01:44 >> okay thank So and then the last question
1:01:47 uh for today. So you mentioned you
1:01:49 started with analytics. I think you
1:01:51 actually started with development then
1:01:52 you moved to analytics then to product.
1:01:55 Um but the question is do you have any
1:01:57 tips to understand if I am more suited
1:02:00 for analytics or I'm more suited for
1:02:02 engineering.
1:02:04 >> I think it's a lot of personal feeling
1:02:07 where you feeling more comfortable with
1:02:08 and what you're feeling comfortable
1:02:09 with. So I see uh when I'm looking at it
1:02:12 the engineers was about the solution. I
1:02:15 enjoyed solving something and this is
1:02:17 something I did not enjoy. Don't tell
1:02:19 anybody. But this was not the core of of
1:02:22 my fun. Okay, this is the problem just
1:02:23 solve it. I enjoyed more from the
1:02:25 problem space. As an analyst you again
1:02:27 you receiving a problem you can help to
1:02:29 refine the problem but you still need to
1:02:30 supply solution. In the product
1:02:33 perspective, what you expected is to
1:02:35 actually define the problem, define the
1:02:37 scope of the problem and then refine it
1:02:40 to the engineers and give it to them. So
1:02:43 how do you will how will you solve it
1:02:45 and then refine again the problem based
1:02:46 on their feedback.
1:02:48 >> And I think that this is what you need
1:02:50 to think about where what you enjoying
1:02:52 more this problem space or the solution
1:02:54 space and then figure out in the
1:02:56 solution space for example what are you
1:02:59 enjoying more to do to work with the
1:03:01 problem and trying to show or explain
1:03:03 the problem or you enjoying more to
1:03:06 solve the problem itself. M so it's
1:03:09 about the mindset uh that like what you
1:03:12 like more
1:03:14 >> if
1:03:15 yeah I think I'm an engineer always have
1:03:18 been like I tried to become a product I
1:03:21 took courses like I mentioned this mind
1:03:23 the product workshops
1:03:25 uh but the funny story so I took a nano
1:03:28 degree in on Udacity about product
1:03:31 management and then one of the parts or
1:03:34 one of the modules there was about uh
1:03:37 all the
1:03:40 um scrum things like all the processes
1:03:42 in the in the team and then I asked my
1:03:44 product manager is it really what you do
1:03:46 and he said like yeah all day long like
1:03:49 okay I I don't want to do this
1:03:53 >> I'd rather just go
1:03:56 so it was a very useful course yeah
1:03:58 >> it's it's giving you some perspective
1:04:00 right and I think that there's a lot of
1:04:01 engineers who's looking to move into
1:04:03 product and vice versa
1:04:06 Uh but the product perspective is really
1:04:09 about the problem. It's to talk to the
1:04:10 stakeholders. It's to understand them.
1:04:11 It's to create the tickets and then
1:04:13 understand that you're creating 100
1:04:15 tickets and out of them only 20 will be
1:04:17 relevant in the next month. And again
1:04:20 mindfulness acceptance
1:04:22 it's not going to work always. While in
1:04:25 the engineering uh you also need to
1:04:27 accept that some of the code not going
1:04:29 to be accepted but at least you're
1:04:30 trying to solve the problem. And I think
1:04:32 this is two two different mindsets.
1:04:35 >> Okay.
1:04:36 Yeah, thanks a lot. Thanks for spending
1:04:38 a bit of extra time with us and
1:04:40 answering the questions uh we got from
1:04:42 the audience. Thanks a lot. Was lovely
1:04:45 talking with you. So when you to write
1:04:47 your third book after Babisabi
1:04:51 um you're always welcome to uh become a
1:04:55 guest again. I am really looking
1:04:56 forward. So maybe not in like 3 four
1:04:58 years but earlier. Um yeah and we will
1:05:02 always need your advice on humus. So
1:05:04 there's no other way around.
1:05:06 >> Yeah. Love to.
1:05:07 >> Yeah.
1:05:08 >> By the way, like speaking of humus, so I
1:05:11 mentioned at the beginning that I got
1:05:12 this one from Rebe, which you didn't
1:05:15 approve. I understand. But like uh from
1:05:17 any of the chains u that we have in
1:05:20 Berlin, is there any one that sells
1:05:23 decent hummus or you always need to go
1:05:24 to like a restaurant to have one? I
1:05:28 still go to a restaurant because if you
1:05:29 look at the ones in Berlin in the
1:05:31 chains, most of them it's not only
1:05:34 chickpeas inside it. And this
1:05:35 >> Yeah, there's some oil, right?
1:05:37 >> I prefer the pure one. But the easiest
1:05:39 one is just buy yourself a can of
1:05:41 chickpeas, which today you can find in
1:05:43 Berlin, something you couldn't find 5
1:05:44 years ago in the chains in the
1:05:46 supermarket chains
1:05:48 >> and make it at home. Just cook it a
1:05:49 little bit, add tahini, mix it, and you
1:05:51 have something that you can already
1:05:53 consume. It's not going to be great, but
1:05:54 it's going to be much better than the
1:05:56 one you're buying in any supermarket
1:05:58 chain.
1:05:58 >> Okay. So, next time we speak, I'll
1:06:00 report on my progress with that.
1:06:02 >> Please do.
1:06:05 >> Thank you. Uh, amazing speaking with
1:06:06 you. Thanks everyone for joining us
1:06:08 today and listening in and see you next