{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3b2f0c-f4da-43d0-aae2-64e2fc98a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7f398ab-a115-4e9d-8746-b2a65b61829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def agent():\n",
    "    print('[agent] start')\n",
    "    for i in range(10):\n",
    "        print('[agent]', i)\n",
    "        await asyncio.sleep(1)\n",
    "    print('[agent] stop')\n",
    "    return 'yahoo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc848d64-f011-4c60-886d-cd6f5dc5b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def guardrail():\n",
    "    print('[guardrail pass] start')\n",
    "    await asyncio.sleep(1.5)\n",
    "    print('[guardrail pass] check pass')\n",
    "    print('[guardrail pass] stop')\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0624df7b-dad3-4fb9-bdae-5b1cbf925dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guardrail] start\n",
      "[guardrail] check pass\n",
      "[guardrail] stop\n",
      "[agent] start\n",
      "[agent] 0\n",
      "[agent] 1\n",
      "[agent] 2\n",
      "[agent] 3\n",
      "[agent] 4\n",
      "[agent] 5\n",
      "[agent] 6\n",
      "[agent] 7\n",
      "[agent] 8\n",
      "[agent] 9\n",
      "[agent] stop\n"
     ]
    }
   ],
   "source": [
    "check_pass = await guardrail()\n",
    "if check_pass:\n",
    "    await agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37702556-f1e5-4eea-811d-9e8ee17d6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guardrail] start\n",
      "[agent] start\n",
      "[agent] 0\n",
      "[agent] 1\n",
      "[guardrail] check pass\n",
      "[guardrail] stop\n",
      "[agent] 2\n",
      "[agent] 3\n",
      "[agent] 4\n",
      "[agent] 5\n",
      "[agent] 6\n",
      "[agent] 7\n",
      "[agent] 8\n",
      "[agent] 9\n",
      "[agent] stop\n"
     ]
    }
   ],
   "source": [
    "r1, r2 = await asyncio.gather(\n",
    "    guardrail(),\n",
    "    agent()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "463bb95f-1cf6-468e-9e25-a80b43bd939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GuardrailFunctionOutput:\n",
    "    output_info: str\n",
    "    tripwire_triggered: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "739ec9f4-f3b1-4520-b891-60b27e3dbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuardrailException(Exception):\n",
    "    def __init__(self, message: str, info: GuardrailFunctionOutput):\n",
    "        super().__init__(message)\n",
    "        self.info = info\n",
    "\n",
    "async def guardrail_fail():\n",
    "    print('[guardrail fail] start')\n",
    "    await asyncio.sleep(2.5)\n",
    "    print('[guardrail fail] check fails')\n",
    "    info = GuardrailFunctionOutput(\n",
    "        output_info='check fails',\n",
    "        tripwire_triggered=True\n",
    "    )\n",
    "    raise GuardrailException(\"check fails\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b795edf8-d10c-4ac8-b369-a8e914493095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[agent] start\n",
      "[agent] 0\n",
      "[guardrail] start\n",
      "[agent] 1\n",
      "[guardrail] check pass\n",
      "[guardrail] stop\n",
      "[agent] 2\n",
      "[agent] 3\n",
      "[agent] 4\n",
      "[agent] 5\n",
      "[agent] 6\n",
      "[agent] 7\n",
      "[agent] 8\n",
      "[agent] 9\n",
      "[agent] stop\n",
      "yahoo\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    agent_task = asyncio.create_task(agent())\n",
    "    guardrail_task = asyncio.create_task(guardrail())\n",
    "\n",
    "    r1, r2 = await asyncio.gather(\n",
    "        agent_task,\n",
    "        guardrail_task\n",
    "    )\n",
    "    print(r1)\n",
    "except GuardrailException as e:\n",
    "    agent_task.cancel()\n",
    "\n",
    "    try:\n",
    "        await agent_task\n",
    "    except asyncio.CancelledError:\n",
    "        print('[main] agent cancelled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a710dbca-9c91-4ab4-a80c-a5af4520ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_with_guardrails(agent_coroutine, guardrails):\n",
    "    \"\"\"\n",
    "    Run `agent_coroutine` while multiple guardrails monitor it.\n",
    "\n",
    "    Parameters:\n",
    "        agent_coroutine: an *awaitable*, e.g. agent()\n",
    "        guardrails: an iterable of *awaitables*, e.g. [guard1(), guard2()]\n",
    "\n",
    "    Returns:\n",
    "        The result of the agent, if no guardrail triggers.\n",
    "\n",
    "    Raises:\n",
    "        GuardrailException from any guardrail.\n",
    "    \"\"\"\n",
    "\n",
    "    agent_task = asyncio.create_task(agent_coroutine)\n",
    "    guard_tasks = [asyncio.create_task(g) for g in guardrails]\n",
    "\n",
    "    try:\n",
    "        # If any guardrail raises GuardrailException,\n",
    "        # gather will throw and we drop into except.\n",
    "        await asyncio.gather(agent_task, *guard_tasks)\n",
    "\n",
    "        # Agent finished successfully.\n",
    "        return agent_task.result()\n",
    "\n",
    "    except GuardrailException as e:\n",
    "        # At least one guardrail fired.\n",
    "        print(\"[guardrail fired]\", e.info)\n",
    "\n",
    "        # Cancel the agent.\n",
    "        agent_task.cancel()\n",
    "        try:\n",
    "            await agent_task\n",
    "        except asyncio.CancelledError:\n",
    "            print(\"[run_with_guardrails] agent cancelled\")\n",
    "\n",
    "        # Cancel all guardrails (they may still be running).\n",
    "        for t in guard_tasks:\n",
    "            t.cancel()\n",
    "        await asyncio.gather(*guard_tasks, return_exceptions=True)\n",
    "\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2bfd5b8-cd2c-4cc2-bec9-54811721dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[agent] start\n",
      "[agent] 0\n",
      "[guardrail pass] start\n",
      "[guardrail fail] start\n",
      "[agent] 1\n",
      "[guardrail pass] check pass\n",
      "[guardrail pass] stop\n",
      "[agent] 2\n",
      "[guardrail fail] check fails\n",
      "[guardrail fired] GuardrailFunctionOutput(output_info='check fails', tripwire_triggered=True)\n",
      "[run_with_guardrails] agent cancelled\n"
     ]
    },
    {
     "ename": "GuardrailException",
     "evalue": "check fails",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGuardrailException\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m run_with_guardrails(\n\u001b[32m      2\u001b[39m     agent(),\n\u001b[32m      3\u001b[39m     [guardrail(), guardrail_fail()]\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mrun_with_guardrails\u001b[39m\u001b[34m(agent_coroutine, guardrails)\u001b[39m\n\u001b[32m     17\u001b[39m guard_tasks = [asyncio.create_task(g) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m guardrails]\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# If any guardrail raises GuardrailException,\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# gather will throw and we drop into except.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(agent_task, *guard_tasks)\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Agent finished successfully.\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m agent_task.result()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mguardrail_fail\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[guardrail fail] check fails\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m info = GuardrailFunctionOutput(\n\u001b[32m     11\u001b[39m     output_info=\u001b[33m'\u001b[39m\u001b[33mcheck fails\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     tripwire_triggered=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m GuardrailException(\u001b[33m\"\u001b[39m\u001b[33mcheck fails\u001b[39m\u001b[33m\"\u001b[39m, info)\n",
      "\u001b[31mGuardrailException\u001b[39m: check fails"
     ]
    }
   ],
   "source": [
    "result = await run_with_guardrails(\n",
    "    agent(),\n",
    "    [guardrail(), guardrail_fail()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "902a3413-9d76-456d-9fe1-c744816fe8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ver31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e858085-d5b6-402e-bd04-49c968384aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import search_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ee4a138-4e89-4956-8cf7-77a6764ea58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = search_agent.create_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32b37663-6b21-4b4f-862b-f46394d605e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guardrail pass] start\n",
      "[guardrail pass] check pass\n",
      "[guardrail pass] stop\n",
      "TOOL CALL (search): input_guardrail({\"message\":\"llm as a judge\"})\n",
      "TOOL CALL (search): search({\"query\": \"using llm as a judge in decision making\"})\n",
      "TOOL CALL (search): search({\"query\": \"how llm can assist in legal judgments\"})\n",
      "TOOL CALL (search): search({\"query\": \"legal applications of LLMs in judiciary\"})\n",
      "TOOL CALL (search): search({\"query\": \"AI judges using LLM technology\"})\n",
      "TOOL CALL (search): search({\"query\": \"impact of LLMs on judicial decisions\"})\n",
      "# Using LLMs as Judges\n",
      "\n",
      "\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Evidently allows for the integration of Large Language Models (LLMs) as evaluators or judges in various contexts, including decision-making processes. It leverages LLMs to evaluate the quality of AI outputs, data integrity, and model performance using custom criteria or established benchmarks.\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "- [LLM Evaluation](quickstart_llm.mdx)\n",
      "\n",
      "\n",
      "## How to Implement an LLM Judge\n",
      "\n",
      "1. **Installation and Setup:**  \n",
      "   To get started, install Evidently:  \n",
      "   ```bash  \n",
      "   pip install evidently  \n",
      "   ```  \n",
      "   Also, set your OpenAI API key as an environment variable:  \n",
      "   ```python  \n",
      "   import os  \n",
      "   os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"  \n",
      "   ```\n",
      "\n",
      "2. **Create Evaluation Dataset:**  \n",
      "   Define a dataset that includes questions, target answers, and responses to be evaluated. Example format:\n",
      "   ```python  \n",
      "   import pandas as pd  \n",
      "   data = [  \n",
      "       [\"What's the capital of France?\", \"Paris\"],  \n",
      "       [\"How many continents are there?\", \"7\"],  \n",
      "   ]  \n",
      "   eval_df = pd.DataFrame(data, columns=[\"question\", \"answer\"])  \n",
      "   ```\n",
      "\n",
      "3. **Define an LLM Evaluator:**  \n",
      "   Create an LLM evaluator using predefined templates for grading logic. Hereâ€™s how to set up a binary classification evaluator:  \n",
      "   ```python  \n",
      "   from evidently.llm.templates import BinaryClassificationPromptTemplate  \n",
      "   evaluator_template = BinaryClassificationPromptTemplate(  \n",
      "       criteria=\"\"\"Evaluates if the answer is correct based on the question.\"\"\",  \n",
      "       target_category=\"CORRECT\",  \n",
      "       non_target_category=\"INCORRECT\"  \n",
      "   )  \n",
      "   ```\n",
      "\n",
      "4. **Run Evaluations:**  \n",
      "   Apply the LLM evaluator to your dataset and analyze the results. Example:\n",
      "   ```python  \n",
      "   from evidently import Dataset, Report  \n",
      "   eval_dataset = Dataset.from_pandas(eval_df)  \n",
      "   report = Report([TextEvals()])  \n",
      "   my_eval = report.run(eval_dataset, None)  \n",
      "   ```\n",
      "\n",
      "5. **Analyze Results:**  \n",
      "   View evaluation results using:\n",
      "   ```python  \n",
      "   eval_dataset.as_dataframe()  \n",
      "   ```\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM Evaluation](quickstart_llm.mdx)\n",
      "- [LLM-as-a-jury](examples/LLM_jury.mdx)\n",
      "\n",
      "\n",
      "## References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "- [LLM Evaluation](quickstart_llm.mdx)\n",
      "- [LLM-as-a-jury](examples/LLM_jury.mdx)\n",
      "- [Configure LLM Judges](metrics/customize_llm_judge.mdx)\n"
     ]
    }
   ],
   "source": [
    "result = await run_with_guardrails(\n",
    "    ver31.run(agent, 'llm as a judge'),\n",
    "    [guardrail()] #, guardrail_fail()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae218d2f-c071-4c5f-beff-438c843bea48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
