{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0828b9-baaa-42b8-a0b8-86daa3054a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "import search_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba84c4c-6279-4939-8d48-d8594501b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    chunk_size: int = 2000\n",
    "    chunk_step: int = 1000\n",
    "    top_k: int = 5\n",
    "\n",
    "    model: str = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6708f026-19b9-4d16-bfa3-24b51b2aa168",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You are a search assistant for the Evidently documentation.\n",
    "\n",
    "Evidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\n",
    "It provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n",
    "\n",
    "Your task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "- search — Use this to explore the topic and retrieve relevant snippets or documentation.\n",
    "- read_file — Use this to retrieve or verify the complete content of a file when:\n",
    "    * A code snippet is incomplete, truncated, or missing definitions.\n",
    "    * You need to check that all variables, imports, and functions referenced in code are defined.\n",
    "    * You must ensure the code example is syntactically correct and runnable.\n",
    "\n",
    "If `read_file` cannot be used or the file content is unavailable, clearly state:\n",
    "> \"Unable to verify with read_file.\"\n",
    "\n",
    "Search Strategy\n",
    "\n",
    "- For every user query:\n",
    "    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n",
    "    * Each search must use a different phrasing or keyword variation of the user's question.\n",
    "    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n",
    "\n",
    "- After collecting search results:\n",
    "    1. Synthesize the information into a concise, accurate answer.\n",
    "    2. If your answer includes code, always validate it with `read_file` before finalizing.\n",
    "    3. If a code snippet or reference is incomplete, explicitly mention it.\n",
    "\n",
    "Important:\n",
    "- The 6-search limit applies only to `search` calls.\n",
    "- You may call `read_file` at any time, even after the search limit is reached.\n",
    "- `read_file` calls are verification steps and do not count toward the 6-search limit.\n",
    "\n",
    "Code Verification and Completeness Rules\n",
    "\n",
    "- All variables, functions, and imports in your final code examples must be defined or imported.\n",
    "- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n",
    "- When something is missing or undefined in the search results:\n",
    "    * Call `read_file` with the likely filename to retrieve the complete file content.\n",
    "    * Replace any partial code with the full verified version.\n",
    "- If the file is not available or cannot be verified:\n",
    "    * Include a clear note: \"Unable to verify this code.\"\n",
    "- Do not reformat, rename variables, or omit lines from the verified code.\n",
    "\n",
    "Output Format\n",
    "\n",
    "- Write your answer clearly and accurately.\n",
    "- Include a \"References\" section listing the search queries or file names you used.\n",
    "- If you couldn’t find a complete answer after 6 searches, set found_answer = False.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    title: str\n",
    "    filename: str\n",
    "\n",
    "class Section(BaseModel):\n",
    "    heading: str\n",
    "    content: str\n",
    "    references: list[Reference]\n",
    "\n",
    "\n",
    "class SearchResultArticle(BaseModel):\n",
    "    found_answer: bool\n",
    "    title: str\n",
    "    sections: list[Section]\n",
    "    references: list[Reference]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a8161f-15af-4202-8f95-b3e44380e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AgentConfig()\n",
    "\n",
    "tools = search_tools.prepare_search_tools(\n",
    "    config.chunk_size,\n",
    "    config.chunk_step,\n",
    "    config.top_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4875f2b4-2946-42b3-835d-c58099312a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1750d26c-4bcd-4657-aab5-37be8e3fbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = [\n",
    "    function_tool(tools.search),\n",
    "    function_tool(tools.read_file)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f51606a-8ca6-496e-88d1-e8eade80b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = Agent(\n",
    "    name='search',\n",
    "    instructions=search_instructions,\n",
    "    tools=agent_tools,\n",
    "    model=config.model,\n",
    "    output_type=SearchResultArticle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b93ab9a-3542-47ec-8b7e-c3a295101445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83621aa5-0df3-41f9-9d59-eb967217ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4186662d-5740-45dc-a2ba-b0a40e64909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxn import StreamingJSONParser, JSONParserHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31327481-ac14-4875-97e3-a4c837e42ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResultHandler(JSONParserHandler):\n",
    "    def on_field_start(self, path: str, field_name: str):\n",
    "        if field_name == \"references\":\n",
    "            level = path.count(\"/\") + 2\n",
    "            print(f\"\\n{'#' * level} References\\n\")\n",
    "\n",
    "    def on_field_end(self, path, field_name, value, parsed_value=None):\n",
    "        if field_name == \"title\" and path == \"\":\n",
    "            print(f\"# {value}\")\n",
    "\n",
    "        elif field_name == \"heading\":\n",
    "            print(f\"\\n\\n## {value}\\n\")\n",
    "        elif field_name == \"content\":\n",
    "            print(\"\\n\") \n",
    "\n",
    "    def on_value_chunk(self, path, field_name, chunk):\n",
    "        if field_name == \"content\":\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    def on_array_item_end(self, path, field_name, item=None):\n",
    "        if field_name == \"references\":\n",
    "            title = item.get(\"title\", \"\")\n",
    "            filename = item.get(\"filename\", \"\")\n",
    "            print(f\"- [{title}]({filename})\")\n",
    "\n",
    "handler = SearchResultHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b938eb8-5e9a-45da-ad96-9e7cfd1913c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.exceptions import MaxTurnsExceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fbd5606-a8f1-447e-aa0a-8f57371990a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = 'data drift'\n",
    "input = 'llm as a judge'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd3f68ec-1b33-4519-8020-26ed44f9fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_stream(agent, input, handler, max_turns=3):\n",
    "    try:\n",
    "        result = Runner.run_streamed(\n",
    "            agent,\n",
    "            input=input,\n",
    "            max_turns=max_turns\n",
    "        )\n",
    "        \n",
    "        parser = StreamingJSONParser(handler)\n",
    "\n",
    "        async for event in result.stream_events():\n",
    "            if event.type == \"run_item_stream_event\":\n",
    "                if event.item.type == \"tool_call_item\":\n",
    "                    tool_call = event.item.raw_item\n",
    "                    f_name = tool_call.name\n",
    "                    args = tool_call.arguments\n",
    "                    print(f\"TOOL CALL ({event.item.agent.name}): {f_name}({args})\")\n",
    "            \n",
    "            if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "                parser.parse_incremental(event.data.delta)\n",
    "\n",
    "        return result\n",
    "    except MaxTurnsExceeded as e:\n",
    "        print('too many turns')\n",
    "        finish_prompt = 'System message: The number of searches has exceeded the limit. Proceed to finishing the writeup'\n",
    "        finish_message = [{'role': 'user', 'content': finish_prompt}]\n",
    "        messages = result.to_input_list() + finish_message\n",
    "        final_result = await run_stream(agent, input=messages, handler=handler, max_turns=1)\n",
    "        return final_result\n",
    "    except InputGuardrailTripwireTriggered as e:\n",
    "        output = e.guardrail_result.output\n",
    "        if output.tripwire_triggered: \n",
    "            print(output.output_info)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb143b54-cd8b-4998-b12c-4ee523b50cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"using llm as a judge in AI evaluation\"})\n",
      "TOOL CALL (search): search({\"query\":\"LLM applications in legal context\"})\n",
      "TOOL CALL (search): search({\"query\":\"AI evaluation ethics and judge role\"})\n",
      "TOOL CALL (search): search({\"query\":\"role of large language models in decision making\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"examples/LLM_judge.mdx\"})\n",
      "TOOL CALL (search): search({\"query\":\"LLM as a judge insights and applications\"})\n",
      "too many turns\n",
      "# Using LLMs as Judges in AI Evaluation\n",
      "\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Large Language Models (LLMs) can be employed as evaluators or 'judges' in various applications, particularly in the context of evaluating responses, generating insights, and monitoring the quality of AI systems. Utilizing an LLM as a judge can enhance the evaluation process by providing consistent, objective metrics and assessments.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "\n",
      "\n",
      "## Evaluation Techniques\n",
      "\n",
      "1. **Reference-Based Evaluation**: This approach involves comparing new outputs against a set of known ground truth responses. This method is beneficial for regression testing where the correctness of the outputs can be established against approved responses.  \n",
      "2. **Open-Ended Evaluation**: In scenarios where there are no fixed references available, LLMs can utilize custom criteria to assess the quality of responses. This flexibility allows for broader applications, particularly in evolving domains.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "\n",
      "\n",
      "## Setting Up an LLM Judge\n",
      "\n",
      "To set up an LLM as a judge, certain steps are involved:  \n",
      "1. **Dataset Preparation**: Create a dataset consisting of questions, target responses, and the new responses to be evaluated. This dataset will serve as the foundation for the evaluations.\n",
      "2. **Evaluation Configuration**: Utilize specific prompt templates that outline the evaluation criteria the LLM should follow, such as correctness or verbosity.\n",
      "3. **Running Evaluations**: Use the LLM to evaluate the responses based on the defined criteria and generate reports that summarize the findings.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "\n",
      "\n",
      "## Updating and Improving Judges\n",
      "\n",
      "Evaluations produced by the LLM can be refined over time. By reviewing the results of the evaluations and adjusting the criteria or strategies used, users can continually improve the quality and accuracy of the assessments made by the LLM. This iterative process enhances the overall reliability of the AI system.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "LLMs serve as valuable tools in evaluating and monitoring AI responses. By integrating these models into evaluation processes, organizations can improve their understanding of model outputs and enhance the overall performance of their AI systems.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "\n",
      "## References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n"
     ]
    }
   ],
   "source": [
    "result = await run_stream(\n",
    "    search_agent,\n",
    "    input,\n",
    "    SearchResultHandler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1877881-b782-4fbe-a1db-dfb917b33fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_instructions = \"\"\"\n",
    "Make sure that the question the user asks is about the Evidently library,\n",
    "monitoring, observability, AI, Machine Learning, or LLMs. If it's not, report it by\n",
    "setting `fail` to True\n",
    "\n",
    "Evidently is an open-source Python library and cloud platform for evaluating,\n",
    "testing, and monitoring data, AI and LLM systems. It provides evaluation metrics,\n",
    "testing APIs, and visual reports for model and data quality.\n",
    "\n",
    "Examples of relevant topics:\n",
    "\n",
    "- Create a custom LLM judge\n",
    "- Customize data drift detection\n",
    "- llm evaluations\n",
    "\n",
    "Explain your decision in the reasoning field, but don't use more than 10 words\n",
    "\"\"\".strip()\n",
    "\n",
    "class EvidentlyDocsGuardrail(BaseModel):\n",
    "    reasoning: str\n",
    "    fail: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "669b1990-907a-4752-a0c3-f61520132005",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardrail_agent = Agent( \n",
    "    name=\"guardrail\",\n",
    "    instructions=guardrail_instructions,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=EvidentlyDocsGuardrail,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f67aa46-863f-4b65-9902-4b6bc7078570",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(guardrail_agent, 'whats sqrt(pi)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb8cbd84-ccb6-4170-8115-e4fff273ea76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvidentlyDocsGuardrail(reasoning='Question is unrelated to Evidently or AI.', fail=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57ecaa92-d3ec-4747-8058-8f532e5d1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import input_guardrail, GuardrailFunctionOutput\n",
    "from agents.exceptions import InputGuardrailTripwireTriggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57e5060a-7855-4783-b049-38b584a06464",
   "metadata": {},
   "outputs": [],
   "source": [
    "@input_guardrail\n",
    "async def documentation_guardrail(ctx, agent, input):\n",
    "    result = await Runner.run(guardrail_agent, input)\n",
    "    final_output = result.final_output\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=final_output.reasoning, \n",
    "        tripwire_triggered=final_output.fail,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21832eb3-0185-433e-9cf7-b4778eb2cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = Agent(\n",
    "    name='search',\n",
    "    instructions=search_instructions,\n",
    "    tools=agent_tools,\n",
    "    input_guardrails=[documentation_guardrail],\n",
    "    model=config.model,\n",
    "    output_type=SearchResultArticle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34283f5a-efaa-4b92-9d9a-e981bf3efa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not related to Evidently or ML topics.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = await Runner.run(search_agent, 'whats sqrt(pi)')\n",
    "except InputGuardrailTripwireTriggered as e:\n",
    "    output = e.guardrail_result.output\n",
    "    if output.tripwire_triggered: \n",
    "        print(output.output_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80d21b54-5944-46ea-ae4d-6fbc04de6275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not related to Evidently library or AI topics.\n"
     ]
    }
   ],
   "source": [
    "result = await run_stream(\n",
    "    search_agent,\n",
    "    'whats sqrt(pi)',\n",
    "    SearchResultHandler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab389e3b-4d8c-425f-9ebb-1a6c11a88f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_stream(\n",
    "    search_agent,\n",
    "    'whats sqrt(pi)',\n",
    "    SearchResultHandler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06413dcc-4b0c-47e0-8ce6-ce8879fa0311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
