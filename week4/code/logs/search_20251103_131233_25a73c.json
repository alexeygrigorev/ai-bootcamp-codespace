{
  "agent_name": "search",
  "system_prompt": [
    "\nYou are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.\n"
  ],
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search",
    "read_file"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How do I monitor data drift in production?",
          "timestamp": "2025-11-03T13:12:28.531036+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\": \"monitor data drift in production\"}",
          "tool_call_id": "call_ca45zrMrsGMANjaWozhxxqg6",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"data drift monitoring techniques\"}",
          "tool_call_id": "call_KzskwISpBMPsb2mtSVZPbMjD",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"Evidently library for data drift\"}",
          "tool_call_id": "call_Je5maDWhH8Fh9g8vI6AvwWLK",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"how to detect data drift in AI models\"}",
          "tool_call_id": "call_QtjjvwtvdOHjvK6vdXX3xVB3",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"data quality monitoring with Evidently\"}",
          "tool_call_id": "call_n7Q9KK2o3CI6n9TJTEbsifhz",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"Evidently documentation for data drift\"}",
          "tool_call_id": "call_gEhGKtKEOpNquN3tKBzcJPtG",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 784,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 125,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-03T13:12:29+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CXoeD3Q7d6BBcxcKxrQosr88kE1Le",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 0,
              "content": "**Pre-requisites**:\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\n* You know how to create [Reports](/docs/library/report).\n  \n**Report.** To run a Preset on your data, comparing `current` data to `ref` data:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n])\n\nmy_eval = report.run(current, ref)\n```\n\n**Test Suite.** To add Tests with explicit pass/fail for each column:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```\n\n## Overview\n\nThe`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.&#x20;\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n* **Column drift.** Checks for shifts in each column. The [drift detection method](/metrics/explainer_drift) is chosen automatically based on the column type and number of observations.\n\n* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These ar",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_ca45zrMrsGMANjaWozhxxqg6",
          "metadata": null,
          "timestamp": "2025-11-03T13:12:32.905469+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 0,
              "content": "**Pre-requisites**:\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\n* You know how to create [Reports](/docs/library/report).\n  \n**Report.** To run a Preset on your data, comparing `current` data to `ref` data:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n])\n\nmy_eval = report.run(current, ref)\n```\n\n**Test Suite.** To add Tests with explicit pass/fail for each column:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```\n\n## Overview\n\nThe`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.&#x20;\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n* **Column drift.** Checks for shifts in each column. The [drift detection method](/metrics/explainer_drift) is chosen automatically based on the column type and number of observations.\n\n* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These ar",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_KzskwISpBMPsb2mtSVZPbMjD",
          "metadata": null,
          "timestamp": "2025-11-03T13:12:32.905414+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 0,
              "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_Je5maDWhH8Fh9g8vI6AvwWLK",
          "metadata": null,
          "timestamp": "2025-11-03T13:12:32.908769+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 0,
              "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 6000,
              "content": " obtain.\u00a0\n\n<Info>\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\n</Info>\n\nIf the drift is detected, Evidently will also calculate the\u00a0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words such as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://e",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 1000,
              "content": " observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\n</Note>\n\n## Dataset drift\n\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\n\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n## Tabular data drift\n\nThe following defaults apply for tabular data: numerical and categorical columns.\n\nFor **small data with \\<= 1000 observations** in the reference dataset:\n\n* For numerical columns (n\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipe",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_QtjjvwtvdOHjvK6vdXX3xVB3",
          "metadata": null,
          "timestamp": "2025-11-03T13:12:32.908731+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 1000,
              "content": "odular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\n\n* Start with local ad hoc checks.\u00a0\n\n* Want to share results? Add a UI to track evaluations over time.\u00a0\n\n* When you run evals, choose to upload raw data or only evaluation results. It\u2019s up to you.\u00a0\n\n* Add monitoring as you are ready to move to production workflows.\n\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \n\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\u00a0\n\n## 100+ built-in evaluations\n\nEvidently puts evaluations and quality testing first.\u00a0\n\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML and LLM use cases. From ranking metrics to data drift algorithms and LLM judges, we\u2019ve done the hard work by implementing metrics and ways to visualize them.\u00a0You can also easily extend Evidently by adding custom metrics.\u00a0\n\nEvidently Cloud also provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios.\n\n## Complete feature set\n\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows: with tracing, synthetic data, rich dashboards, built-in alerting etc.\n\nGet the [Platform overview](/docs/platform/overview).\n\n![](/images/dashboard_llm_tabs.gif)\n\n## Loved by community\n\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews).\n\nWe\u2019re also known for openly sharing knowledge that helps developers succeed. Check out resources like [LLM evaluation course](https://www.evidentlya",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            },
            {
              "start": 0,
              "content": "AI observability lets you evaluate the quality of the inputs and outputs of your AI application as it runs in production. This gives an up-to-date view of your system behavior and helps spot and fix issues.\n\nEvidently offers several ways to set up monitoring.\n\n## Batch monitoring jobs\n\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don\u2019t need instant quality evaluations.\n\n![](/images/monitoring_flow_batch.png)\n\n**How it works**:\n\n* **Build your evaluation pipeline**. Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow. Run it at regular intervals (e.g., hourly, daily) or trigger it when new data or labels arrive.\n\n* **Run metric calculations**. Implement the evaluation step in the pipeline using the Evidently Python library. Select the evals, and compute the `Reports` that will summarize data, metrics, and test results.\n\n* **Store and visualize the results**. Store the Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**. In this setup, does not store raw data or model predictions unless you choose to. By default, it only retains the aggregated data summaries and test results. This protects data privacy and avoids duplicating logs if they\u2019re already stored elsewhere, like for retraining.\n\n* **Full control over the evaluation pipeline**. You decide when evaluations happen. This setup is great for batch ML models, where you can easily add monitoring as another step in your existing pipeline. For online inference, you can log your predictions to a database and set up separate monitoring jobs to query data at intervals.\n\n* **Fits most ML evaluation scenarios**. Many evaluations, like data drift detection,",
              "title": "Overview",
              "description": "How production AI quality monitoring works.",
              "filename": "docs/platform/monitoring_overview.mdx"
            },
            {
              "start": 0,
              "content": "We\u2019re building Evidently AI to help teams ship reliable AI products: whether it\u2019s an ML model, an LLM app, or a complex agent workflow.\n\nOur tools are model-, framework-, and application-agnostic, so you can build and evaluate AI systems your way without limitations.\n\n## We are open-source\n\n[**Evidently**](https://github.com/evidentlyai/evidently) is an open-source library with over 25 million downloads, 5000+ GitHub stars, and a thriving community. It's licensed under Apache 2.0. This gives full transparency - you can see exactly how every metric works and trust the implementation. It also delivers an intuitive API designed for a great developer experience.\n\nThe **Evidently Platform** builds on the library with additional UI features and workflows for team collaboration. For enterprise users, we offer both Cloud and self-hosted options for full data privacy and control.\n\n## Evidently is very modular\u00a0\n\nEvidently is built to adapt to your needs without lock-ins or complex setups. It\u2019s modular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\n\n* Start with local ad hoc checks.\u00a0\n\n* Want to share results? Add a UI to track evaluations over time.\u00a0\n\n* When you run evals, choose to upload raw data or only evaluation results. It\u2019s up to you.\u00a0\n\n* Add monitoring as you are ready to move to production workflows.\n\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \n\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\u00a0\n\n## 100+ built-in evaluations\n\nEvidently puts evaluations and quality testing first.\u00a0\n\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML an",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            },
            {
              "start": 1000,
              "content": " `Reports` that will summarize data, metrics, and test results.\n\n* **Store and visualize the results**. Store the Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**. In this setup, does not store raw data or model predictions unless you choose to. By default, it only retains the aggregated data summaries and test results. This protects data privacy and avoids duplicating logs if they\u2019re already stored elsewhere, like for retraining.\n\n* **Full control over the evaluation pipeline**. You decide when evaluations happen. This setup is great for batch ML models, where you can easily add monitoring as another step in your existing pipeline. For online inference, you can log your predictions to a database and set up separate monitoring jobs to query data at intervals.\n\n* **Fits most ML evaluation scenarios**. Many evaluations, like data drift detection, naturally work in batches since you need to collect a set of new data points before running them. Model quality checks often only happen when new labeled data arrives, which can be delayed. Analyzing prediction or user behavior shifts is also usually more meaningful when done at intervals like hourly or daily rather than recalculating after every single event.\n\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch).\n\n## Tracing with scheduled evals\n\n<Check>\n  Supported in: `Evidently Cloud` and `Evidently Enterprise`. Scheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\n</Check>\n\n**Best for**: LLM-powered applications\n\n![](/images/monitoring_flow_tracing.png)\n\n**How it works:**\n\n* **Instrument your app**. Use the `Tracely` library to capture all relevant data from your application, including inputs, outputs and intermediate steps. ([Tracing](/docs/platform/tracing_setup)).\n\n* **Store raw data**. Evidently Platform stores al",
              "title": "Overview",
              "description": "How production AI quality monitoring works.",
              "filename": "docs/platform/monitoring_overview.mdx"
            },
            {
              "start": 2000,
              "content": "d LLM use cases. From ranking metrics to data drift algorithms and LLM judges, we\u2019ve done the hard work by implementing metrics and ways to visualize them.\u00a0You can also easily extend Evidently by adding custom metrics.\u00a0\n\nEvidently Cloud also provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios.\n\n## Complete feature set\n\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows: with tracing, synthetic data, rich dashboards, built-in alerting etc.\n\nGet the [Platform overview](/docs/platform/overview).\n\n![](/images/dashboard_llm_tabs.gif)\n\n## Loved by community\n\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews).\n\nWe\u2019re also known for openly sharing knowledge that helps developers succeed. Check out resources like [LLM evaluation course](https://www.evidentlyai.com/llm-evaluations-course), open-source [ML observability course](https://www.evidentlyai.com/ml-observability-course), [guides](https://www.evidentlyai.com/mlops-guides), and [blogs](https://www.evidentlyai.com/blog).\n\n## Handles both ML and LLM\u00a0\n\nEvidently supports both ML and LLM tasks. We believe this matters even if you\u2019re focused solely on LLMs and not training your models.\n\nReal-world AI systems are rarely just one thing, and two types of workflows overlap. For example:\n\n* an LLM-based chatbot may need **classification** steps like detecting user intent.\n\n* if you are building with RAG, you are solving a **ranking** problem first.\n\nThe Evidently Platform supports both complex nested workflows and structured tabular data, providing relevant metrics and views for each. This means you won't be locked into a single approach - or have to reinvent the wheel to measure things like Hit Rate or Precision over traces.\n\n## Built for collaboration\n\nEvidently started as an open-source pro",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            }
          ],
          "tool_call_id": "call_n7Q9KK2o3CI6n9TJTEbsifhz",
          "metadata": null,
          "timestamp": "2025-11-03T13:12:32.911519+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 0,
              "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_gEhGKtKEOpNquN3tKBzcJPtG",
          "metadata": null,
          "timestamp": "2025-11-03T13:12:32.912034+00:00",
          "part_kind": "tool-return"
        },
        {
          "content": "System message: The maximal number of searches has exceeded 6. Proceed to finishing the writeup",
          "timestamp": "2025-11-03T13:12:32.912697+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "args": "{\"found_answer\":true,\"title\":\"Monitoring Data Drift in Production\",\"sections\":[{\"heading\":\"Overview of Data Drift Monitoring\",\"content\":\"Monitoring data drift in production is crucial for maintaining the performance of machine learning models. Data drift occurs when the statistical properties of the input data change, potentially leading to model degradation. Here are key practices for monitoring data drift effectively:\\n\\n1. **Use Data Drift Detection Tools**: Tools like Evidently allow you to evaluate shifts in data distribution. With the `DataDriftPreset`, you can compare current data against reference data to identify significant changes in distributions across features.\\n\\n2. **Set Up Evaluation Pipelines**: Create a structured pipeline to run monitoring jobs at regular intervals. This can be integrated into existing ML workflows and can trigger alerts when drift is detected.\\n\\n3. **Utilize Proxy Metrics**: When true labels are not available, monitor feature drift and prediction drift as proxy metrics. If either drift is detected, it may indicate a need for retraining or labeling new data.\\n\\n4. **Customize Thresholds and Parameters**: You can adjust detection thresholds and select appropriate statistical tests based on your data\u2019s characteristics to improve the accuracy of drift detection.\\n\\n5. **Analyze Historical Data**: Investigate historical data drift to understand patterns and refine retraining strategies.\\n\\n6. **Dashboard for Visualization**: Store evaluation results and make them available on a dashboard for easier analysis and trend monitoring.\" ,\"references\":[{\"title\":\"Data Drift\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"Data Drift Detection Techniques\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Monitoring in Production\",\"filename\":\"docs/platform/monitoring_overview.mdx\"}]}],\"references\":[{\"title\":\"Monitoring in Production\",\"filename\":\"docs/platform/monitoring_overview.mdx\"},{\"title\":\"Data Drift\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"How Data Drift Detection Works\",\"filename\":\"metrics/explainer_drift.mdx\"}]}",
          "tool_call_id": "call_5q7EZuaj02h5CowjBW20bUfB",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 15500,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 421,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-03T13:12:33+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CXoeH1RotTAEZz2lBcElVtgRxqdRu",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "content": "Final result processed.",
          "tool_call_id": "call_5q7EZuaj02h5CowjBW20bUfB",
          "metadata": null,
          "timestamp": "2025-11-03T13:12:44.483027+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": null,
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "args": "{\"found_answer\":true,\"title\":\"Monitoring Data Drift in Production\",\"sections\":[{\"heading\":\"Overview of Data Drift Monitoring\",\"content\":\"Monitoring data drift in production is crucial for maintaining the performance of machine learning models. Data drift occurs when the statistical properties of the input data change, potentially leading to model degradation. Here are key practices for monitoring data drift effectively:\\n\\n1. **Use Data Drift Detection Tools**: Tools like Evidently allow you to evaluate shifts in data distribution. With the `DataDriftPreset`, you can compare current data against reference data to identify significant changes in distributions across features.\\n\\n2. **Set Up Evaluation Pipelines**: Create a structured pipeline to run monitoring jobs at regular intervals. This can be integrated into existing ML workflows and can trigger alerts when drift is detected.\\n\\n3. **Utilize Proxy Metrics**: When true labels are not available, monitor feature drift and prediction drift as proxy metrics. If either drift is detected, it may indicate a need for retraining or labeling new data.\\n\\n4. **Customize Thresholds and Parameters**: You can adjust detection thresholds and select appropriate statistical tests based on your data\u2019s characteristics to improve the accuracy of drift detection.\\n\\n5. **Analyze Historical Data**: Investigate historical data drift to understand patterns and refine retraining strategies.\\n\\n6. **Dashboard for Visualization**: Store evaluation results and make them available on a dashboard for easier analysis and trend monitoring.\" ,\"references\":[{\"title\":\"Data Drift\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"Data Drift Detection Techniques\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Monitoring in Production\",\"filename\":\"docs/platform/monitoring_overview.mdx\"}]}],\"references\":[{\"title\":\"Monitoring in Production\",\"filename\":\"docs/platform/monitoring_overview.mdx\"},{\"title\":\"Data Drift\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"How Data Drift Detection Works\",\"filename\":\"metrics/explainer_drift.mdx\"}]}",
          "tool_call_id": "call_5q7EZuaj02h5CowjBW20bUfB",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 15500,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 421,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-03T13:12:33+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CXoeH1RotTAEZz2lBcElVtgRxqdRu",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "content": "Final result processed.",
          "tool_call_id": "call_5q7EZuaj02h5CowjBW20bUfB",
          "metadata": null,
          "timestamp": "2025-11-03T13:12:44.483174+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": null,
      "kind": "request"
    }
  ],
  "usage": {
    "input_tokens": 16284,
    "cache_write_tokens": 0,
    "cache_read_tokens": 0,
    "output_tokens": 546,
    "input_audio_tokens": 0,
    "cache_audio_read_tokens": 0,
    "output_audio_tokens": 0,
    "details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "requests": 2,
    "tool_calls": 6
  },
  "output": {
    "found_answer": true,
    "title": "Monitoring Data Drift in Production",
    "sections": [
      {
        "heading": "Overview of Data Drift Monitoring",
        "content": "Monitoring data drift in production is crucial for maintaining the performance of machine learning models. Data drift occurs when the statistical properties of the input data change, potentially leading to model degradation. Here are key practices for monitoring data drift effectively:\n\n1. **Use Data Drift Detection Tools**: Tools like Evidently allow you to evaluate shifts in data distribution. With the `DataDriftPreset`, you can compare current data against reference data to identify significant changes in distributions across features.\n\n2. **Set Up Evaluation Pipelines**: Create a structured pipeline to run monitoring jobs at regular intervals. This can be integrated into existing ML workflows and can trigger alerts when drift is detected.\n\n3. **Utilize Proxy Metrics**: When true labels are not available, monitor feature drift and prediction drift as proxy metrics. If either drift is detected, it may indicate a need for retraining or labeling new data.\n\n4. **Customize Thresholds and Parameters**: You can adjust detection thresholds and select appropriate statistical tests based on your data\u2019s characteristics to improve the accuracy of drift detection.\n\n5. **Analyze Historical Data**: Investigate historical data drift to understand patterns and refine retraining strategies.\n\n6. **Dashboard for Visualization**: Store evaluation results and make them available on a dashboard for easier analysis and trend monitoring.",
        "references": [
          {
            "title": "Data Drift",
            "filename": "metrics/preset_data_drift.mdx"
          },
          {
            "title": "Data Drift Detection Techniques",
            "filename": "metrics/explainer_drift.mdx"
          },
          {
            "title": "Monitoring in Production",
            "filename": "docs/platform/monitoring_overview.mdx"
          }
        ]
      }
    ],
    "references": [
      {
        "title": "Monitoring in Production",
        "filename": "docs/platform/monitoring_overview.mdx"
      },
      {
        "title": "Data Drift",
        "filename": "metrics/preset_data_drift.mdx"
      },
      {
        "title": "How Data Drift Detection Works",
        "filename": "metrics/explainer_drift.mdx"
      }
    ]
  }
}