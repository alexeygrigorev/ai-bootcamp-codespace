{
  "agent_name": "search",
  "system_prompt": [
    "\nYou are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.\n"
  ],
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search",
    "read_file"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How do I monitor data drift in production?",
          "timestamp": "2025-11-04T10:18:16.933707+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\": \"monitoring data drift in production with Evidently\"}",
          "tool_call_id": "call_ssymnr5x4qxlQkSva5uJsoDi",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"data drift detection using Evidently\"}",
          "tool_call_id": "call_uxooJfjX2ZutM1QvV2c6Xywp",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"Evidently data quality monitoring\"}",
          "tool_call_id": "call_fmpCDGsawFYamVzzq1p89oPm",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"Evidently monitoring data\"}",
          "tool_call_id": "call_f0pgdH9zYRyx1jd23bbJwkvx",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"Evidently data drift evaluation metrics\"}",
          "tool_call_id": "call_NBE7bvuB945jDaptHTO4QfKF",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 784,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 109,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-04T10:18:18+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CY8PC9d0W5nZmchqk1mfKStVpn2BA",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 0,
              "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            }
          ],
          "tool_call_id": "call_ssymnr5x4qxlQkSva5uJsoDi",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:21.268106+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 6000,
              "content": " obtain.\u00a0\n\n<Info>\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\n</Info>\n\nIf the drift is detected, Evidently will also calculate the\u00a0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words such as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://e",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 0,
              "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 1000,
              "content": " observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\n</Note>\n\n## Dataset drift\n\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\n\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n## Tabular data drift\n\nThe following defaults apply for tabular data: numerical and categorical columns.\n\nFor **small data with \\<= 1000 observations** in the reference dataset:\n\n* For numerical columns (n\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipe",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_uxooJfjX2ZutM1QvV2c6Xywp",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:21.261414+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 1000,
              "content": "odular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\n\n* Start with local ad hoc checks.\u00a0\n\n* Want to share results? Add a UI to track evaluations over time.\u00a0\n\n* When you run evals, choose to upload raw data or only evaluation results. It\u2019s up to you.\u00a0\n\n* Add monitoring as you are ready to move to production workflows.\n\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \n\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\u00a0\n\n## 100+ built-in evaluations\n\nEvidently puts evaluations and quality testing first.\u00a0\n\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML and LLM use cases. From ranking metrics to data drift algorithms and LLM judges, we\u2019ve done the hard work by implementing metrics and ways to visualize them.\u00a0You can also easily extend Evidently by adding custom metrics.\u00a0\n\nEvidently Cloud also provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios.\n\n## Complete feature set\n\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows: with tracing, synthetic data, rich dashboards, built-in alerting etc.\n\nGet the [Platform overview](/docs/platform/overview).\n\n![](/images/dashboard_llm_tabs.gif)\n\n## Loved by community\n\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews).\n\nWe\u2019re also known for openly sharing knowledge that helps developers succeed. Check out resources like [LLM evaluation course](https://www.evidentlya",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            },
            {
              "start": 0,
              "content": "AI observability lets you evaluate the quality of the inputs and outputs of your AI application as it runs in production. This gives an up-to-date view of your system behavior and helps spot and fix issues.\n\nEvidently offers several ways to set up monitoring.\n\n## Batch monitoring jobs\n\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don\u2019t need instant quality evaluations.\n\n![](/images/monitoring_flow_batch.png)\n\n**How it works**:\n\n* **Build your evaluation pipeline**. Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow. Run it at regular intervals (e.g., hourly, daily) or trigger it when new data or labels arrive.\n\n* **Run metric calculations**. Implement the evaluation step in the pipeline using the Evidently Python library. Select the evals, and compute the `Reports` that will summarize data, metrics, and test results.\n\n* **Store and visualize the results**. Store the Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**. In this setup, does not store raw data or model predictions unless you choose to. By default, it only retains the aggregated data summaries and test results. This protects data privacy and avoids duplicating logs if they\u2019re already stored elsewhere, like for retraining.\n\n* **Full control over the evaluation pipeline**. You decide when evaluations happen. This setup is great for batch ML models, where you can easily add monitoring as another step in your existing pipeline. For online inference, you can log your predictions to a database and set up separate monitoring jobs to query data at intervals.\n\n* **Fits most ML evaluation scenarios**. Many evaluations, like data drift detection,",
              "title": "Overview",
              "description": "How production AI quality monitoring works.",
              "filename": "docs/platform/monitoring_overview.mdx"
            },
            {
              "start": 0,
              "content": "We\u2019re building Evidently AI to help teams ship reliable AI products: whether it\u2019s an ML model, an LLM app, or a complex agent workflow.\n\nOur tools are model-, framework-, and application-agnostic, so you can build and evaluate AI systems your way without limitations.\n\n## We are open-source\n\n[**Evidently**](https://github.com/evidentlyai/evidently) is an open-source library with over 25 million downloads, 5000+ GitHub stars, and a thriving community. It's licensed under Apache 2.0. This gives full transparency - you can see exactly how every metric works and trust the implementation. It also delivers an intuitive API designed for a great developer experience.\n\nThe **Evidently Platform** builds on the library with additional UI features and workflows for team collaboration. For enterprise users, we offer both Cloud and self-hosted options for full data privacy and control.\n\n## Evidently is very modular\u00a0\n\nEvidently is built to adapt to your needs without lock-ins or complex setups. It\u2019s modular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\n\n* Start with local ad hoc checks.\u00a0\n\n* Want to share results? Add a UI to track evaluations over time.\u00a0\n\n* When you run evals, choose to upload raw data or only evaluation results. It\u2019s up to you.\u00a0\n\n* Add monitoring as you are ready to move to production workflows.\n\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \n\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\u00a0\n\n## 100+ built-in evaluations\n\nEvidently puts evaluations and quality testing first.\u00a0\n\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML an",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            },
            {
              "start": 1000,
              "content": " `Reports` that will summarize data, metrics, and test results.\n\n* **Store and visualize the results**. Store the Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**. In this setup, does not store raw data or model predictions unless you choose to. By default, it only retains the aggregated data summaries and test results. This protects data privacy and avoids duplicating logs if they\u2019re already stored elsewhere, like for retraining.\n\n* **Full control over the evaluation pipeline**. You decide when evaluations happen. This setup is great for batch ML models, where you can easily add monitoring as another step in your existing pipeline. For online inference, you can log your predictions to a database and set up separate monitoring jobs to query data at intervals.\n\n* **Fits most ML evaluation scenarios**. Many evaluations, like data drift detection, naturally work in batches since you need to collect a set of new data points before running them. Model quality checks often only happen when new labeled data arrives, which can be delayed. Analyzing prediction or user behavior shifts is also usually more meaningful when done at intervals like hourly or daily rather than recalculating after every single event.\n\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch).\n\n## Tracing with scheduled evals\n\n<Check>\n  Supported in: `Evidently Cloud` and `Evidently Enterprise`. Scheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\n</Check>\n\n**Best for**: LLM-powered applications\n\n![](/images/monitoring_flow_tracing.png)\n\n**How it works:**\n\n* **Instrument your app**. Use the `Tracely` library to capture all relevant data from your application, including inputs, outputs and intermediate steps. ([Tracing](/docs/platform/tracing_setup)).\n\n* **Store raw data**. Evidently Platform stores al",
              "title": "Overview",
              "description": "How production AI quality monitoring works.",
              "filename": "docs/platform/monitoring_overview.mdx"
            },
            {
              "start": 2000,
              "content": "d LLM use cases. From ranking metrics to data drift algorithms and LLM judges, we\u2019ve done the hard work by implementing metrics and ways to visualize them.\u00a0You can also easily extend Evidently by adding custom metrics.\u00a0\n\nEvidently Cloud also provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios.\n\n## Complete feature set\n\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows: with tracing, synthetic data, rich dashboards, built-in alerting etc.\n\nGet the [Platform overview](/docs/platform/overview).\n\n![](/images/dashboard_llm_tabs.gif)\n\n## Loved by community\n\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews).\n\nWe\u2019re also known for openly sharing knowledge that helps developers succeed. Check out resources like [LLM evaluation course](https://www.evidentlyai.com/llm-evaluations-course), open-source [ML observability course](https://www.evidentlyai.com/ml-observability-course), [guides](https://www.evidentlyai.com/mlops-guides), and [blogs](https://www.evidentlyai.com/blog).\n\n## Handles both ML and LLM\u00a0\n\nEvidently supports both ML and LLM tasks. We believe this matters even if you\u2019re focused solely on LLMs and not training your models.\n\nReal-world AI systems are rarely just one thing, and two types of workflows overlap. For example:\n\n* an LLM-based chatbot may need **classification** steps like detecting user intent.\n\n* if you are building with RAG, you are solving a **ranking** problem first.\n\nThe Evidently Platform supports both complex nested workflows and structured tabular data, providing relevant metrics and views for each. This means you won't be locked into a single approach - or have to reinvent the wheel to measure things like Hit Rate or Precision over traces.\n\n## Built for collaboration\n\nEvidently started as an open-source pro",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            }
          ],
          "tool_call_id": "call_fmpCDGsawFYamVzzq1p89oPm",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:21.289427+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 1000,
              "content": "odular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\n\n* Start with local ad hoc checks.\u00a0\n\n* Want to share results? Add a UI to track evaluations over time.\u00a0\n\n* When you run evals, choose to upload raw data or only evaluation results. It\u2019s up to you.\u00a0\n\n* Add monitoring as you are ready to move to production workflows.\n\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \n\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\u00a0\n\n## 100+ built-in evaluations\n\nEvidently puts evaluations and quality testing first.\u00a0\n\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML and LLM use cases. From ranking metrics to data drift algorithms and LLM judges, we\u2019ve done the hard work by implementing metrics and ways to visualize them.\u00a0You can also easily extend Evidently by adding custom metrics.\u00a0\n\nEvidently Cloud also provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios.\n\n## Complete feature set\n\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows: with tracing, synthetic data, rich dashboards, built-in alerting etc.\n\nGet the [Platform overview](/docs/platform/overview).\n\n![](/images/dashboard_llm_tabs.gif)\n\n## Loved by community\n\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews).\n\nWe\u2019re also known for openly sharing knowledge that helps developers succeed. Check out resources like [LLM evaluation course](https://www.evidentlya",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            },
            {
              "start": 0,
              "content": "We\u2019re building Evidently AI to help teams ship reliable AI products: whether it\u2019s an ML model, an LLM app, or a complex agent workflow.\n\nOur tools are model-, framework-, and application-agnostic, so you can build and evaluate AI systems your way without limitations.\n\n## We are open-source\n\n[**Evidently**](https://github.com/evidentlyai/evidently) is an open-source library with over 25 million downloads, 5000+ GitHub stars, and a thriving community. It's licensed under Apache 2.0. This gives full transparency - you can see exactly how every metric works and trust the implementation. It also delivers an intuitive API designed for a great developer experience.\n\nThe **Evidently Platform** builds on the library with additional UI features and workflows for team collaboration. For enterprise users, we offer both Cloud and self-hosted options for full data privacy and control.\n\n## Evidently is very modular\u00a0\n\nEvidently is built to adapt to your needs without lock-ins or complex setups. It\u2019s modular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\n\n* Start with local ad hoc checks.\u00a0\n\n* Want to share results? Add a UI to track evaluations over time.\u00a0\n\n* When you run evals, choose to upload raw data or only evaluation results. It\u2019s up to you.\u00a0\n\n* Add monitoring as you are ready to move to production workflows.\n\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \n\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\u00a0\n\n## 100+ built-in evaluations\n\nEvidently puts evaluations and quality testing first.\u00a0\n\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML an",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            },
            {
              "start": 2000,
              "content": "d LLM use cases. From ranking metrics to data drift algorithms and LLM judges, we\u2019ve done the hard work by implementing metrics and ways to visualize them.\u00a0You can also easily extend Evidently by adding custom metrics.\u00a0\n\nEvidently Cloud also provides advanced testing features, including synthetic data generation and adversarial testing, allowing you to easily create and run test scenarios.\n\n## Complete feature set\n\nWhy evals are core, the Evidently Platform offers a comprehensive feature set to support AI quality workflows: with tracing, synthetic data, rich dashboards, built-in alerting etc.\n\nGet the [Platform overview](/docs/platform/overview).\n\n![](/images/dashboard_llm_tabs.gif)\n\n## Loved by community\n\nThousands of companies, from startups to enterprises, use Evidently. Check some of [our reviews](https://www.evidentlyai.com/reviews).\n\nWe\u2019re also known for openly sharing knowledge that helps developers succeed. Check out resources like [LLM evaluation course](https://www.evidentlyai.com/llm-evaluations-course), open-source [ML observability course](https://www.evidentlyai.com/ml-observability-course), [guides](https://www.evidentlyai.com/mlops-guides), and [blogs](https://www.evidentlyai.com/blog).\n\n## Handles both ML and LLM\u00a0\n\nEvidently supports both ML and LLM tasks. We believe this matters even if you\u2019re focused solely on LLMs and not training your models.\n\nReal-world AI systems are rarely just one thing, and two types of workflows overlap. For example:\n\n* an LLM-based chatbot may need **classification** steps like detecting user intent.\n\n* if you are building with RAG, you are solving a **ranking** problem first.\n\nThe Evidently Platform supports both complex nested workflows and structured tabular data, providing relevant metrics and views for each. This means you won't be locked into a single approach - or have to reinvent the wheel to measure things like Hit Rate or Precision over traces.\n\n## Built for collaboration\n\nEvidently started as an open-source pro",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            },
            {
              "start": 0,
              "content": "## 1. Create an Account\n\n- If not yet, sign up for a [free Evidently Cloud account](https://app.evidently.cloud/signup).\n- After logging in, create an **Organization** and name it.\n\n## 2. Connect from Python\n\n<Info>\n  You need this for programmatic tasks like tracing or logging local evals. Many other tasks can be done directly on the platform.\n</Info>\n\n### Get a Token\n\nClick the **Key** menu icon to open the [Token page](https://app.evidently.cloud/token). Generate and save token securely.\n\n### Install Evidently\n\n[Install](/docs/setup/installation) the Evidently Python library.\n\n```python\npip install evidently ## or pip install evidently[llm]\n```\n\n### Connect\n\nImport the cloud workspace and pass your API token to connect:\n\n```python\nfrom evidently.ui.workspace import CloudWorkspace\n\nws = CloudWorkspace(\ntoken=\"API_KEY\",\nurl=\"https://app.evidently.cloud\")\n```\n\n<Note>\nFor Evidently 0.6.7 and Evidently Cloud v1, use `from evidently.ui.workspace.cloud import CloudWorkspace`. [Read more](/faq/cloud_v2).\n</Note>\n\n<Check>\n  You can also provide the API key by setting the environment variable\u00a0`EVIDENTLY_API_KEY`.\n</Check>\n\nYou are all set\\! Create a Project and run your first [evaluation](/quickstart_llm).",
              "title": "Evidently Cloud",
              "description": "How to set up Evidently Cloud account.",
              "icon": "cloud",
              "filename": "docs/setup/cloud.mdx"
            },
            {
              "start": 3000,
              "content": "i.com/llm-evaluations-course), open-source [ML observability course](https://www.evidentlyai.com/ml-observability-course), [guides](https://www.evidentlyai.com/mlops-guides), and [blogs](https://www.evidentlyai.com/blog).\n\n## Handles both ML and LLM\u00a0\n\nEvidently supports both ML and LLM tasks. We believe this matters even if you\u2019re focused solely on LLMs and not training your models.\n\nReal-world AI systems are rarely just one thing, and two types of workflows overlap. For example:\n\n* an LLM-based chatbot may need **classification** steps like detecting user intent.\n\n* if you are building with RAG, you are solving a **ranking** problem first.\n\nThe Evidently Platform supports both complex nested workflows and structured tabular data, providing relevant metrics and views for each. This means you won't be locked into a single approach - or have to reinvent the wheel to measure things like Hit Rate or Precision over traces.\n\n## Built for collaboration\n\nEvidently started as an open-source project loved by data scientists and AI/ML engineers. But we\u2019re building more than a developer tool - we\u2019re building a platform where domain experts and engineers can work together easily.\n\nReliable AI systems require teams to work together: on curating test data, gathering feedback, and running evaluations. We build our platform with this in mind: combine **no-code** workflows for non-technical users with an intuitive **API**. Everyone gets what they need to do their best work.\n\n## Trusted partner\u00a0\n\nFounded in 2021, Evidently AI is built by a team with 10+ years of experience deploying AI in high-scale, critical scenarios. We are backed by world-class investors like Y Combinator, Fly Ventures, Runa Capital, Nauta Capital and angel investors. Our core Evidently library has a stable history of development and earned trust from the community and enterprise users alike.",
              "title": "Why Evidently?",
              "description": "Why choose Evidently.",
              "filename": "faq/why_evidently.mdx"
            }
          ],
          "tool_call_id": "call_f0pgdH9zYRyx1jd23bbJwkvx",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:21.277787+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 0,
              "content": "**Pre-requisites**:\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\n* You know how to create [Reports](/docs/library/report).\n  \n**Report.** To run a Preset on your data, comparing `current` data to `ref` data:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n])\n\nmy_eval = report.run(current, ref)\n```\n\n**Test Suite.** To add Tests with explicit pass/fail for each column:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```\n\n## Overview\n\nThe`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.&#x20;\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n* **Column drift.** Checks for shifts in each column. The [drift detection method](/metrics/explainer_drift) is chosen automatically based on the column type and number of observations.\n\n* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These ar",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_NBE7bvuB945jDaptHTO4QfKF",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:21.284571+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "read_file",
          "args": "{\"filename\": \"metrics/preset_data_drift.mdx\"}",
          "tool_call_id": "call_4zr44l2s5LtZXRftV4lFC5OY",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "read_file",
          "args": "{\"filename\": \"metrics/explainer_drift.mdx\"}",
          "tool_call_id": "call_GzDGxLSyyvMDQxS13CIpooRL",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "read_file",
          "args": "{\"filename\": \"docs/platform/monitoring_overview.mdx\"}",
          "tool_call_id": "call_gH5JEUiSxaUPunFzNNXW5SRe",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 13083,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 80,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-04T10:18:22+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CY8PGfjyt2HHQVMXa37vAiJcvbs3C",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "read_file",
          "content": "**Pre-requisites**:\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\n* You know how to create [Reports](/docs/library/report).\n  \n**Report.** To run a Preset on your data, comparing `current` data to `ref` data:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n])\n\nmy_eval = report.run(current, ref)\n```\n\n**Test Suite.** To add Tests with explicit pass/fail for each column:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```\n\n## Overview\n\nThe`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.&#x20;\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n* **Column drift.** Checks for shifts in each column. The [drift detection method](/metrics/explainer_drift) is chosen automatically based on the column type and number of observations.\n\n* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\"prediction\")` to your Report so that you see the drift in this value in a separate widget.\n\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\n\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\n\n<Info>\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\n</Info>",
          "tool_call_id": "call_4zr44l2s5LtZXRftV4lFC5OY",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:25.584003+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "read_file",
          "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\n</Note>\n\n## Dataset drift\n\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\n\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n## Tabular data drift\n\nThe following defaults apply for tabular data: numerical and categorical columns.\n\nFor **small data with \\<= 1000 observations** in the reference dataset:\n\n* For numerical columns (n\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\n\n* For categorical columns or numerical columns with n\\_unique \\<= 5: [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).\n\n* For binary categorical features (n\\_unique \\<= 2): proportion difference test for independent samples based on Z-score.\n\n<Info>\n  All tests use a 0.95 confidence level by default. Drift score is P-value. (=\\< 0.05 means drift).\n</Info>\n\nFor **larger data with > 1000 observations** in the reference dataset:\n\n* For numerical columns (n\\_unique > 5):[Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\n\n* For categorical columns or numerical with n\\_unique \\<= 5):[Jensen--Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\n\n<Info>\n  All metrics use a threshold = 0.1 by default. Drift score is distance/divergence. (>= 0.1 means drift).\n</Info>\n\n**You can modify this drift detection logic**. You can select any method available in the library (PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc.), specify thresholds, or pass a custom test. Read more about [data drift parameters and available methods](/metrics/customize_data_drift).\n\n**Exploring drift.** You can see the distribution of each individual column inside the `DataDriftPreset` or using `ValueDrift` metric:\n\n![](/images/metrics/preset_data_drift-min.png)\n\nFor numerical features, you can also explore the values mapped in a plot.\n\n* The dark green line is the **mean**, as seen in the reference dataset.\n\n* The green area covers **one standard deviation** from the mean.\n\n![](/images/metrics/preset_data_drift_3-min.png)\n\nIndex is binned to 150 or uses timestamp if provided.&#x20;\n\n## Text data drift\n\nText content drift using a **domain classifier**. Evidently trains a binary classification model to discriminate between data from reference and current distributions.&#x20;\n\n![](/images/concepts/text_data_drift_domain_classifier.png)\n\nIf the model can confidently identify which text samples belong to the \u201cnewer\u201d data, you can consider that the two datasets are significantly different.\n\n<Info>\n  You can read more about the domain classifier approach in the\u00a0[paper\u00a0](https://arxiv.org/pdf/1810.11953.pdf)\u201cFailing Loudly: An Empirical Study of Methods for Detecting Dataset Shift.\u201d\n</Info>\n\nThe drift score in this case is the ROC AUC of the resulting classifier.\n\nThe default for **larger data with > 1000 observations** detects drift if the ROC AUC > 0.55. The ROC AUC of the obtained classifier is directly compared against the set ROC AUC threshold. You can set a different threshold as a parameter.\n\nThe default for **small data with \\<= 1000 observations** detects drift if the ROC AUC of the drift detection classifier > possible ROC AUC of the random classifier at a 95th percentile. This approach\u00a0**protects against false positive**\u00a0drift results for small datasets since we explicitly compare the classifier score against the \u201cbest random score\u201d we could obtain.\u00a0\n\n<Info>\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\n</Info>\n\nIf the drift is detected, Evidently will also calculate the\u00a0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words such as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
          "tool_call_id": "call_GzDGxLSyyvMDQxS13CIpooRL",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:25.581553+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "read_file",
          "content": "AI observability lets you evaluate the quality of the inputs and outputs of your AI application as it runs in production. This gives an up-to-date view of your system behavior and helps spot and fix issues.\n\nEvidently offers several ways to set up monitoring.\n\n## Batch monitoring jobs\n\n<Check>\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\n</Check>\n\n**Best for**: batch ML pipelines, regression testing, and near real-time ML systems that don\u2019t need instant quality evaluations.\n\n![](/images/monitoring_flow_batch.png)\n\n**How it works**:\n\n* **Build your evaluation pipeline**. Create a pipeline in your infrastructure to run monitoring jobs. This can be a Python script, cron job, or orchestrated with a tool like Airflow. Run it at regular intervals (e.g., hourly, daily) or trigger it when new data or labels arrive.\n\n* **Run metric calculations**. Implement the evaluation step in the pipeline using the Evidently Python library. Select the evals, and compute the `Reports` that will summarize data, metrics, and test results.\n\n* **Store and visualize the results**. Store the Report runs in Evidently Cloud or in a designated self-hosted workspace, and monitor results on a Dashboard.\n\n**Benefits of this approach**:\n\n* **Decouples log storage and monitoring metrics**. In this setup, does not store raw data or model predictions unless you choose to. By default, it only retains the aggregated data summaries and test results. This protects data privacy and avoids duplicating logs if they\u2019re already stored elsewhere, like for retraining.\n\n* **Full control over the evaluation pipeline**. You decide when evaluations happen. This setup is great for batch ML models, where you can easily add monitoring as another step in your existing pipeline. For online inference, you can log your predictions to a database and set up separate monitoring jobs to query data at intervals.\n\n* **Fits most ML evaluation scenarios**. Many evaluations, like data drift detection, naturally work in batches since you need to collect a set of new data points before running them. Model quality checks often only happen when new labeled data arrives, which can be delayed. Analyzing prediction or user behavior shifts is also usually more meaningful when done at intervals like hourly or daily rather than recalculating after every single event.\n\n**Next step:** check the [batch monitoring docs](/docs/platform/monitoring_local_batch).\n\n## Tracing with scheduled evals\n\n<Check>\n  Supported in: `Evidently Cloud` and `Evidently Enterprise`. Scheduled evaluations are in beta on Evidently Cloud. Contact our team to try it.\n</Check>\n\n**Best for**: LLM-powered applications\n\n![](/images/monitoring_flow_tracing.png)\n\n**How it works:**\n\n* **Instrument your app**. Use the `Tracely` library to capture all relevant data from your application, including inputs, outputs and intermediate steps. ([Tracing](/docs/platform/tracing_setup)).\n\n* **Store raw data**. Evidently Platform stores all raw data, providing a complete record of activity.\n\n* **Schedule evaluations**. Set up evaluations to run automatically at scheduled times. This will generate Reports or run Tests directly on the Evidently Platform. You can also manually run evaluations anytime to assess individual outputs.\n\n**Benefits of this approach**:\n\n* **Solves the data capture**. You collect complex traces and all production data in one place, making it easier to manage and analyze.\n\n* **Easy to re-run evals**. With raw traces stored on the platform, you can easily re-run evaluations or add new metrics whenever needed.\n\n* **No-code**. Once your trace instrumentation is set up, you can manage everything from the UI.\n\n**Next step:** check the [Tracing Quickstart](/quickstart_tracing).",
          "tool_call_id": "call_gH5JEUiSxaUPunFzNNXW5SRe",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:25.579038+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": "You are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "args": "{\"found_answer\":true,\"title\":\"Monitoring Data Drift in Production\",\"sections\":[{\"heading\":\"Overview\",\"content\":\"In production, monitoring data drift is essential for maintaining the performance and reliability of your machine learning models. Data drift refers to the changes in the distribution of input data over time, which can adversely affect model predictions and lead to quality degradation. Evidently provides a framework for effectively monitoring data drift using various metrics and reports.\",\"references\":[{\"title\":\"Data Drift Overview\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"How Data Drift Detection Works\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Monitoring Overview\",\"filename\":\"docs/platform/monitoring_overview.mdx\"}]},{\"heading\":\"Methodology\",\"content\":\"To monitor data drift with Evidently, follow these steps:\\n\\n1. **Set Up Your Evaluation Pipeline**: Create a periodic job (e.g., using a cron job, Airflow, or a simple Python script) to run evaluations either at fixed intervals or upon receiving new data.\\n2. **Use the DataDriftPreset**: This preset allows you to compare a current dataset against a reference dataset to identify shifts in data distribution.\\n\\n    ```python\\n    from evidently.report import Report\\n    from evidently.preset.data_drift import DataDriftPreset\\n\\n    report = Report([\\n        DataDriftPreset(),\\n    ])\\n    my_eval = report.run(current_data, reference_data)\\n    ```  \\n3. **Customize Your Reports**: Include decisions based on the drift results by setting up tests that can trigger model retraining or flagging the necessity of intervention. You can also modify methods to include statistical tests best suited to your data types.\\n4. **Monitor and Visualize Results**: Store evaluation reports in Evidently Cloud or a self-hosted workspace for visualization. This helps in tracking performance over time.\\n5. **Regularly Review and Update**: Schedule evaluations for continuous monitoring and adapt your model as necessary based on observed data drift.\",\"references\":[{\"title\":\"Data Drift Evaluation\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"How to Use Data Drift Presets\",\"filename\":\"metrics/explainer_drift.mdx\"}]},{\"heading\":\"Key Features\",\"content\":\"- **Detection Methods**: Evidently includes multiple detection algorithms for measuring drift, like the two-sample Kolmogorov-Smirnov test and Jensen-Shannon divergence, tailored to different data types (categorical, numerical, text).\\n- **Report Customization**: Users can customize reports, selecting specific columns for drift analysis and integrating additional data quality checks.\\n- **Batch Monitoring**: Suitable for ML pipelines requiring periodic evaluation, offering flexibility in how and when evaluations are performed.\",\"references\":[{\"title\":\"Batch Monitoring\",\"filename\":\"docs/platform/monitoring_overview.mdx\"},{\"title\":\"Data Drift Methods\",\"filename\":\"metrics/explainer_drift.mdx\"}]}],\"references\":[{\"title\":\"Data Drift Overview\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"How Data Drift Detection Works\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Monitoring Overview\",\"filename\":\"docs/platform/monitoring_overview.mdx\"}]}",
          "tool_call_id": "call_uh0ZDpse9QJ1noghlHjeVmcP",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 17422,
        "cache_write_tokens": 0,
        "cache_read_tokens": 13056,
        "output_tokens": 672,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-04T10:18:25+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CY8PJO160BZfosFUIAagfsdfMQ3FE",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "content": "Final result processed.",
          "tool_call_id": "call_uh0ZDpse9QJ1noghlHjeVmcP",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:38.432601+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": null,
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "args": "{\"found_answer\":true,\"title\":\"Monitoring Data Drift in Production\",\"sections\":[{\"heading\":\"Overview\",\"content\":\"In production, monitoring data drift is essential for maintaining the performance and reliability of your machine learning models. Data drift refers to the changes in the distribution of input data over time, which can adversely affect model predictions and lead to quality degradation. Evidently provides a framework for effectively monitoring data drift using various metrics and reports.\",\"references\":[{\"title\":\"Data Drift Overview\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"How Data Drift Detection Works\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Monitoring Overview\",\"filename\":\"docs/platform/monitoring_overview.mdx\"}]},{\"heading\":\"Methodology\",\"content\":\"To monitor data drift with Evidently, follow these steps:\\n\\n1. **Set Up Your Evaluation Pipeline**: Create a periodic job (e.g., using a cron job, Airflow, or a simple Python script) to run evaluations either at fixed intervals or upon receiving new data.\\n2. **Use the DataDriftPreset**: This preset allows you to compare a current dataset against a reference dataset to identify shifts in data distribution.\\n\\n    ```python\\n    from evidently.report import Report\\n    from evidently.preset.data_drift import DataDriftPreset\\n\\n    report = Report([\\n        DataDriftPreset(),\\n    ])\\n    my_eval = report.run(current_data, reference_data)\\n    ```  \\n3. **Customize Your Reports**: Include decisions based on the drift results by setting up tests that can trigger model retraining or flagging the necessity of intervention. You can also modify methods to include statistical tests best suited to your data types.\\n4. **Monitor and Visualize Results**: Store evaluation reports in Evidently Cloud or a self-hosted workspace for visualization. This helps in tracking performance over time.\\n5. **Regularly Review and Update**: Schedule evaluations for continuous monitoring and adapt your model as necessary based on observed data drift.\",\"references\":[{\"title\":\"Data Drift Evaluation\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"How to Use Data Drift Presets\",\"filename\":\"metrics/explainer_drift.mdx\"}]},{\"heading\":\"Key Features\",\"content\":\"- **Detection Methods**: Evidently includes multiple detection algorithms for measuring drift, like the two-sample Kolmogorov-Smirnov test and Jensen-Shannon divergence, tailored to different data types (categorical, numerical, text).\\n- **Report Customization**: Users can customize reports, selecting specific columns for drift analysis and integrating additional data quality checks.\\n- **Batch Monitoring**: Suitable for ML pipelines requiring periodic evaluation, offering flexibility in how and when evaluations are performed.\",\"references\":[{\"title\":\"Batch Monitoring\",\"filename\":\"docs/platform/monitoring_overview.mdx\"},{\"title\":\"Data Drift Methods\",\"filename\":\"metrics/explainer_drift.mdx\"}]}],\"references\":[{\"title\":\"Data Drift Overview\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"How Data Drift Detection Works\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Monitoring Overview\",\"filename\":\"docs/platform/monitoring_overview.mdx\"}]}",
          "tool_call_id": "call_uh0ZDpse9QJ1noghlHjeVmcP",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 17422,
        "cache_write_tokens": 0,
        "cache_read_tokens": 13056,
        "output_tokens": 672,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-04T10:18:25+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CY8PJO160BZfosFUIAagfsdfMQ3FE",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "content": "Final result processed.",
          "tool_call_id": "call_uh0ZDpse9QJ1noghlHjeVmcP",
          "metadata": null,
          "timestamp": "2025-11-04T10:18:38.432795+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": null,
      "kind": "request"
    }
  ],
  "usage": {
    "input_tokens": 31289,
    "cache_write_tokens": 0,
    "cache_read_tokens": 13056,
    "output_tokens": 861,
    "input_audio_tokens": 0,
    "cache_audio_read_tokens": 0,
    "output_audio_tokens": 0,
    "details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "requests": 3,
    "tool_calls": 8
  },
  "output": {
    "found_answer": true,
    "title": "Monitoring Data Drift in Production",
    "sections": [
      {
        "heading": "Overview",
        "content": "In production, monitoring data drift is essential for maintaining the performance and reliability of your machine learning models. Data drift refers to the changes in the distribution of input data over time, which can adversely affect model predictions and lead to quality degradation. Evidently provides a framework for effectively monitoring data drift using various metrics and reports.",
        "references": [
          {
            "title": "Data Drift Overview",
            "filename": "metrics/preset_data_drift.mdx"
          },
          {
            "title": "How Data Drift Detection Works",
            "filename": "metrics/explainer_drift.mdx"
          },
          {
            "title": "Monitoring Overview",
            "filename": "docs/platform/monitoring_overview.mdx"
          }
        ]
      },
      {
        "heading": "Methodology",
        "content": "To monitor data drift with Evidently, follow these steps:\n\n1. **Set Up Your Evaluation Pipeline**: Create a periodic job (e.g., using a cron job, Airflow, or a simple Python script) to run evaluations either at fixed intervals or upon receiving new data.\n2. **Use the DataDriftPreset**: This preset allows you to compare a current dataset against a reference dataset to identify shifts in data distribution.\n\n    ```python\n    from evidently.report import Report\n    from evidently.preset.data_drift import DataDriftPreset\n\n    report = Report([\n        DataDriftPreset(),\n    ])\n    my_eval = report.run(current_data, reference_data)\n    ```  \n3. **Customize Your Reports**: Include decisions based on the drift results by setting up tests that can trigger model retraining or flagging the necessity of intervention. You can also modify methods to include statistical tests best suited to your data types.\n4. **Monitor and Visualize Results**: Store evaluation reports in Evidently Cloud or a self-hosted workspace for visualization. This helps in tracking performance over time.\n5. **Regularly Review and Update**: Schedule evaluations for continuous monitoring and adapt your model as necessary based on observed data drift.",
        "references": [
          {
            "title": "Data Drift Evaluation",
            "filename": "metrics/preset_data_drift.mdx"
          },
          {
            "title": "How to Use Data Drift Presets",
            "filename": "metrics/explainer_drift.mdx"
          }
        ]
      },
      {
        "heading": "Key Features",
        "content": "- **Detection Methods**: Evidently includes multiple detection algorithms for measuring drift, like the two-sample Kolmogorov-Smirnov test and Jensen-Shannon divergence, tailored to different data types (categorical, numerical, text).\n- **Report Customization**: Users can customize reports, selecting specific columns for drift analysis and integrating additional data quality checks.\n- **Batch Monitoring**: Suitable for ML pipelines requiring periodic evaluation, offering flexibility in how and when evaluations are performed.",
        "references": [
          {
            "title": "Batch Monitoring",
            "filename": "docs/platform/monitoring_overview.mdx"
          },
          {
            "title": "Data Drift Methods",
            "filename": "metrics/explainer_drift.mdx"
          }
        ]
      }
    ],
    "references": [
      {
        "title": "Data Drift Overview",
        "filename": "metrics/preset_data_drift.mdx"
      },
      {
        "title": "How Data Drift Detection Works",
        "filename": "metrics/explainer_drift.mdx"
      },
      {
        "title": "Monitoring Overview",
        "filename": "docs/platform/monitoring_overview.mdx"
      }
    ]
  }
}