{
  "agent_name": "search",
  "system_prompt": [
    "\nYou are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.\n"
  ],
  "provider": "openai",
  "model": "gpt-4o-mini",
  "tools": [
    "search",
    "read_file"
  ],
  "messages": [
    {
      "parts": [
        {
          "content": "How do I monitor data drift in production?",
          "timestamp": "2025-11-04T10:56:59.067577+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "args": "{\"query\": \"monitoring data drift in production\"}",
          "tool_call_id": "call_v5uYaDpqSkSKsIJJ5S5EIbkB",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"data drift detection methods in production\"}",
          "tool_call_id": "call_8pAflYRGMPzxuhFMusUf8aiU",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"Evidently data drift monitoring features\"}",
          "tool_call_id": "call_J7iDIuasE5y0qxE7jqEVXA1k",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"track data drift using Evidently\"}",
          "tool_call_id": "call_TyqfGYjCUrWXOVXiuIeQPqVd",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"observing data shift in production systems\"}",
          "tool_call_id": "call_fiuIJKWX8bH9M3LsWaKlcElf",
          "id": null,
          "part_kind": "tool-call"
        },
        {
          "tool_name": "search",
          "args": "{\"query\": \"how to detect data drift in machine learning\"}",
          "tool_call_id": "call_Ccf1lBMJr3wV1dqtxvMAmveQ",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 784,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 128,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-04T10:56:59+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CY90duksEDlYiNBST8alMGTvLzwNh",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 1000,
              "content": " observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\n</Note>\n\n## Dataset drift\n\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\n\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n## Tabular data drift\n\nThe following defaults apply for tabular data: numerical and categorical columns.\n\nFor **small data with \\<= 1000 observations** in the reference dataset:\n\n* For numerical columns (n\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipe",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            }
          ],
          "tool_call_id": "call_v5uYaDpqSkSKsIJJ5S5EIbkB",
          "metadata": null,
          "timestamp": "2025-11-04T10:57:04.459204+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 6000,
              "content": " obtain.\u00a0\n\n<Info>\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\n</Info>\n\nIf the drift is detected, Evidently will also calculate the\u00a0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words such as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://e",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 0,
              "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 0,
              "content": "All Metrics and Presets that evaluate shift in data distributions use the default [Data Drift algorithm](/metrics/explainer_drift). It automatically selects the drift detection method based on the column type (text, categorical, numerical) and volume.\n\nYou can override the defaults by passing a custom parameter to the chosen Metric or Preset. You can modify the drift detection method (choose from 20+ available), thresholds, or both.&#x20;\n\nYou can also implement fully custom drift detection methods.\n\n**Pre-requisites**:\n\n* You know how to use [Data Definition ](/docs/library/data_definition)to map column types.\n\n* You know how to create [Reports](/docs/library/report) and run [Tests](/docs/library/tests).\n\n## Data drift parameters\n\n<Note>\n  Setting conditions for data drift works differently from the usual Test  API (with `gt`, `lt`, etc.) This accounts for nuances like varying role of thresholds across drift detection methods, where \"greater\"  can be better or worse depending on the method.&#x20;\n</Note>\n\n### Dataset-level\n\n**Dataset drift share**. You can set the share of drifting columns that signals **dataset drift** (default: 0.5) in the relevant Metrics or Presets. For example, to set it at 70%:\n\n```python\nreport = Report([\n    DataDriftPreset(drift_share=0.7)\n]\n```\n\nThis will detect dataset drift if over 70% columns are drifting, using defaults for each column.\n\n**Drift methods**. You can also specify the drift detection methods used on the column level. For example, to use PSI (Population Stability Index) for all columns in the dataset:\n\n```python\nreport = Report([\n    DataDriftPreset(drift_share=0.7, method=\"psi\")\n]\n```\n\nThis will check if over 70% columns are drifting, using PSI method with default thresholds.\n\n<Tip>\n  See all available methods in the table below.\n</Tip>\n\n**Drift thresholds**. You can set thresholds for each method. For example, use PSI with a threshold of 0.3 for categorical columns.\n\n```python\nreport = Report([\n    DataDriftPreset(cat_me",
              "title": "Customize Data Drift",
              "description": "How to change data drift detection methods and conditions.",
              "filename": "metrics/customize_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_8pAflYRGMPzxuhFMusUf8aiU",
          "metadata": null,
          "timestamp": "2025-11-04T10:57:04.477050+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 4000,
              "content": "lumns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\"prediction\")` to your Report so that you see the drift in this value in a separate widget.\n\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\n\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\n\n<Info>\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\n</Info>",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_J7iDIuasE5y0qxE7jqEVXA1k",
          "metadata": null,
          "timestamp": "2025-11-04T10:57:04.464032+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 3000,
              "content": "cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n\n**Change drift parameters.** You can modify how drift detection works:\n\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n\n* **Implement a custom method**. You can implement a custom drift method as Python function.\n\n<Info>\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n</Info>\n\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n\n* **Single out the Target/Prediction column.** If you want to evaluate drift in",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 0,
              "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            }
          ],
          "tool_call_id": "call_TyqfGYjCUrWXOVXiuIeQPqVd",
          "metadata": null,
          "timestamp": "2025-11-04T10:57:04.467231+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 0,
              "content": "**Pre-requisites**:\n* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\n* You know how to create [Reports](/docs/library/report).\n  \n**Report.** To run a Preset on your data, comparing `current` data to `ref` data:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n])\n\nmy_eval = report.run(current, ref)\n```\n\n**Test Suite.** To add Tests with explicit pass/fail for each column:\n\n```python\nreport = Report([\n    DataDriftPreset(),\n],\ninclude_tests=True)\n\nmy_eval = report.run(current, ref)\n```\n\n## Overview\n\nThe`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.&#x20;\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n* **Column drift.** Checks for shifts in each column. The [drift detection method](/metrics/explainer_drift) is chosen automatically based on the column type and number of observations.\n\n* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These ar",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            },
            {
              "start": 6000,
              "content": " obtain.\u00a0\n\n<Info>\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\n</Info>\n\nIf the drift is detected, Evidently will also calculate the\u00a0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words such as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://e",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 1000,
              "content": "Target value, it will be evaluated together with other columns.\n\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n\n![](/images/metrics/preset_data_drift-min.png)\n\nIf you choose to enable Tests, you will get an additional Test Suite view:\n\n![](/images/metrics/test_preset_data_drift-min.png)\n\n<Info>\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n</Info>\n\n## Use case\n\nYou can evaluate data drift in different scenarios.\n\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_fiuIJKWX8bH9M3LsWaKlcElf",
          "metadata": null,
          "timestamp": "2025-11-04T10:57:04.473840+00:00",
          "part_kind": "tool-return"
        },
        {
          "tool_name": "search",
          "content": [
            {
              "start": 7000,
              "content": "ch as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\n\n* [\"My data drifted. What's next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 0,
              "content": "In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\n\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\n\n<Info>\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\n</Info>\n\n## How it works\n\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\n\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\n\n* column type: categorical, numerical, text data \n\n* the number of observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 6000,
              "content": " obtain.\u00a0\n\n<Info>\n  **How this works.** The drift score is the ROC-AUC score of the domain classifier computed on a validation dataset. This ROC AUC is compared to the ROC AUC of the random classifier at a set percentile. To ensure the result is statistically meaningful, we repeat the calculation 1000 times with randomly assigned target class probabilities. This produces a distribution with a mean of 0.5. We then take the 95th percentile (default) of this distribution and compare it to the ROC-AUC score of the classifier. If the classifier score is higher, we consider the data drift to be detected. You can also set a different percentile as a parameter.\n</Info>\n\nIf the drift is detected, Evidently will also calculate the\u00a0**top features of the domain classifier**. The resulting output contains specific characteristic words that help identify whether a given sample belongs to reference or current. They are normalized based on vocabulary, for example, to exclude non-interpretable words such as articles.\n\n<Tip>\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\n</Tip>\n\n\n## Resources\n\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\n\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\n\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n\nAdditional links:\n\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\n\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://e",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 1000,
              "content": " observations in the reference dataset\n\n* the number of unique values in the column (n\\_unique)\n\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\n\n## Data requirements\n\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\n\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\n\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\n\n<Note>\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important indicator, we recommend running separate tests on share of nulls in the dataset and/or columns. You can choose from several [tests](/metrics/all_metrics#column-data-quality).\n</Note>\n\n## Dataset drift\n\nWith Presets like `DatasetDriftPreset()` and Metrics like `DriftedColumnsCount(),`  you can also set a rule on top of the individual column drift results to detect dataset-level drift.\n\nFor example, you can declare dataset drift if 50% of all features (columns) drifted. In this case, each column in the Dataset is tested for drift individually using a default method for the column type. You can specify a custom threshold as a [parameter](/metrics/customize_data_drift).\n\n![](/images/metrics/preset_data_drift_2-min.png)\n\n## Tabular data drift\n\nThe following defaults apply for tabular data: numerical and categorical columns.\n\nFor **small data with \\<= 1000 observations** in the reference dataset:\n\n* For numerical columns (n\\_unique > 5): [two-sample Kolmogorov-Smirnov test](https://en.wikipe",
              "title": "Data drift",
              "description": "How data drift detection works",
              "filename": "metrics/explainer_drift.mdx"
            },
            {
              "start": 2000,
              "content": "e proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n\n<Info>\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n</Info>\n\n## Data requirements\n\n* **Input columns**. You can provide any input columns. They must be non-empty.\n\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n\n<Info>\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n</Info>\n\n## Report customization\n\nYou have multiple customization options.\n\n**Select co",
              "title": "Data Drift",
              "description": "Overview of the Data Drift Preset.",
              "filename": "metrics/preset_data_drift.mdx"
            }
          ],
          "tool_call_id": "call_Ccf1lBMJr3wV1dqtxvMAmveQ",
          "metadata": null,
          "timestamp": "2025-11-04T10:57:04.480126+00:00",
          "part_kind": "tool-return"
        },
        {
          "content": "System message: The maximal number of searches has exceeded 6. Proceed to finishing the writeup",
          "timestamp": "2025-11-04T10:57:04.480872+00:00",
          "part_kind": "user-prompt"
        }
      ],
      "instructions": "You are a search assistant for the Evidently documentation.\n\nEvidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\nIt provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n\nYour task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n\nYou have access to the following tools:\n\n- search \u2014 Use this to explore the topic and retrieve relevant snippets or documentation.\n- read_file \u2014 Use this to retrieve or verify the complete content of a file when:\n    * A code snippet is incomplete, truncated, or missing definitions.\n    * You need to check that all variables, imports, and functions referenced in code are defined.\n    * You must ensure the code example is syntactically correct and runnable.\n\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\n> \"Unable to verify with read_file.\"\n\nSearch Strategy\n\n- For every user query:\n    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n    * Each search must use a different phrasing or keyword variation of the user's question.\n    * Keep all searches relevant to Evidently (no need to include \"Evidently\" in the search text).\n\n- After collecting search results:\n    1. Synthesize the information into a concise, accurate answer.\n    2. If your answer includes code, always validate it with `read_file` before finalizing.\n    3. If a code snippet or reference is incomplete, explicitly mention it.\n\nImportant:\n- The 6-search limit applies only to `search` calls.\n- You may call `read_file` at any time, even after the search limit is reached.\n- `read_file` calls are verification steps and do not count toward the 6-search limit.\n\nCode Verification and Completeness Rules\n\n- All variables, functions, and imports in your final code examples must be defined or imported.\n- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n- When something is missing or undefined in the search results:\n    * Call `read_file` with the likely filename to retrieve the complete file content.\n    * Replace any partial code with the full verified version.\n- If the file is not available or cannot be verified:\n    * Include a clear note: \"Unable to verify this code.\"\n- Do not reformat, rename variables, or omit lines from the verified code.\n\nOutput Format\n\n- Write your answer clearly and accurately.\n- Include a \"References\" section listing the search queries or file names you used.\n- If you couldn\u2019t find a complete answer after 6 searches, set found_answer = False.",
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "args": "{\"found_answer\":true,\"title\":\"Monitoring Data Drift in Production\",\"sections\":[{\"heading\":\"Overview\",\"content\":\"To effectively monitor data drift in production, you can use the Evidently library, which offers various methods for detecting shifts in data distributions from historical reference data to current datasets. Monitoring data drift is crucial to ensure your machine learning models continue to perform well as the data changes over time.\" ,\"references\":[{\"title\":\"Data Drift\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Data Drift\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"Customize Data Drift\",\"filename\":\"metrics/customize_data_drift.mdx\"}]}],\"references\":[{\"title\":\"monitoring data drift in production\",\"filename\":\"\"},{\"title\":\"data drift detection methods in production\",\"filename\":\"\"},{\"title\":\"Evidently data drift monitoring features\",\"filename\":\"\"},{\"title\":\"track data drift using Evidently\",\"filename\":\"\"},{\"title\":\"observing data shift in production systems\",\"filename\":\"\"},{\"title\":\"how to detect data drift in machine learning\",\"filename\":\"\"}]}",
          "tool_call_id": "call_lfA8eZu966ZIG7DMSzfI13W6",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 15269,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 229,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-04T10:57:04+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CY90iI0BQfeLvbrMdQ9aSJevhRAp5",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "content": "Final result processed.",
          "tool_call_id": "call_lfA8eZu966ZIG7DMSzfI13W6",
          "metadata": null,
          "timestamp": "2025-11-04T10:57:11.022352+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": null,
      "kind": "request"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "args": "{\"found_answer\":true,\"title\":\"Monitoring Data Drift in Production\",\"sections\":[{\"heading\":\"Overview\",\"content\":\"To effectively monitor data drift in production, you can use the Evidently library, which offers various methods for detecting shifts in data distributions from historical reference data to current datasets. Monitoring data drift is crucial to ensure your machine learning models continue to perform well as the data changes over time.\" ,\"references\":[{\"title\":\"Data Drift\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Data Drift\",\"filename\":\"metrics/preset_data_drift.mdx\"},{\"title\":\"Customize Data Drift\",\"filename\":\"metrics/customize_data_drift.mdx\"}]}],\"references\":[{\"title\":\"monitoring data drift in production\",\"filename\":\"\"},{\"title\":\"data drift detection methods in production\",\"filename\":\"\"},{\"title\":\"Evidently data drift monitoring features\",\"filename\":\"\"},{\"title\":\"track data drift using Evidently\",\"filename\":\"\"},{\"title\":\"observing data shift in production systems\",\"filename\":\"\"},{\"title\":\"how to detect data drift in machine learning\",\"filename\":\"\"}]}",
          "tool_call_id": "call_lfA8eZu966ZIG7DMSzfI13W6",
          "id": null,
          "part_kind": "tool-call"
        }
      ],
      "usage": {
        "input_tokens": 15269,
        "cache_write_tokens": 0,
        "cache_read_tokens": 0,
        "output_tokens": 229,
        "input_audio_tokens": 0,
        "cache_audio_read_tokens": 0,
        "output_audio_tokens": 0,
        "details": {
          "accepted_prediction_tokens": 0,
          "audio_tokens": 0,
          "reasoning_tokens": 0,
          "rejected_prediction_tokens": 0
        }
      },
      "model_name": "gpt-4o-mini-2024-07-18",
      "timestamp": "2025-11-04T10:57:04+00:00",
      "kind": "response",
      "provider_name": "openai",
      "provider_details": {
        "finish_reason": "tool_calls"
      },
      "provider_response_id": "chatcmpl-CY90iI0BQfeLvbrMdQ9aSJevhRAp5",
      "finish_reason": "tool_call"
    },
    {
      "parts": [
        {
          "tool_name": "final_result",
          "content": "Final result processed.",
          "tool_call_id": "call_lfA8eZu966ZIG7DMSzfI13W6",
          "metadata": null,
          "timestamp": "2025-11-04T10:57:11.022516+00:00",
          "part_kind": "tool-return"
        }
      ],
      "instructions": null,
      "kind": "request"
    }
  ],
  "usage": {
    "input_tokens": 16053,
    "cache_write_tokens": 0,
    "cache_read_tokens": 0,
    "output_tokens": 357,
    "input_audio_tokens": 0,
    "cache_audio_read_tokens": 0,
    "output_audio_tokens": 0,
    "details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "requests": 2,
    "tool_calls": 6
  },
  "output": {
    "found_answer": true,
    "title": "Monitoring Data Drift in Production",
    "sections": [
      {
        "heading": "Overview",
        "content": "To effectively monitor data drift in production, you can use the Evidently library, which offers various methods for detecting shifts in data distributions from historical reference data to current datasets. Monitoring data drift is crucial to ensure your machine learning models continue to perform well as the data changes over time.",
        "references": [
          {
            "title": "Data Drift",
            "filename": "metrics/explainer_drift.mdx"
          },
          {
            "title": "Data Drift",
            "filename": "metrics/preset_data_drift.mdx"
          },
          {
            "title": "Customize Data Drift",
            "filename": "metrics/customize_data_drift.mdx"
          }
        ]
      }
    ],
    "references": [
      {
        "title": "monitoring data drift in production",
        "filename": ""
      },
      {
        "title": "data drift detection methods in production",
        "filename": ""
      },
      {
        "title": "Evidently data drift monitoring features",
        "filename": ""
      },
      {
        "title": "track data drift using Evidently",
        "filename": ""
      },
      {
        "title": "observing data shift in production systems",
        "filename": ""
      },
      {
        "title": "how to detect data drift in machine learning",
        "filename": ""
      }
    ]
  }
}